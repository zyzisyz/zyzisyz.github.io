{"meta":{"title":"zyzisyz","subtitle":"zyziszy blog","description":"","author":"Yang Zhang","url":"http://yoursite.com","root":"/"},"pages":[{"title":"books","date":"2020-06-10T03:09:48.245Z","updated":"2020-06-10T03:09:48.245Z","comments":true,"path":"books/index.html","permalink":"http://yoursite.com/books/index.html","excerpt":"","text":".total{display:-ms-flexbox;display:flex;-ms-flex-wrap:wrap;flex-wrap:wrap;-ms-flex-pack:center;justify-content:center;-ms-flex-align:start;align-items:flex-start;-ms-flex-line-pack:start;align-content:flex-start;width:100%}.item{position:relative;width:270px;height:150px;margin:0 10px 20px;box-shadow:0 12px 15px 0 rgba(0,0,0,.19),0 17px 50px 0 rgba(0,0,0,.12)}.item img:not(.LGallerySlider-Slide){width:100%;height:100%;transition:all .5s}.item span{position:absolute;bottom:0;left:0;padding:1px 0;display:block;width:100%;height:20px;line-height:20px;text-align:center;color:#fcfaf9;background-color:#0E0E0E80;transition:all .5s}.item:hover{box-shadow:0 12px 15px 0 transparent,0 17px 50px 0 transparent}.item:hover img:not(.LGallerySlider-Slide){opacity:1;transform:scale(.8) rotate3d(-1,1,0,-20deg);box-shadow:-10px -10px 2px .3px rgba(0,0,0,.6),-20px -20px 3px .3px rgba(0,0,0,.3),-30px -30px 4px .3px rgba(0,0,0,.1)}.item:hover span{height:75px;line-height:75px;opacity:0;z-index:-1}"},{"title":"about","date":"2020-05-27T12:53:07.000Z","updated":"2020-06-09T07:42:07.478Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""}],"posts":[{"title":"Speech领域专业名词（持续更新）","slug":"001_speech_note","date":"2020-06-29T16:00:00.000Z","updated":"2020-06-29T06:19:34.223Z","comments":true,"path":"2020/06/30/001_speech_note/","link":"","permalink":"http://yoursite.com/2020/06/30/001_speech_note/","excerpt":"","text":"频率 Frequency &amp; 幅度 Magnitude &amp; 相位 Phase音高 Pitch &amp; 音量 Loudness &amp; 音色 Timbre分贝 dB模数转换 Analog to Digital Conversion，ADC音素 Phoneme描述一种语言的基本单位被称为音素Phoneme，例如BRYAN这个词就可以看做是由B, R, AY, AX, N五个音素构成的。英语中大概有50多个音素，可以用50几个HMM state来表示这些音素。 掩蔽效应 Masking Effects频谱图 Spectrogram声学模型 Acoustic model语音中很多发音都是连在一起的，很难区分，所以一般用左中右三个HMM state来描述一个音素。举例来说BRYAN这个词中的R音素就变成了用B-R, R, R-AY三个HMM state来表示。这样BRYAN这个词根据上下文就需要15个state了，根据所有单词的上下文总共大概需要几千个HMM state，这种方式属于context dependent模型中的三音素triphone模式。这个HMM state的个数在各家语音识别系统中都不一样，是一个需要调的参数。所以声学模型就是如何设置HMM state。 语音模型 Language Model, LMLexicon帧 Framechunk隐马尔可夫模型 Hidden Markov Model, HMM参考[1] CSLT微信公众号-语音识别基础：（一）语音是什么 [2] CSLT微信公众号-语音识别基础：（二）语音识别方法 [3] 语音识别基础：（三）完整的语音识别实验","categories":[{"name":"Speech","slug":"Speech","permalink":"http://yoursite.com/categories/Speech/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"语音","slug":"语音","permalink":"http://yoursite.com/tags/%E8%AF%AD%E9%9F%B3/"}]},{"title":"美化 Python 异常信息","slug":"200617_pyerrors","date":"2020-06-16T16:00:00.000Z","updated":"2020-06-21T01:24:52.050Z","comments":true,"path":"2020/06/17/200617_pyerrors/","link":"","permalink":"http://yoursite.com/2020/06/17/200617_pyerrors/","excerpt":"","text":"安装Github链接PrettyErrors python -m pip install pretty_errors python -m pretty_errors 参考[1] 知乎-一行代码简化Python异常信息","categories":[{"name":"toolkit","slug":"toolkit","permalink":"http://yoursite.com/categories/toolkit/"}],"tags":[{"name":"开发","slug":"开发","permalink":"http://yoursite.com/tags/%E5%BC%80%E5%8F%91/"},{"name":"工具","slug":"工具","permalink":"http://yoursite.com/tags/%E5%B7%A5%E5%85%B7/"},{"name":"效率","slug":"效率","permalink":"http://yoursite.com/tags/%E6%95%88%E7%8E%87/"}]},{"title":"计算机网络复习","slug":"200621_computer_network","date":"2020-06-16T16:00:00.000Z","updated":"2020-06-21T01:34:11.246Z","comments":true,"path":"2020/06/17/200621_computer_network/","link":"","permalink":"http://yoursite.com/2020/06/17/200621_computer_network/","excerpt":"","text":"工作装备 物理层：中继器，集线器 数据链路：网桥，交换机 网络层：路由器 传输层： 网关 网络协议 物理层： RJ45 、 CLOCK 、 IEEE802.3 数据链路： PPP 、 FR 、 HDLC 、 VLAN 、 MAC 网络层： IP 、 ICMP 、 ARP 、 RARP 、 OSPF 、 IPX 、 RIP 、 IGRP 传输层： TCP 、 UDP 、 SPX 会话层： NFS 、 SQL 、 NETBIOS 、 RPC 表示层： JPEG 、 MPEG 、 ASII 应用层： FTP 、 DNS 、 Telnet 、 SMTP 、 HTTP 、 WWW 、 NFS","categories":[{"name":"offer","slug":"offer","permalink":"http://yoursite.com/categories/offer/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://yoursite.com/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"Linear Discriminant Analysis","slug":"200614_Linear_Discriminant_Analysis","date":"2020-06-13T16:00:00.000Z","updated":"2020-06-21T01:33:51.517Z","comments":true,"path":"2020/06/14/200614_Linear_Discriminant_Analysis/","link":"","permalink":"http://yoursite.com/2020/06/14/200614_Linear_Discriminant_Analysis/","excerpt":"","text":"线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的线性分类方法。LDA是一种有监督的线性分类算法。LDA的基本思想是将数据投影到低维空间后，使得：同一类数据尽可能接近，不同类数据尽可能疏远。在对新样本进行分类时，将其投影到同样的低维空间上，再根据投影点的位置来确定新样本的类别。 值得一提的是，LDA 可从贝时斯决策理论的角度来阐释，并可证明，当两类数据同先验、满足高斯分布且协方差相等时，LDA 可达到最优分类。 Fisher discriminant criterion $X_i$ 表示 $i$ 类示例的集合 $N$ 表示有 $N$ 类，且第 $i$ 类的样本数为 $N_i$ $n$ 表示总共有 $n$ 个数据 $\\mu_i$ 表示 $i$ 类示例的均值向量，$\\mu$ 表示总的均值向量 $\\Sigma_i$ 表示 $i$ 类示例的协方差矩阵 $S_w$ 表示类内散度矩阵（within-class scatter matrix） $S_b$ 表示类间散度矩阵（between-class scatter matrix） $S_t$ 表示全局散度矩阵 $w$ 为变换矩阵 $J(w)$ 是最大目标Fisher判别准则 $$\\boldsymbol{\\mu_i} = \\frac{1}{N_i} \\sum_{\\boldsymbol{x} \\in X_i}\\boldsymbol{x}$$ $$\\Sigma_i = \\sum_{\\boldsymbol{x} \\in X_i}(\\boldsymbol{x}-\\boldsymbol{\\mu_i})(\\boldsymbol{x}-\\boldsymbol{\\mu_i})^T$$ $$S_w = \\sum^N_{i=1}S_{w_i} = \\sum^N_{i=1}\\Sigma_i = \\sum^N_{i=1}\\sum_{\\boldsymbol{x} \\in X_i}(\\boldsymbol{x}-\\boldsymbol{\\mu_i})(\\boldsymbol{x}-\\boldsymbol{\\mu_i})^T$$ $$S_b = S_t - S_w = \\sum^N_{i=1}N_i(\\boldsymbol{\\mu_i}-\\boldsymbol{\\mu})(\\boldsymbol{\\mu_i}-\\boldsymbol{\\mu})^T$$ $$S_t = S_b + S_w =\\sum_{i=1}^{n}(\\boldsymbol{x_i}-\\boldsymbol{\\mu})(\\boldsymbol{x_i}-\\boldsymbol{\\mu})^T$$ $$J(w) = \\frac{w^TS_bw}{w^TS_ww}$$ 优化目标推导过程 $$ \\frac{\\mathrm{d}J(w)}{\\mathrm{d}w} = 0 $$ $$\\frac{\\mathrm{d}J(w)}{\\mathrm{d}w} = \\frac{\\mathrm{d}}{\\mathrm{d}w}(\\frac{w^TS_bw}{w^TS_ww}) =$$ 多分类 LDA 可以有多种实现方法：使用 $S_b$, $S_w$ , $S_t$ 三者中的任何两个即可 LDA 算法的训练流程 计算类内散度矩阵 $S_w$ 计算类间散度矩阵 $S_b$ 计算矩阵 $S_w^{-1}S_b$ 计算矩阵 $S_w^{-1}S_b$ 的特征值与特征向量，按从小到大的顺序选取前 $d$ 个特征值和对应的 $d$ 个特征向量，得到投影矩阵 $w$ sklearn包 LDA 的使用from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA lda = LDA(solver='svd', n_components=LDA_components) # fit lda.fit(train_data, train_label) # transform train_data = lda.transform(train_data) test_data = lda.transform(test_data) LDA Python 代码实现相关论文参考[1] CSLT-THU王东老师PPT-Static Analysis: LDA [2] 西瓜书-线性判别分析 [3] CSLT-THU王东老师-现代机器学习导论 [4] 机器学习实验室微信公众号-数学推导LDA线性判别分析 [5] 博客园-LDA [6] 知乎-线性判别分析LDA原理及推导过程（非常详细） [7] THU袁博老师数据挖掘课程-数据预处理PPT [8] 知乎-Fisher判别分析(Fisher Discriminant Analysis)","categories":[{"name":"backends","slug":"backends","permalink":"http://yoursite.com/categories/backends/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"笔记","slug":"笔记","permalink":"http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"}]},{"title":"flow 模型","slug":"002_flow","date":"2020-06-12T10:02:51.899Z","updated":"2020-06-12T10:02:51.899Z","comments":true,"path":"2020/06/12/002_flow/","link":"","permalink":"http://yoursite.com/2020/06/12/002_flow/","excerpt":"","text":"基础数学生成模型要解决的问题给定两组数据 $z$ 和 $x$ ，其中 $z$ 服从已知的简单先验分布 $\\pi(z)$ ，通常是高斯分布，$x$ 服从复杂的分布 $p(x)$ ,即训练数据代表的分布，现在我们想要找到一个变换函数 $f$ ，它能建立一种 $z$ 到 $x$ 的映射，使得每对于 $\\pi(z)$ 中的一个采样点，都能在 $p(x)$ 中有一个（新）样本点与之对应。 $$ p_g(x) = \\int_z p(x|z)p(z)dz\\ $$ 其中，$ p(x|z) $ - $ the \\ probability \\ of \\ x \\ given \\ z$ 概率分布的变换数学例题Jacobian Matrix$$\\left[ \\begin{array}{ccc} \\frac{\\partial f_1 }{\\partial x_1 } &amp; \\cdots &amp; \\frac{\\partial f_1 }{\\partial x_n } \\\\ \\vdots\\quad &amp; \\ddots &amp; \\vdots\\quad \\\\ \\frac{\\partial f_n }{\\partial x_1 } &amp; \\cdots &amp; \\frac{\\partial f_n }{\\partial x_n } \\\\ \\end{array}\\right]$$ $$J_{ij}=\\frac{\\partial f_i }{\\partial x_j }$$ Determinant耦合层（Coupling Layer）NICERealNVPMAFGlow相关论文参考","categories":[{"name":"flows","slug":"flows","permalink":"http://yoursite.com/categories/flows/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"笔记","slug":"笔记","permalink":"http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"DNF 模型的Pytorch实现","slug":"005_DNF_implement","date":"2020-06-12T09:54:16.554Z","updated":"2020-06-12T09:54:30.506Z","comments":true,"path":"2020/06/12/005_DNF_implement/","link":"","permalink":"http://yoursite.com/2020/06/12/005_DNF_implement/","excerpt":"","text":"","categories":[{"name":"flows","slug":"flows","permalink":"http://yoursite.com/categories/flows/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"笔记","slug":"笔记","permalink":"http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"flow 模型的Pytorch实现","slug":"004_flow_implement","date":"2020-06-12T09:52:20.594Z","updated":"2020-06-12T09:53:37.515Z","comments":true,"path":"2020/06/12/004_flow_implement/","link":"","permalink":"http://yoursite.com/2020/06/12/004_flow_implement/","excerpt":"","text":"","categories":[{"name":"flows","slug":"flows","permalink":"http://yoursite.com/categories/flows/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"笔记","slug":"笔记","permalink":"http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"DNF 模型","slug":"003_DNF","date":"2020-06-12T09:51:49.177Z","updated":"2020-06-12T09:53:18.047Z","comments":true,"path":"2020/06/12/003_DNF/","link":"","permalink":"http://yoursite.com/2020/06/12/003_DNF/","excerpt":"","text":"","categories":[{"name":"flows","slug":"flows","permalink":"http://yoursite.com/categories/flows/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"笔记","slug":"笔记","permalink":"http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"多个 PDF 文件的快速合并","slug":"200610_pdf_merge","date":"2020-06-09T16:00:00.000Z","updated":"2020-06-14T15:18:43.552Z","comments":true,"path":"2020/06/10/200610_pdf_merge/","link":"","permalink":"http://yoursite.com/2020/06/10/200610_pdf_merge/","excerpt":"","text":"毕设论文需要合并多个pdf文件，记录一下pdf文件的合并过程 安装sudo apt-get install poppler-utils 合并# 全部 pdfunite *.pdf all.pdf # 顺序 pdfunite 1.pdf 2.pdf 3.pdf 4.pdf all-1234.pdf 参考[1] linux bash合并PDF文件","categories":[{"name":"toolkit","slug":"toolkit","permalink":"http://yoursite.com/categories/toolkit/"}],"tags":[{"name":"开发","slug":"开发","permalink":"http://yoursite.com/tags/%E5%BC%80%E5%8F%91/"},{"name":"工具","slug":"工具","permalink":"http://yoursite.com/tags/%E5%B7%A5%E5%85%B7/"},{"name":"效率","slug":"效率","permalink":"http://yoursite.com/tags/%E6%95%88%E7%8E%87/"}]},{"title":"Softmax & Cross-Entropy & KLD","slug":"200609_softmax","date":"2020-06-08T16:00:00.000Z","updated":"2020-06-12T09:41:24.762Z","comments":true,"path":"2020/06/09/200609_softmax/","link":"","permalink":"http://yoursite.com/2020/06/09/200609_softmax/","excerpt":"","text":"Softmax$ Softmax $ 的作用是把一个序列$\\boldsymbol{a}$，变成概率。 $$ Softmax(\\boldsymbol{a}) = \\frac{e^{a_i}}{\\sum_j e^{a_j}} $$ 从概率的角度解释 $Softmax$ 的话，就是 $S_i = \\frac{e^{a_i}}{\\sum_{k=1}^Ne^{a_k}} = P(y=i|a)$ 其中，$LR$ 是 $Softmax$ 的类别数为 2 时 $Softmax$ 的特殊形式 Cross-Entropy给定两个概率分布： $p$ （理想结果即正确标签向量）和 $q$ （神经网络输出结果即经过 $softmax$ 转换后的结果向量），则通过 $q$ 来表示 $p$ 的交叉熵为 $$ H(p, q) = -\\sum_x p(x)log(q(x)) $$ 例如 $$H(p=[1,0,0], q=[0.5,0.4,0.1]) = -(1 \\cdot log(0.5) + 0 \\cdot log(0.4) + 0 \\cdot log(0.1)) $$ nll_loss (negative log likelihood loss)","categories":[{"name":"概率论与线性代数","slug":"概率论与线性代数","permalink":"http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"概率论","slug":"概率论","permalink":"http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"}]},{"title":"Pytorch Data Loader","slug":"200609_pytorch_data_loader","date":"2020-06-08T16:00:00.000Z","updated":"2020-06-09T11:26:25.832Z","comments":true,"path":"2020/06/09/200609_pytorch_data_loader/","link":"","permalink":"http://yoursite.com/2020/06/09/200609_pytorch_data_loader/","excerpt":"","text":"DotaLoader的构建 DataLoader(dataset, batch_size=200, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None) 官方模板PyTorch官方为我们提供了自定义数据读取的标准化代码代码模块。 from torch.utils.data import Dataset class CustomDataset(Dataset): def __init__(self, ...): pass def __getitem__(self, index): return (img, label) def __len__(self): # return examples size return count __init__()函数用于初始化数据读取逻辑，比如读取包含标签和图片地址的csv文件、定义transform组合等。 __getitem__()函数用来返回数据和标签。目的上是为了能够被后续的dataloader所调用。 __len__()函数则用于返回样本数量。 其中，__getitem__() 和__len__()用于构建Map-style datasets；__iter__()用于构建Iterable-style datasets（一般不太用） 训练集和验证集的划分如果需要对数据划分训练集和验证集，torch的Dataset对象也提供了random_split函数作为数据划分工具，且划分结果可直接供后续的DataLoader使用。 from torch.utils.data import random_split trainset, valset = random_split(dataset, [len_dataset*0.7, len_dataset*0.3]) Mnist Data Loader 实现参考[1] 官方文档 torch.utils.data [2] 夕小瑶的卖萌屋-PyTorch数据Pipeline标准化代码模板","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://yoursite.com/tags/Pytorch/"}]},{"title":"概率论复习","slug":"200608_bayes","date":"2020-06-07T16:00:00.000Z","updated":"2020-06-14T15:17:44.051Z","comments":true,"path":"2020/06/08/200608_bayes/","link":"","permalink":"http://yoursite.com/2020/06/08/200608_bayes/","excerpt":"","text":"加法公式$$ P(A \\cup B) = P(A) + P(B) - P(A\\cap B) $$ 乘法公式$$ P(AB) = P(A)P(B|A) $$ 协方差矩阵 与 散度矩阵将协方差矩阵乘以系数 $n-1$ 就得到了散度矩阵，矩阵的大小由特征维数 $d$ 决定，是一个为 $d×d$ 的半正定矩阵 贝叶斯公式贝叶斯公式用于描述两个条件概率之间的关系，比如 $P(A|B)$ 和 $P(B|A)$。 $$ P(A|B) = P(A) \\frac{P(B|A)}{P(B)} $$ $P(A|B)$ ：后验概率 $P(A)$：先验概率 $$ P(A_i|B) = \\frac{P(A_i)P(B|A_i)}{\\sum^n_{j=1} P(A_j)P(B|A_j)} $$ 全概率公式$$ P(B) = \\sum^n_{i=1} P(A_i)P(B|A_i) $$ 事件 $B$ 总伴随着某个 $A_i$ 出现 贝叶斯公式与全概率公式之间的关系$P(A_i|B)$ 后验概率（新信息 $B$ 出现后 $A$ 发生的概率） = $P(A)$ 先验概率（ $A$ 发生的概率） $ｘ$ 可能性函数（新信息带出现来的调整） 条件概率条件概率是指事件 $A$ 在另外一个事件 $B$ 已经发生条件下的发生概率。条件概率表示为：$P(A|B)$，读作“在B的条件下A的概率”或是the probability of x given z 边缘概率贝叶斯估计和极大似然估计 MLE, Maxium Likelihood Estimator：极大似然估计 MAP, Maxium a Posterior：最大后验概率 最大似然估计和贝叶斯估计最大区别便在于估计的参数不同。 最大似然估计是一种确定模型参数值的方法。确定参数值的过程，是找到能最大化模型产生真实观察数据可能性的那一组参数。要估计的参数 $θ$ 被当作是固定形式的一个未知变量，然后我们结合真实数据通过最大化似然函数来求解这个固定形式的未知变量。 贝叶斯估计则是将参数视为是有某种已知先验分布的随机变量，意思便是这个参数他不是一个固定的未知数，而是符合一定先验分布如：随机变量θ符合正态分布等！那么在贝叶斯估计中除了类条件概率密度 $p(x|w)$ 符合一定的先验分布，参数 $θ$ 也符合一定的先验分布。我们通过贝叶斯规则将参数的先验分布转化成后验分布进行求解。 在贝叶斯模型使用过程中，贝叶斯估计用的是后验概率，而最大似然估计直接使用的是类条件概率密度。 参考[1] B站-「一个模型」教你搞定贝叶斯和全概率公式 [2] 知乎-从最大似然估计开始，你需要打下的机器学习基石 [3] B站-MLE(极大似然)和MAP(最大后验)","categories":[{"name":"概率论与线性代数","slug":"概率论与线性代数","permalink":"http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"概率论","slug":"概率论","permalink":"http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"}]},{"title":"线段树","slug":"200531_segment_tree","date":"2020-05-30T16:00:00.000Z","updated":"2020-06-10T09:42:36.707Z","comments":true,"path":"2020/05/31/200531_segment_tree/","link":"","permalink":"http://yoursite.com/2020/05/31/200531_segment_tree/","excerpt":"","text":"线段树的功能线段树用来处理数组的区间查询（range query）和元素更新（update）操作。可以进行区间最大值，区间最小值或者区间异或值的查询。对应于树状数组，线段树进行更新（update）的操作为O(logn)，进行区间查询（range query）的操作也为O(logn)。 线段树的初始化更新（update）区间查询（range query）完整的板子参考练习题参考","categories":[{"name":"板子","slug":"板子","permalink":"http://yoursite.com/categories/%E6%9D%BF%E5%AD%90/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"板子","slug":"板子","permalink":"http://yoursite.com/tags/%E6%9D%BF%E5%AD%90/"}]},{"title":"神经网络结构","slug":"200531_nnet","date":"2020-05-30T16:00:00.000Z","updated":"2020-06-23T08:16:21.260Z","comments":true,"path":"2020/05/31/200531_nnet/","link":"","permalink":"http://yoursite.com/2020/05/31/200531_nnet/","excerpt":"","text":"多层感知机（MLP, Multilayer Perceptron）Fully Connected Layer 时延神经网络(TDNN, Time-Delay Neural Networks)TDNN是一个常用于语音信号处理领域卷积神经网络，使用 FFT 预处理的语音信号作为输入，其隐含层由2个一维卷积核组成，以提取频率域上的平移不变特征。 卷积神经网络（CNN, Convolutional Neural Networks）内积 (dot product, scalar product)Architect Convolution Pooling Flatten Property Some of patterns are much smaller than the whole image The smae pattterns appear in different regions Subsampling the pixels will not change the object 1 * 1 Convolution循环神经网络（RNN, Recurrent Neural Networks）长短期记忆网络（LSTM, Long Short-Term Memorys）参考[1] B站-李宏毅讲解卷积神经网络（带字幕","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"笔记","slug":"笔记","permalink":"http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"神经网络","slug":"神经网络","permalink":"http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"FFT 板子","slug":"200601_FFT","date":"2020-05-30T16:00:00.000Z","updated":"2020-06-10T09:42:31.664Z","comments":true,"path":"2020/05/31/200601_FFT/","link":"","permalink":"http://yoursite.com/2020/05/31/200601_FFT/","excerpt":"","text":"傅里叶变换 VS 离散傅里叶变换","categories":[{"name":"板子","slug":"板子","permalink":"http://yoursite.com/categories/%E6%9D%BF%E5%AD%90/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"板子","slug":"板子","permalink":"http://yoursite.com/tags/%E6%9D%BF%E5%AD%90/"}]},{"title":"优化器","slug":"200530_optimizer","date":"2020-05-29T16:00:00.000Z","updated":"2020-06-10T09:43:12.446Z","comments":true,"path":"2020/05/30/200530_optimizer/","link":"","permalink":"http://yoursite.com/2020/05/30/200530_optimizer/","excerpt":"","text":"深度学习中的优化算法主要围绕梯度下降算法展开，其主要思想是：选取一定的训练样本，按照一定的步长（学习率）沿着梯度的方向调整更新参数，优化模型的目标函数。 SGD随机梯度下降法是每次使用一批数据进行梯度的计算，而非计算全部数据的梯度，因为如果每次计算全部数据的梯度，会导致运算量加大，运算时间变长，容易陷入局部最优解，而随机梯度下降可能每次不是朝着真正最小的方向，这样反而可以跳出局部的最优解。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"优化器","slug":"优化器","permalink":"http://yoursite.com/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"},{"name":"笔记","slug":"笔记","permalink":"http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"}]},{"title":"生成式模型 VS 判别式模型","slug":"200530_Difference_between_generative_model_and_discriminative_model","date":"2020-05-29T16:00:00.000Z","updated":"2020-06-10T09:43:06.321Z","comments":true,"path":"2020/05/30/200530_Difference_between_generative_model_and_discriminative_model/","link":"","permalink":"http://yoursite.com/2020/05/30/200530_Difference_between_generative_model_and_discriminative_model/","excerpt":"","text":"生成式模型 VS 判别式模型Discriminative models learn the classification (hard or soft) boundary between classes. A discriminative model learns the conditional probability distribution p(y|x) - which you should read as “the probability of y given x”. Generative models model the distribution of individual classes. A generative model learns the joint probability distribution p(x,y) 生成模型是模拟这个结果是如何产生的,然后算出产生各个结果的概率。判别模型是发现各个结果之间的不同,不关心产生结果的过程。 典型代表模型生成式模型 朴素贝叶斯 K紧邻（KNN） 混合高斯模型 隐马尔科夫模型（HMM） 贝叶斯网络 Sigmoid Belief Networks 马尔科夫随机场（Markov Random Fields） 深度信念网络（DBN） 判别式模型 线性回归（Linear Regression） 逻辑斯蒂回归（Logistic Regression） 神经网络（NN） 支持向量机（SVM） 高斯过程（Gaussian Process） 条件随机场（CRF） CART(Classification and Regression Tree) 参考[1] On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes [2] The difference between a generative and a discriminative algorithm? [3] 判别式模型与生成式模型","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"笔记","slug":"笔记","permalink":"http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"生成式模型","slug":"生成式模型","permalink":"http://yoursite.com/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/"}]},{"title":"My First Blog","slug":"200527_first","date":"2020-05-26T16:00:00.000Z","updated":"2020-06-08T13:54:05.725Z","comments":true,"path":"2020/05/27/200527_first/","link":"","permalink":"http://yoursite.com/2020/05/27/200527_first/","excerpt":"","text":"欢迎来到zyz的个人网站。这是我的第一篇博客。","categories":[],"tags":[{"name":"test","slug":"test","permalink":"http://yoursite.com/tags/test/"},{"name":"博客","slug":"博客","permalink":"http://yoursite.com/tags/%E5%8D%9A%E5%AE%A2/"}]}],"categories":[{"name":"Speech","slug":"Speech","permalink":"http://yoursite.com/categories/Speech/"},{"name":"toolkit","slug":"toolkit","permalink":"http://yoursite.com/categories/toolkit/"},{"name":"offer","slug":"offer","permalink":"http://yoursite.com/categories/offer/"},{"name":"backends","slug":"backends","permalink":"http://yoursite.com/categories/backends/"},{"name":"flows","slug":"flows","permalink":"http://yoursite.com/categories/flows/"},{"name":"概率论与线性代数","slug":"概率论与线性代数","permalink":"http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"name":"板子","slug":"板子","permalink":"http://yoursite.com/categories/%E6%9D%BF%E5%AD%90/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"},{"name":"语音","slug":"语音","permalink":"http://yoursite.com/tags/%E8%AF%AD%E9%9F%B3/"},{"name":"开发","slug":"开发","permalink":"http://yoursite.com/tags/%E5%BC%80%E5%8F%91/"},{"name":"工具","slug":"工具","permalink":"http://yoursite.com/tags/%E5%B7%A5%E5%85%B7/"},{"name":"效率","slug":"效率","permalink":"http://yoursite.com/tags/%E6%95%88%E7%8E%87/"},{"name":"面试","slug":"面试","permalink":"http://yoursite.com/tags/%E9%9D%A2%E8%AF%95/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"概率论","slug":"概率论","permalink":"http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://yoursite.com/tags/Pytorch/"},{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"板子","slug":"板子","permalink":"http://yoursite.com/tags/%E6%9D%BF%E5%AD%90/"},{"name":"神经网络","slug":"神经网络","permalink":"http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"优化器","slug":"优化器","permalink":"http://yoursite.com/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"},{"name":"生成式模型","slug":"生成式模型","permalink":"http://yoursite.com/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/"},{"name":"test","slug":"test","permalink":"http://yoursite.com/tags/test/"},{"name":"博客","slug":"博客","permalink":"http://yoursite.com/tags/%E5%8D%9A%E5%AE%A2/"}]}