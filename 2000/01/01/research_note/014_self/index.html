<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/icon.png">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="Yang Zhang">
  <meta name="keywords" content="">
  <title>Self-Supervised Pretrain Speech Representation for Speaker Recongnition - zyzisyz</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>zyz's space</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('https://wx1.sbimg.cn/2020/06/09/yourname.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.2)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2000-01-01 00:00">
      2000年1月1日 凌晨
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      2.3k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      31
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>使用预训练模型提取语音声学特征从而提高当前说话人识别系统的性能。</p>
<p>Self-Supervised Pretrain speech representation for Speaker Recogntion</p>
<p>The goal of speech representation learning is to find a transformation from the surface features that makes high-level properties of speech more accessible to downstream tasks.</p>
<p><img src="https://wx2.sbimg.cn/2020/07/12/CuUHw.png" srcset="/img/loading.gif" alt="avatar"></p>
<h3 id="Backgroud"><a href="#Backgroud" class="headerlink" title="Backgroud"></a>Backgroud</h3><p>在说话人识别任务中，第一步大多都是对语音进行<strong>声学特征提取</strong>，从语音信号中提取例如 mfcc, fbank 这样的声学特征。通过提取声学特征，一帧高维度的语音信号(waveform)可以号用一个12~40维向量简洁地表示出来；一整段语音信号，就被表示为这种向量的一个序列。例如，<strong>梅尔刻度 (Mel scale)</strong>是一种基于人耳对等距的音高(pitch)变化的感官判断而定的非线性频率刻度，它与频率关系如下:</p>
<p>$$M(f) = 2595 \cdot log_{10}(1+\frac{f}{700})$$</p>
<p>MFCC 的性能表现可能会受到如下因素的影响: </p>
<ol>
<li>the number of filters</li>
<li>the shape of filters </li>
<li>the way that filters are spaced</li>
<li>the way that the power spectrum is warped</li>
</ol>
<p>这些传统的声学特征是根据人耳的感知机理，例如掩蔽效应，人为设计的特征提取方法。这类方法可能并不适用于计算机的模式识别，使用传统的声学特征用于说话人识别可能存在一定的不合理性。例如:</p>
<ol>
<li>声学特征提取过程中可能会存在一定的信息损失，导致识别性能降低。</li>
<li>声学特征参数的选取往往依赖于人的经验。不同的参数设置，会导致不同的系统识别的性能。</li>
<li>提取特征的过程中可能没有考虑人说话的上下文 context 信息。</li>
</ol>
<h3 id="What-is-self-supervised-learning"><a href="#What-is-self-supervised-learning" class="headerlink" title="What is self-supervised learning?"></a>What is self-supervised learning?</h3><p>最近，self-supervised learning 逐渐成为科研工作者们的关注重点。self-supervised learning 是无监督学习里的一类方法，主要是希望能够<strong>学习到一种通用的特征表达用于下游任务</strong></p>
<p>Yann LeCun in AAAI2020</p>
<blockquote>
<p>in self-supervised learning, the systerm learns to predict part of its input from other parts of it input.</p>
</blockquote>
<ul>
<li>Goal: Learning to represent the world before learning tasks.</li>
<li>Predict any part of the input from any other part</li>
<li>Predict the <strong>future</strong> from the <strong>recent past</strong></li>
<li>Predict the <strong>past</strong> from the <strong>present</strong></li>
<li>Predict the <strong>top</strong> from the <strong>bottom</strong></li>
</ul>
<p>什么是一个好的 Representation ?</p>
<ol>
<li>high-performing on a diverse set of downstream tasks using simple models</li>
<li>useful in transfer learning with small amounts of data for a new task</li>
</ol>
<p>自监督学习已经在 CV 和 NLP 领域取得了极大的成功，以大名鼎鼎的 BERT 模型为例，它利用大规模无标注语料训练、获得文本的包含丰富语义信息的 Representation，然后将文本的语义表示在特定 NLP 任务中作微调后应用于该任务上。</p>
<p>目前self-supervised learning的主要有如下方法：</p>
<p><img src="https://wx1.sbimg.cn/2020/07/12/Cubk8.png" srcset="/img/loading.gif" alt="avatar"></p>
<p>在 speech 领域，也有很多大佬进行了一系列的探索。从 interspeech2020 和 ICML2020 两个学术会议中，我们可以看出未来应该会有更多的大佬加入 self supervised learning 在 speech 领域的研究。</p>
<ol>
<li><a href="https://self-supervised-sp.github.io/Interspeech2020-Special-Session" target="_blank" rel="noopener">Interspeech2020 Special Session: New Trends in self-supervised speech processing</a></li>
<li><a href="https://icml-sas.gitlab.io/" target="_blank" rel="noopener">ICML2020: Self-supervision in Audio and Speech</a></li>
</ol>
<h3 id="Contrastive-Predictive-Coding"><a href="#Contrastive-Predictive-Coding" class="headerlink" title="Contrastive Predictive Coding"></a>Contrastive Predictive Coding</h3><p>Contrastive Predictive Coding (CPC) 是由 Google DeepMind 出品的非常有代表性的自监督学习方法，可以用于语音、图片、文本以及强化学习。<strong>CPC的主要思想就是基于 context 信息的未来数据预测，以及通过采样的方式进行训练。</strong></p>
<p><img src="https://wx2.sbimg.cn/2020/07/20/CfofV.png" srcset="/img/loading.gif" alt="avatar"></p>
<p>CPC 的出发点是最大化context vector $c$ 和数据输入 $x$ 之间的<strong>互信息（Mutual Information）</strong>，使得 $c$ 包含足够的原始数据信息，因而可以作为新的特征表示。互信息定义如下：</p>
<p>$$MI(x;c) = \sum_{x,c}p(x,c)log\frac{p(x|c)}{p(x)}$$</p>
<p>“对比学习”中的“对比”是 positive 样本 和 negative 样本的对比。在学习到的“表示空间”内，增大某样本与其 positive 样本之间的相似度，减少与negative 样本的相似度。CPC 目标是学习一个 encoder $f$</p>
<p>$$\textrm{score}(f(x),f(x^{+})) &gt;&gt; \textrm{score}(f(x), f(x^{-}))$$</p>
<p>score 函数是相似度的度量，例如可以设置为 $\textrm{score}(a, b) = a^Tb$。一般在一个训练批量的 $N$ 个样本中，使用 1 个 positive 样本和 $N-1$ 个 negative 样本。</p>
<p>如果大量无监督但数据采样训练，最大化互信息，我们可以得到数据新的表征。非常值得一提的是，论文中对不同说话人表征在 $t-SNE$ 可视化图中展现出了不错的区分性。</p>
<p><img src="https://wx2.sbimg.cn/2020/07/15/CZBTR.md.png" srcset="/img/loading.gif" alt="avatar"></p>
<h3 id="WAV2VEC"><a href="#WAV2VEC" class="headerlink" title="WAV2VEC"></a>WAV2VEC</h3><p>依据 CPC 论文的思路，FaceBook FAIR 团队提出了 wav2vec 模型利用大量未标记的音频数据预训练的方法用于提高 ASR 系统的性能。预训练模型是在 Fairseq 工具包中的 PyTorch 中实现的。</p>
<p><img src="https://wx2.sbimg.cn/2020/07/15/CZqK2.png" srcset="/img/loading.gif" alt="avatar"></p>
<p>模型由两个卷积神经网络组成:</p>
<ul>
<li><strong>Encoder Network</strong> $x \rightarrow z$ : embeds the audio signal in a latent space</li>
<li><strong>Context Network</strong> $z \rightarrow c$ : combines multiple time-steps of the encoder to obtain contextualized representations</li>
</ul>
<p>与 CPC 原论文里略有不同的是，在 Context Network 上，wav2vec 采用的是卷积神经网络，而 CPC 采用的 Autoregressive Model 自回归模型，因此在 infer 的速度上 wav2vec 会更快。</p>
<p>预训练结束后，将上下文网络 $c_i$ 生成的表示形式作为声学特征输入，而不是使用 log-mel 滤波器组特征。</p>
<h3 id="Self-Supervised-Feature-for-Speaker-Recongnition"><a href="#Self-Supervised-Feature-for-Speaker-Recongnition" class="headerlink" title="Self-Supervised Feature for Speaker Recongnition"></a>Self-Supervised Feature for Speaker Recongnition</h3><p>我想使用 self supervised learning 的方法改进当前说话人识别系统。创新点在于利用了大量无说话人 label 的语音数据去提高当前说话人识别系统的性能</p>
<p>首先通过大量没有任何标注的语音数据，依据语音时序上的上下文 context 信息，使用神经网络去重新学一个更加合理 speech 的 representation。这类 self-supervised features 对语音信号前后关系有更好地建模。可以对于说话人长时性描述更强。之后我们通过预训练好的模型，对语音进行特征提取，新的声学特征将取代 mfcc 和 fbank 这类传统的声学特征，作为说话人模型对输入。我们利用这种新的特征，和传统 i-vector 和 x-vector 表征学习相互结合。</p>
<p>总结来说就是使用预训练模型提取语音声学特征取代 mfcc 和 fbank 从而提高当前说话人识别系统的性能。预期可能取得如下结果:</p>
<ol>
<li>进一步提高当前的说话人识别系统的性能，成为新的STOA。</li>
<li>说话人训练数据在 low resources 的情况下取得更好的性能。例如 self supervised feature＋100个spk数据集训练出来的模型性能约等于甚至好于 mfcc＋500个spk数据集训练出的模型。</li>
<li>降低当前说话人识别模型的参数量，即 x-vector 模型的网络结构不需要很深很复杂也可以达到不错的说话人识别效果。</li>
<li>进一步提高当前 anti-spoofing 系统的性能。</li>
</ol>
<h2 id="My-Experiment"><a href="#My-Experiment" class="headerlink" title="My Experiment"></a>My Experiment</h2><h3 id="Toolkits"><a href="#Toolkits" class="headerlink" title="Toolkits"></a>Toolkits</h3><p>我的实验主要使用了如下两个工具:</p>
<ol>
<li><strong><a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">fairseq</a></strong>: 它是一个由 facebook AI research 团队维护的 seq2seq 模型工具包，wav2vec 是 fairseq 内的一个子项目，其中同时开源了相关的一些预训练模型</li>
<li><strong><a href="https://github.com/kaldi-asr/kaldi" target="_blank" rel="noopener">Kaldi</a></strong>: 它是一个主要由 Danney Povey 等人主要维护的语音工具包，其中包含了经典的说话人识别模型 (i-vector, x-vector等) 的全部 pipline 代码</li>
</ol>
<h3 id="Dataset-amp-Configs"><a href="#Dataset-amp-Configs" class="headerlink" title="Dataset &amp; Configs"></a>Dataset &amp; Configs</h3><p>为了快速验证这个想法，我在一个含有100个说话人的小规模的命令词数据集上，分别使用了几个不同的声学特征提取方法进行了3组实验。我们使用80个说话人进行说话人模型的训练，另外20个人用于测试。具体配置如下：</p>
<h4 id="Group-1-Average-Pooling"><a href="#Group-1-Average-Pooling" class="headerlink" title="Group 1: Average Pooling"></a>Group 1: Average Pooling</h4><ul>
<li><strong>baseline MFCC average</strong>: MFCC feature with average pooling</li>
<li><strong>wav2vec c average</strong>: wav2vec pretrain feature c with average pooling</li>
<li><strong>wav2vec z average</strong>: wav2vec pretrain feature z with average pooling</li>
</ul>
<h4 id="Group-2-i-vector"><a href="#Group-2-i-vector" class="headerlink" title="Group 2: i-vector"></a>Group 2: i-vector</h4><ul>
<li><strong>baseline MFCC i-vector</strong>: MFCC feature with i-vector ASV system</li>
<li><strong>wav2vec c i-vector</strong>: wav2vec pretrain feature c with i-vector ASV system</li>
<li><strong>wav2vec z i-vector</strong>: wav2vec pretrain feature z with i-vector ASV system</li>
</ul>
<h4 id="Group-3-x-vector"><a href="#Group-3-x-vector" class="headerlink" title="Group 3: x-vector"></a>Group 3: x-vector</h4><ul>
<li><strong>baseline MFCC x-vector</strong>: MFCC feature with x-vector ASV system</li>
<li><strong>wav2vec c x-vector</strong>: wav2vec pretrain feature c with x-vector ASV system</li>
<li><strong>wav2vec z x-vector</strong>: wav2vec pretrain feature z with x-vector ASV system</li>
</ul>
<h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><h4 id="Group-1-Average-Pooling-1"><a href="#Group-1-Average-Pooling-1" class="headerlink" title="Group 1: Average Pooling"></a>Group 1: Average Pooling</h4><table>
<thead>
<tr>
<th align="center">Equel Error Rate</th>
<th align="center">MFCC average</th>
<th align="center">wav2vec c average</th>
<th align="center">wav2vec z average</th>
</tr>
</thead>
<tbody><tr>
<td align="center">cosine</td>
<td align="center"><strong>24.59 %</strong></td>
<td align="center">33.52 %</td>
<td align="center">27.51 %</td>
</tr>
<tr>
<td align="center">PLDA</td>
<td align="center">11.68 %</td>
<td align="center">8.697 %</td>
<td align="center"><strong>5.92 %</strong></td>
</tr>
<tr>
<td align="center">LDA PLDA</td>
<td align="center">13.25 %</td>
<td align="center">8.614 %</td>
<td align="center"><strong>5.981 %</strong></td>
</tr>
</tbody></table>
<h4 id="Group-2-i-vector-1"><a href="#Group-2-i-vector-1" class="headerlink" title="Group 2: i-vector"></a>Group 2: i-vector</h4><table>
<thead>
<tr>
<th align="center">Equel Error Rate</th>
<th align="center">MFCC i-vector</th>
<th align="center">wav2vec c i-vector</th>
<th align="center">wav2vec z i-vector</th>
</tr>
</thead>
<tbody><tr>
<td align="center">cosine</td>
<td align="center">19.2 %</td>
<td align="center">22.23 %</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">PLDA</td>
<td align="center">10.45 %</td>
<td align="center">10.35 %</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">LDA PLDA</td>
<td align="center">9.972 %</td>
<td align="center">10.4 %</td>
<td align="center"></td>
</tr>
</tbody></table>
<h4 id="Group-3-x-vector-1"><a href="#Group-3-x-vector-1" class="headerlink" title="Group 3: x-vector"></a>Group 3: x-vector</h4><table>
<thead>
<tr>
<th align="center">Equel Error Rate</th>
<th align="center">MFCC x-vector</th>
<th align="center">wav2vec c x-vector</th>
<th align="center">wav2vec z x-vector</th>
</tr>
</thead>
<tbody><tr>
<td align="center">cosine</td>
<td align="center">12.23 %</td>
<td align="center">17.32 %</td>
<td align="center"><strong>11.71 %</strong></td>
</tr>
<tr>
<td align="center">PLDA</td>
<td align="center">12.04 %</td>
<td align="center">15.53 %</td>
<td align="center"><strong>8.926 %</strong></td>
</tr>
<tr>
<td align="center">LDA PLDA</td>
<td align="center">11.48 %</td>
<td align="center">11.71 %</td>
<td align="center"><strong>8.784 %</strong></td>
</tr>
</tbody></table>
<h4 id="T-SNE"><a href="#T-SNE" class="headerlink" title="T-SNE"></a>T-SNE</h4><p><img src="https://wx1.sbimg.cn/2020/07/24/DEeCM.png" srcset="/img/loading.gif" alt="avatar"></p>
<h3 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h3><ol>
<li><strong>Dataset</strong>: VoxCeleb dataset</li>
<li><strong>Dataset</strong>: VAD &amp; Data Arguement</li>
<li><strong>Pretrain</strong>: center loss for pretrain</li>
<li><strong>Pretrain</strong>: triplet loss for pretrain</li>
</ol>
<h2 id="Related-Research"><a href="#Related-Research" class="headerlink" title="Related Research"></a>Related Research</h2><h3 id="Speech-Pretrain-Papers"><a href="#Speech-Pretrain-Papers" class="headerlink" title="Speech Pretrain Papers"></a>Speech Pretrain Papers</h3><ol>
<li><a href="https://arxiv.org/abs/1904.05862" target="_blank" rel="noopener">[Facebook] wav2vec: Unsupervised Pre-Training for Speech Recognition, 2019</a></li>
<li><a href="https://arxiv.org/abs/2002.12764" target="_blank" rel="noopener">[Google] Towards Learning a Universal Non-Semantic Representation of Speech, 2020</a></li>
<li><a href="https://arxiv.org/abs/1807.03748" target="_blank" rel="noopener">[DeepMind] Representation Learning with Contrastive Predictive Coding, 2019</a></li>
<li><a href="https://arxiv.org/abs/1904.03240" target="_blank" rel="noopener">[MIT] An Unsupervised Autoregressive Model for Speech Representation Learning, 2019</a></li>
<li><a href="https://arxiv.org/abs/1912.01679" target="_blank" rel="noopener">[Amazon] Deep Contextualized Acoustic Representations for Semi-Supervised Speech Recognition, 2020</a></li>
<li><a href="https://arxiv.org/abs/1910.12638" target="_blank" rel="noopener">[NTU] MOCKINGJAY: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders, 2020</a></li>
</ol>
<h3 id="Speaker-Related-Papers"><a href="#Speaker-Related-Papers" class="headerlink" title="Speaker Related Papers"></a>Speaker Related Papers</h3><ol>
<li><a href="https://arxiv.org/abs/1812.00271" target="_blank" rel="noopener">[Yoshua Bengio] Learning Speaker Representations with Mutual Information, 2018</a></li>
<li><a href="https://arxiv.org/abs/2002.12764" target="_blank" rel="noopener">[Google] Towards Learning a Universal Non-Semantic Representation of Speech, 2020</a></li>
<li><a href="https://arxiv.org/abs/1904.03240" target="_blank" rel="noopener">[MIT] An Unsupervised Autoregressive Model for Speech Representation Learning, 2019</a></li>
</ol>
<h3 id="Pretrain-Related-Toolkits-amp-Code"><a href="#Pretrain-Related-Toolkits-amp-Code" class="headerlink" title="Pretrain Related Toolkits &amp; Code"></a>Pretrain Related Toolkits &amp; Code</h3><ol>
<li><a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">[FaceBook] fairseq</a></li>
<li><a href="https://github.com/awslabs/speech-representations" target="_blank" rel="noopener">[Amazon] speech-representations</a></li>
<li><a href="https://github.com/andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" target="_blank" rel="noopener">[NTU] MOCKINGJAY</a></li>
<li><a href="https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark" target="_blank" rel="noopener">[Google] non semantic speech benchmark</a></li>
</ol>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html" target="_blank" rel="noopener">ankeshanand Contrative SSL blog</a></li>
<li><a href="https://self-supervised-sp.github.io/Interspeech2020-Special-Session" target="_blank" rel="noopener">Interspeech2020 Special Session: New Trends in self-supervised speech processing</a></li>
<li><a href="https://icml-sas.gitlab.io/" target="_blank" rel="noopener">ICML2020: Self-supervision in Audio and Speech</a></li>
</ol>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
              </div>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2000/01/01/research_note/014_self_cvss/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">【CSLT-THU cvss SELF】</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2000/01/01/research_note/15_cpc_asv_work/">
                        <span class="hidden-mobile">CPC Work Plan</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 1,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "Self-Supervised Pretrain Speech Representation for Speaker Recongnition&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script>

  










  <script  src="https://cdn.staticfile.org/mermaid/8.5.0/mermaid.min.js" ></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({"theme":"default"});
    }
  </script>







</body>
</html>
