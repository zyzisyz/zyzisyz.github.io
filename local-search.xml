<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Git Flight Rules</title>
    <link href="/2020/11/28/2020/201128_git_fight_rules/"/>
    <url>/2020/11/28/2020/201128_git_fight_rules/</url>
    
    <content type="html"><![CDATA[<h1 id="Git飞行规则-Flight-Rules"><a href="#Git飞行规则-Flight-Rules" class="headerlink" title="Git飞行规则(Flight Rules)"></a><a href="https://github.com/k88hudson/git-flight-rules/blob/master/README_zh-CN.md" target="_blank" rel="noopener">Git飞行规则(Flight Rules)</a></h1><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><ul><li>英文原版<a href="https://github.com/k88hudson/git-flight-rules/blob/master/README.md" target="_blank" rel="noopener">README</a></li><li>翻译可能存在错误或不标准的地方，欢迎大家指正和修改，谢谢！</li></ul><h4 id="什么是”飞行规则”"><a href="#什么是”飞行规则”" class="headerlink" title="什么是”飞行规则”?"></a>什么是”飞行规则”?</h4><p>这是一篇给宇航员（这里就是指使用Git的程序员们）的指南，用来指导问题出现后的应对之法。</p><blockquote><p> <em>飞行规则(Flight Rules)</em> 是记录在手册上的来之不易的一系列知识，记录了某个事情发生的原因，以及怎样一步一步的进行处理。本质上, 它们是特定场景的非常详细的标准处理流程。 […]</p></blockquote><blockquote><p>自20世纪60年代初以来，NASA一直在捕捉(capturing)我们的失误，灾难和解决方案, 当时水星时代(Mercury-era)的地面小组首先开始将“经验教训”收集到一个纲要(compendium)中，该纲现在已经有上千个问题情景，从发动机故障到破损的舱口把手到计算机故障，以及它们对应的解决方案。</p></blockquote><p>&mdash; Chris Hadfield, <em>一个宇航员的生活指南(An Astronaut’s Guide to Life)</em>。</p><h4 id="这篇文章的约定"><a href="#这篇文章的约定" class="headerlink" title="这篇文章的约定"></a>这篇文章的约定</h4><p>为了清楚的表述，这篇文档里的所有例子使用了自定义的bash 提示，以便指示当前分支和是否有暂存的变化(changes)。分支名用小括号括起来，分支名后面跟的<code>*</code>表示暂存的变化(changes)。</p><p><a href="https://gitter.im/k88hudson/git-flight-rules?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge" target="_blank" rel="noopener"><img src="https://badges.gitter.im/Join%20Chat.svg" srcset="/img/loading.gif" alt="Join the chat at https://gitter.im/k88hudson/git-flight-rules"></a></p><!-- START doctoc generated TOC please keep comment here to allow auto update --><!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --><p><strong>Table of Contents</strong>  <em>generated with <a href="https://github.com/thlorenz/doctoc" target="_blank" rel="noopener">DocToc</a></em></p><ul><li><a href="#%E7%BC%96%E8%BE%91%E6%8F%90%E4%BA%A4editting-commits">编辑提交(editting commits)</a><ul><li><a href="#%E6%88%91%E5%88%9A%E6%89%8D%E6%8F%90%E4%BA%A4%E4%BA%86%E4%BB%80%E4%B9%88">我刚才提交了什么?</a></li><li><a href="#%E6%88%91%E7%9A%84%E6%8F%90%E4%BA%A4%E4%BF%A1%E6%81%AFcommit-message%E5%86%99%E9%94%99%E4%BA%86">我的提交信息(commit message)写错了</a></li><li><a href="#%E6%88%91%E6%8F%90%E4%BA%A4commit%E9%87%8C%E7%9A%84%E7%94%A8%E6%88%B7%E5%90%8D%E5%92%8C%E9%82%AE%E7%AE%B1%E4%B8%8D%E5%AF%B9">我提交(commit)里的用户名和邮箱不对</a></li><li><a href="#%E6%88%91%E6%83%B3%E4%BB%8E%E4%B8%80%E4%B8%AA%E6%8F%90%E4%BA%A4commit%E9%87%8C%E7%A7%BB%E9%99%A4%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6">我想从一个提交(commit)里移除一个文件</a></li><li><a href="#%E6%88%91%E6%83%B3%E5%88%A0%E9%99%A4%E6%88%91%E7%9A%84%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E6%AC%A1%E6%8F%90%E4%BA%A4commit">我想删除我的的最后一次提交(commit)</a></li><li><a href="#%E5%88%A0%E9%99%A4%E4%BB%BB%E6%84%8F%E6%8F%90%E4%BA%A4commit">删除任意提交(commit)</a></li><li><a href="#%E6%88%91%E5%B0%9D%E8%AF%95%E6%8E%A8%E4%B8%80%E4%B8%AA%E4%BF%AE%E6%AD%A3%E5%90%8E%E7%9A%84%E6%8F%90%E4%BA%A4amended-commit%E5%88%B0%E8%BF%9C%E7%A8%8B%E4%BD%86%E6%98%AF%E6%8A%A5%E9%94%99">我尝试推一个修正后的提交(amended commit)到远程，但是报错：</a></li><li><a href="#%E6%88%91%E6%84%8F%E5%A4%96%E7%9A%84%E5%81%9A%E4%BA%86%E4%B8%80%E6%AC%A1%E7%A1%AC%E9%87%8D%E7%BD%AEhard-reset%E6%88%91%E6%83%B3%E6%89%BE%E5%9B%9E%E6%88%91%E7%9A%84%E5%86%85%E5%AE%B9">我意外的做了一次硬重置(hard reset)，我想找回我的内容</a></li></ul></li><li><a href="#%E6%9A%82%E5%AD%98staging">暂存(Staging)</a><ul><li><a href="#%E6%88%91%E9%9C%80%E8%A6%81%E6%8A%8A%E6%9A%82%E5%AD%98%E7%9A%84%E5%86%85%E5%AE%B9%E6%B7%BB%E5%8A%A0%E5%88%B0%E4%B8%8A%E4%B8%80%E6%AC%A1%E7%9A%84%E6%8F%90%E4%BA%A4commit">我需要把暂存的内容添加到上一次的提交(commit)</a></li><li><a href="#%E6%88%91%E6%83%B3%E8%A6%81%E6%9A%82%E5%AD%98%E4%B8%80%E4%B8%AA%E6%96%B0%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%80%E9%83%A8%E5%88%86%E8%80%8C%E4%B8%8D%E6%98%AF%E8%BF%99%E4%B8%AA%E6%96%87%E4%BB%B6%E7%9A%84%E5%85%A8%E9%83%A8">我想要暂存一个新文件的一部分，而不是这个文件的全部</a></li><li><a href="#%E6%88%91%E6%83%B3%E6%8A%8A%E5%9C%A8%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E9%87%8C%E7%9A%84%E5%8F%98%E5%8C%96changes%E5%8A%A0%E5%88%B0%E4%B8%A4%E4%B8%AA%E6%8F%90%E4%BA%A4commit%E9%87%8C">我想把在一个文件里的变化(changes)加到两个提交(commit)里</a></li><li><a href="#%E6%88%91%E6%83%B3%E6%8A%8A%E6%9A%82%E5%AD%98%E7%9A%84%E5%86%85%E5%AE%B9%E5%8F%98%E6%88%90%E6%9C%AA%E6%9A%82%E5%AD%98%E6%8A%8A%E6%9C%AA%E6%9A%82%E5%AD%98%E7%9A%84%E5%86%85%E5%AE%B9%E6%9A%82%E5%AD%98%E8%B5%B7%E6%9D%A5">我想把暂存的内容变成未暂存，把未暂存的内容暂存起来</a></li></ul></li><li><a href="#%E6%9C%AA%E6%9A%82%E5%AD%98unstaged%E7%9A%84%E5%86%85%E5%AE%B9">未暂存(Unstaged)的内容</a><ul><li><a href="#%E6%88%91%E6%83%B3%E6%8A%8A%E6%9C%AA%E6%9A%82%E5%AD%98%E7%9A%84%E5%86%85%E5%AE%B9%E7%A7%BB%E5%8A%A8%E5%88%B0%E4%B8%80%E4%B8%AA%E6%96%B0%E5%88%86%E6%94%AF">我想把未暂存的内容移动到一个新分支</a></li><li><a href="#%E6%88%91%E6%83%B3%E6%8A%8A%E6%9C%AA%E6%9A%82%E5%AD%98%E7%9A%84%E5%86%85%E5%AE%B9%E7%A7%BB%E5%8A%A8%E5%88%B0%E5%8F%A6%E4%B8%80%E4%B8%AA%E5%B7%B2%E5%AD%98%E5%9C%A8%E7%9A%84%E5%88%86%E6%94%AF">我想把未暂存的内容移动到另一个已存在的分支</a></li><li><a href="#%E6%88%91%E6%83%B3%E4%B8%A2%E5%BC%83%E6%9C%AC%E5%9C%B0%E6%9C%AA%E6%8F%90%E4%BA%A4%E7%9A%84%E5%8F%98%E5%8C%96uncommitted-changes">我想丢弃本地未提交的变化(uncommitted changes)</a></li><li><a href="#%E6%88%91%E6%83%B3%E4%B8%A2%E5%BC%83%E6%9F%90%E4%BA%9B%E6%9C%AA%E6%9A%82%E5%AD%98%E7%9A%84%E5%86%85%E5%AE%B9">我想丢弃某些未暂存的内容</a></li></ul></li><li><a href="#%E5%88%86%E6%94%AFbranches">分支(Branches)</a><ul><li><a href="#%E6%88%91%E4%BB%8E%E9%94%99%E8%AF%AF%E7%9A%84%E5%88%86%E6%94%AF%E6%8B%89%E5%8F%96%E4%BA%86%E5%86%85%E5%AE%B9%E6%88%96%E6%8A%8A%E5%86%85%E5%AE%B9%E6%8B%89%E5%8F%96%E5%88%B0%E4%BA%86%E9%94%99%E8%AF%AF%E7%9A%84%E5%88%86%E6%94%AF">我从错误的分支拉取了内容，或把内容拉取到了错误的分支</a></li><li><a href="#%E6%88%91%E6%83%B3%E6%89%94%E6%8E%89%E6%9C%AC%E5%9C%B0%E7%9A%84%E6%8F%90%E4%BA%A4commit%E4%BB%A5%E4%BE%BF%E6%88%91%E7%9A%84%E5%88%86%E6%94%AF%E4%B8%8E%E8%BF%9C%E7%A8%8B%E7%9A%84%E4%BF%9D%E6%8C%81%E4%B8%80%E8%87%B4">我想扔掉本地的提交(commit)，以便我的分支与远程的保持一致</a></li><li><a href="#%E6%88%91%E9%9C%80%E8%A6%81%E6%8F%90%E4%BA%A4%E5%88%B0%E4%B8%80%E4%B8%AA%E6%96%B0%E5%88%86%E6%94%AF%E4%BD%86%E9%94%99%E8%AF%AF%E7%9A%84%E6%8F%90%E4%BA%A4%E5%88%B0%E4%BA%86master">我需要提交到一个新分支，但错误的提交到了master</a></li><li><a href="#%E6%88%91%E6%83%B3%E4%BF%9D%E7%95%99%E6%9D%A5%E8%87%AA%E5%8F%A6%E5%A4%96%E4%B8%80%E4%B8%AAref-ish%E7%9A%84%E6%95%B4%E4%B8%AA%E6%96%87%E4%BB%B6">我想保留来自另外一个ref-ish的整个文件</a></li><li><a href="#%E6%88%91%E6%8A%8A%E5%87%A0%E4%B8%AA%E6%8F%90%E4%BA%A4commit%E6%8F%90%E4%BA%A4%E5%88%B0%E4%BA%86%E5%90%8C%E4%B8%80%E4%B8%AA%E5%88%86%E6%94%AF%E8%80%8C%E8%BF%99%E4%BA%9B%E6%8F%90%E4%BA%A4%E5%BA%94%E8%AF%A5%E5%88%86%E5%B8%83%E5%9C%A8%E4%B8%8D%E5%90%8C%E7%9A%84%E5%88%86%E6%94%AF%E9%87%8C">我把几个提交(commit)提交到了同一个分支，而这些提交应该分布在不同的分支里</a></li><li><a href="#%E6%88%91%E6%83%B3%E5%88%A0%E9%99%A4%E4%B8%8A%E6%B8%B8upstream%E5%88%86%E6%94%AF%E8%A2%AB%E5%88%A0%E9%99%A4%E4%BA%86%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%88%86%E6%94%AF">我想删除上游(upstream)分支被删除了的本地分支</a></li><li><a href="#%E6%88%91%E4%B8%8D%E5%B0%8F%E5%BF%83%E5%88%A0%E9%99%A4%E4%BA%86%E6%88%91%E7%9A%84%E5%88%86%E6%94%AF">我不小心删除了我的分支</a></li><li><a href="#%E6%88%91%E6%83%B3%E5%88%A0%E9%99%A4%E4%B8%80%E4%B8%AA%E5%88%86%E6%94%AF">我想删除一个分支</a></li><li><a href="#%E6%88%91%E6%83%B3%E4%BB%8E%E5%88%AB%E4%BA%BA%E6%AD%A3%E5%9C%A8%E5%B7%A5%E4%BD%9C%E7%9A%84%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF%E7%AD%BE%E5%87%BAcheckout%E4%B8%80%E4%B8%AA%E5%88%86%E6%94%AF">我想从别人正在工作的远程分支签出(checkout)一个分支</a></li></ul></li><li><a href="#rebasing-%E5%92%8C%E5%90%88%E5%B9%B6merging">Rebasing 和合并(Merging)</a><ul><li><a href="#%E6%88%91%E6%83%B3%E6%92%A4%E9%94%80rebasemerge">我想撤销rebase/merge</a></li><li><a href="#%E6%88%91%E5%B7%B2%E7%BB%8Frebase%E8%BF%87-%E4%BD%86%E6%98%AF%E6%88%91%E4%B8%8D%E6%83%B3%E5%BC%BA%E6%8E%A8force-push">我已经rebase过, 但是我不想强推(force push)</a></li><li><a href="#%E6%88%91%E9%9C%80%E8%A6%81%E7%BB%84%E5%90%88combine%E5%87%A0%E4%B8%AA%E6%8F%90%E4%BA%A4commit">我需要组合(combine)几个提交(commit)</a><ul><li><a href="#%E5%AE%89%E5%85%A8%E5%90%88%E5%B9%B6merging%E7%AD%96%E7%95%A5">安全合并(merging)策略</a></li><li><a href="#%E6%88%91%E9%9C%80%E8%A6%81%E5%B0%86%E4%B8%80%E4%B8%AA%E5%88%86%E6%94%AF%E5%90%88%E5%B9%B6%E6%88%90%E4%B8%80%E4%B8%AA%E6%8F%90%E4%BA%A4commit">我需要将一个分支合并成一个提交(commit)</a></li><li><a href="#%E6%88%91%E5%8F%AA%E6%83%B3%E7%BB%84%E5%90%88combine%E6%9C%AA%E6%8E%A8%E7%9A%84%E6%8F%90%E4%BA%A4unpushed-commit">我只想组合(combine)未推的提交(unpushed commit)</a></li></ul></li><li><a href="#%E6%A3%80%E6%9F%A5%E6%98%AF%E5%90%A6%E5%88%86%E6%94%AF%E4%B8%8A%E7%9A%84%E6%89%80%E6%9C%89%E6%8F%90%E4%BA%A4commit%E9%83%BD%E5%90%88%E5%B9%B6merge%E8%BF%87%E4%BA%86">检查是否分支上的所有提交(commit)都合并(merge)过了</a></li><li><a href="#%E4%BA%A4%E4%BA%92%E5%BC%8Frebaseinteractive-rebase%E5%8F%AF%E8%83%BD%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98">交互式rebase(interactive rebase)可能出现的问题</a><ul><li><a href="#%E8%BF%99%E4%B8%AArebase-%E7%BC%96%E8%BE%91%E5%B1%8F%E5%B9%95%E5%87%BA%E7%8E%B0noop">这个rebase 编辑屏幕出现’noop’</a></li><li><a href="#%E6%9C%89%E5%86%B2%E7%AA%81%E7%9A%84%E6%83%85%E5%86%B5">有冲突的情况</a></li></ul></li></ul></li><li><a href="#stash">Stash</a><ul><li><a href="#%E6%9A%82%E5%AD%98%E6%89%80%E6%9C%89%E6%94%B9%E5%8A%A8">暂存所有改动</a></li><li><a href="#%E6%9A%82%E5%AD%98%E6%8C%87%E5%AE%9A%E6%96%87%E4%BB%B6">暂存指定文件</a></li><li><a href="#%E6%9A%82%E5%AD%98%E6%97%B6%E8%AE%B0%E5%BD%95%E6%B6%88%E6%81%AF">暂存时记录消息</a></li><li><a href="#%E4%BD%BF%E7%94%A8%E6%9F%90%E4%B8%AA%E6%8C%87%E5%AE%9A%E6%9A%82%E5%AD%98">使用某个指定暂存</a></li><li><a href="#%E6%9A%82%E5%AD%98%E6%97%B6%E4%BF%9D%E7%95%99%E6%9C%AA%E6%9A%82%E5%AD%98%E7%9A%84%E5%86%85%E5%AE%B9">暂存时保留未暂存的内容</a></li></ul></li><li><a href="#%E6%9D%82%E9%A1%B9miscellaneous-objects">杂项(Miscellaneous Objects)</a><ul><li><a href="#%E5%85%8B%E9%9A%86%E6%89%80%E6%9C%89%E5%AD%90%E6%A8%A1%E5%9D%97">克隆所有子模块</a></li><li><a href="#%E5%88%A0%E9%99%A4%E6%A0%87%E7%AD%BEtag">删除标签(tag)</a></li><li><a href="#%E6%81%A2%E5%A4%8D%E5%B7%B2%E5%88%A0%E9%99%A4%E6%A0%87%E7%AD%BEtag">恢复已删除标签(tag)</a></li><li><a href="#%E5%B7%B2%E5%88%A0%E9%99%A4%E8%A1%A5%E4%B8%81patch">已删除补丁(patch)</a></li></ul></li><li><a href="#%E8%B7%9F%E8%B8%AA%E6%96%87%E4%BB%B6tracking-files">跟踪文件(Tracking Files)</a><ul><li><a href="#%E6%88%91%E5%8F%AA%E6%83%B3%E6%94%B9%E5%8F%98%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E5%90%8D%E5%AD%97%E7%9A%84%E5%A4%A7%E5%B0%8F%E5%86%99%E8%80%8C%E4%B8%8D%E4%BF%AE%E6%94%B9%E5%86%85%E5%AE%B9">我只想改变一个文件名字的大小写，而不修改内容</a></li><li><a href="#%E6%88%91%E6%83%B3%E4%BB%8Egit%E5%88%A0%E9%99%A4%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E4%BD%86%E4%BF%9D%E7%95%99%E8%AF%A5%E6%96%87%E4%BB%B6">我想从Git删除一个文件，但保留该文件</a></li></ul></li><li><a href="#%E9%85%8D%E7%BD%AEconfiguration">配置(Configuration)</a><ul><li><a href="#%E6%88%91%E6%83%B3%E7%BB%99%E4%B8%80%E4%BA%9Bgit%E5%91%BD%E4%BB%A4%E6%B7%BB%E5%8A%A0%E5%88%AB%E5%90%8Dalias">我想给一些Git命令添加别名(alias)</a></li><li><a href="#%E6%88%91%E6%83%B3%E7%BC%93%E5%AD%98%E4%B8%80%E4%B8%AA%E4%BB%93%E5%BA%93repository%E7%9A%84%E7%94%A8%E6%88%B7%E5%90%8D%E5%92%8C%E5%AF%86%E7%A0%81">我想缓存一个仓库(repository)的用户名和密码</a></li></ul></li><li><a href="#%E6%88%91%E4%B8%8D%E7%9F%A5%E9%81%93%E6%88%91%E5%81%9A%E9%94%99%E4%BA%86%E4%BA%9B%E4%BB%80%E4%B9%88">我不知道我做错了些什么</a><ul><li><a href="#%E5%85%B6%E5%AE%83%E8%B5%84%E6%BA%90other-resources">其它资源(Other Resources)</a></li></ul></li><li><a href="#%E4%B9%A6books">书(Books)</a></li><li><a href="#%E6%95%99%E7%A8%8Btutorials">教程(Tutorials)</a></li><li><a href="#%E8%84%9A%E6%9C%AC%E5%92%8C%E5%B7%A5%E5%85%B7scripts-and-tools">脚本和工具(Scripts and Tools)</a></li><li><a href="#gui%E5%AE%A2%E6%88%B7%E7%AB%AFgui-clients">GUI客户端(GUI Clients)</a></li></ul><!-- END doctoc generated TOC please keep comment here to allow auto update --><h2 id="编辑提交-editting-commits"><a href="#编辑提交-editting-commits" class="headerlink" title="编辑提交(editting commits)"></a>编辑提交(editting commits)</h2><p><a name="diff-last"></a></p><h3 id="我刚才提交了什么"><a href="#我刚才提交了什么" class="headerlink" title="我刚才提交了什么?"></a>我刚才提交了什么?</h3><p>如果你用 <code>git commit -a</code> 提交了一次变化(changes)，而你又不确定到底这次提交了哪些内容。 你就可以用下面的命令显示当前<code>HEAD</code>上的最近一次的提交(commit):</p><pre><code class="hljs sh">(master)$ git show</code></pre><p>或者</p><pre><code class="hljs sh">$ git <span class="hljs-built_in">log</span> -n1 -p</code></pre><p><a name="#i-wrote-the-wrong-thing-in-a-commit-message"></a></p><h3 id="我的提交信息-commit-message-写错了"><a href="#我的提交信息-commit-message-写错了" class="headerlink" title="我的提交信息(commit message)写错了"></a>我的提交信息(commit message)写错了</h3><p>如果你的提交信息(commit message)写错了且这次提交(commit)还没有推(push), 你可以通过下面的方法来修改提交信息(commit message):</p><pre><code class="hljs sh">$ git commit --amend --only</code></pre><p>这会打开你的默认编辑器, 在这里你可以编辑信息. 另一方面, 你也可以用一条命令一次完成:</p><pre><code class="hljs sh">$ git commit --amend --only -m <span class="hljs-string">'xxxxxxx'</span></code></pre><p>如果你已经推(push)了这次提交(commit), 你可以修改这次提交(commit)然后强推(force push), 但是不推荐这么做。</p><p><a name="commit-wrong-author"></a></p><h3 id="我提交-commit-里的用户名和邮箱不对"><a href="#我提交-commit-里的用户名和邮箱不对" class="headerlink" title="我提交(commit)里的用户名和邮箱不对"></a>我提交(commit)里的用户名和邮箱不对</h3><p>如果这只是单个提交(commit)，修改它：</p><pre><code class="hljs sh">$ git commit --amend --author <span class="hljs-string">"New Authorname &lt;authoremail@mydomain.com&gt;"</span></code></pre><p>如果你需要修改所有历史, 参考 ‘git filter-branch’的指南页.</p><p><a href="#i-want-to-remove-a-file-from-a-commit"></a></p><h3 id="我想从一个提交-commit-里移除一个文件"><a href="#我想从一个提交-commit-里移除一个文件" class="headerlink" title="我想从一个提交(commit)里移除一个文件"></a>我想从一个提交(commit)里移除一个文件</h3><p>通过下面的方法，从一个提交(commit)里移除一个文件:</p><pre><code class="hljs sh">$ git checkout HEAD^ myfile$ git add -A$ git commit --amend</code></pre><p>这将非常有用，当你有一个开放的补丁(open patch)，你往上面提交了一个不必要的文件，你需要强推(force push)去更新这个远程补丁。</p><p><a name="delete-pushed-commit"></a></p><h3 id="我想删除我的的最后一次提交-commit"><a href="#我想删除我的的最后一次提交-commit" class="headerlink" title="我想删除我的的最后一次提交(commit)"></a>我想删除我的的最后一次提交(commit)</h3><p>如果你需要删除推了的提交(pushed commits)，你可以使用下面的方法。可是，这会不可逆的改变你的历史，也会搞乱那些已经从该仓库拉取(pulled)了的人的历史。简而言之，如果你不是很确定，千万不要这么做。</p><pre><code class="hljs sh">$ git reset HEAD^ --hard$ git push -f [remote] [branch]</code></pre><p>如果你还没有推到远程, 把Git重置(reset)到你最后一次提交前的状态就可以了(同时保存暂存的变化):</p><pre><code class="hljs perl">(<span class="hljs-keyword">my</span>-branch*)$ git <span class="hljs-keyword">reset</span> --soft HEAD@&#123;1&#125;</code></pre><p>这只能在没有推送之前有用. 如果你已经推了, 唯一安全能做的是 <code>git revert SHAofBadCommit</code>， 那会创建一个新的提交(commit)用于撤消前一个提交的所有变化(changes)； 或者, 如果你推的这个分支是rebase-safe的 (例如： 其它开发者不会从这个分支拉), 只需要使用 <code>git push -f</code>； 更多, 请参考 <a href="#deleteremove-last-pushed-commit">the above section</a>。</p><p><a name="delete-any-commit"></a></p><h3 id="删除任意提交-commit"><a href="#删除任意提交-commit" class="headerlink" title="删除任意提交(commit)"></a>删除任意提交(commit)</h3><p>同样的警告：不到万不得已的时候不要这么做.</p><pre><code class="hljs sh">$ git rebase --onto SHA1_OF_BAD_COMMIT^ SHA1_OF_BAD_COMMIT$ git push -f [remote] [branch]</code></pre><p>或者做一个 <a href="#interactive-rebase">交互式rebase</a> 删除那些你想要删除的提交(commit)里所对应的行。</p><p><a name="#force-push"></a></p><h3 id="我尝试推一个修正后的提交-amended-commit-到远程，但是报错："><a href="#我尝试推一个修正后的提交-amended-commit-到远程，但是报错：" class="headerlink" title="我尝试推一个修正后的提交(amended commit)到远程，但是报错："></a>我尝试推一个修正后的提交(amended commit)到远程，但是报错：</h3><pre><code class="hljs sh">To https://github.com/yourusername/repo.git! [rejected]        mybranch -&gt; mybranch (non-fast-forward)error: failed to push some refs to <span class="hljs-string">'https://github.com/tanay1337/webmaker.org.git'</span>hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Integrate the remote changes (e.g.hint: <span class="hljs-string">'git pull ...'</span>) before pushing again.hint: See the <span class="hljs-string">'Note about fast-forwards'</span> <span class="hljs-keyword">in</span> <span class="hljs-string">'git push --help'</span> <span class="hljs-keyword">for</span> details.</code></pre><p>注意, rebasing(见下面)和修正(amending)会用一个<strong>新的提交(commit)代替旧的</strong>, 所以如果之前你已经往远程仓库上推过一次修正前的提交(commit)，那你现在就必须强推(force push) (<code>-f</code>)。 注意 &ndash; <em>总是</em> 确保你指明一个分支!</p><pre><code class="hljs sh">(my-branch)$ git push origin mybranch -f</code></pre><p>一般来说, <strong>要避免强推</strong>. 最好是创建和推(push)一个新的提交(commit)，而不是强推一个修正后的提交。后者会使那些与该分支或该分支的子分支工作的开发者，在源历史中产生冲突。</p><p><a href="undo-git-reset-hard"></a></p><h3 id="我意外的做了一次硬重置-hard-reset-，我想找回我的内容"><a href="#我意外的做了一次硬重置-hard-reset-，我想找回我的内容" class="headerlink" title="我意外的做了一次硬重置(hard reset)，我想找回我的内容"></a>我意外的做了一次硬重置(hard reset)，我想找回我的内容</h3><p>如果你意外的做了 <code>git reset --hard</code>, 你通常能找回你的提交(commit), 因为Git对每件事都会有日志，且都会保存几天。</p><pre><code class="hljs sh">(master)$ git reflog</code></pre><p>你将会看到一个你过去提交(commit)的列表, 和一个重置的提交。 选择你想要回到的提交(commit)的SHA，再重置一次:</p><pre><code class="hljs sh">(master)$ git reset --hard SHA1234</code></pre><p>这样就完成了。</p><h2 id="暂存-Staging"><a href="#暂存-Staging" class="headerlink" title="暂存(Staging)"></a>暂存(Staging)</h2><p><a href="#i-need-to-add-staged-changes-to-the-previous-commit"></a></p><h3 id="我需要把暂存的内容添加到上一次的提交-commit"><a href="#我需要把暂存的内容添加到上一次的提交-commit" class="headerlink" title="我需要把暂存的内容添加到上一次的提交(commit)"></a>我需要把暂存的内容添加到上一次的提交(commit)</h3><pre><code class="hljs sh">(my-branch*)$ git commit --amend</code></pre><p><a name="commit-partial-new-file"></a></p><h3 id="我想要暂存一个新文件的一部分，而不是这个文件的全部"><a href="#我想要暂存一个新文件的一部分，而不是这个文件的全部" class="headerlink" title="我想要暂存一个新文件的一部分，而不是这个文件的全部"></a>我想要暂存一个新文件的一部分，而不是这个文件的全部</h3><p>一般来说, 如果你想暂存一个文件的一部分, 你可这样做:</p><pre><code class="hljs sh">$ git add --patch filename.x</code></pre><p><code>-p</code> 简写。这会打开交互模式， 你将能够用 <code>s</code> 选项来分隔提交(commit)； 然而, 如果这个文件是新的, 会没有这个选择， 添加一个新文件时, 这样做:</p><pre><code class="hljs sh">$ git add -N filename.x</code></pre><p>然后, 你需要用 <code>e</code> 选项来手动选择需要添加的行，执行 <code>git diff --cached</code> 将会显示哪些行暂存了哪些行只是保存在本地了。</p><p><a href="stage-in-two-commits"></a></p><h3 id="我想把在一个文件里的变化-changes-加到两个提交-commit-里"><a href="#我想把在一个文件里的变化-changes-加到两个提交-commit-里" class="headerlink" title="我想把在一个文件里的变化(changes)加到两个提交(commit)里"></a>我想把在一个文件里的变化(changes)加到两个提交(commit)里</h3><p><code>git add</code> 会把整个文件加入到一个提交. <code>git add -p</code> 允许交互式的选择你想要提交的部分.</p><p><a href="unstaging-edits-and-staging-the-unstaged"></a></p><h3 id="我想把暂存的内容变成未暂存，把未暂存的内容暂存起来"><a href="#我想把暂存的内容变成未暂存，把未暂存的内容暂存起来" class="headerlink" title="我想把暂存的内容变成未暂存，把未暂存的内容暂存起来"></a>我想把暂存的内容变成未暂存，把未暂存的内容暂存起来</h3><p>多数情况下，你应该将所有的内容变为未暂存，然后再选择你想要的内容进行commit。<br>但假定你就是想要这么做，这里你可以创建一个临时的commit来保存你已暂存的内容，然后暂存你的未暂存的内容并进行stash。然后reset最后一个commit将原本暂存的内容变为未暂存，最后stash pop回来。</p><pre><code class="hljs sh">$ git commit -m <span class="hljs-string">"WIP"</span>$ git add .$ git stash$ git reset HEAD^$ git stash pop --index 0</code></pre><p>注意1: 这里使用<code>pop</code>仅仅是因为想尽可能保持幂等。<br>注意2: 假如你不加上<code>--index</code>你会把暂存的文件标记为为存储.这个<a href="https://stackoverflow.com/questions/31595873/git-stash-with-staged-files-does-stash-convert-staged-files-to-unstaged?answertab=active#tab-top" target="_blank" rel="noopener">链接</a> 解释得比较清楚。（不过是英文的，其大意是说，这是一个较为底层的问题，stash时会做2个commit，其中一个会记录index状态，staged的文件等东西，另一个记录worktree和其他的一些东西，如果你不在apply时加index，git会把两个一起销毁，所以staged里就空了）。</p><h2 id="未暂存-Unstaged-的内容"><a href="#未暂存-Unstaged-的内容" class="headerlink" title="未暂存(Unstaged)的内容"></a>未暂存(Unstaged)的内容</h2><p><a href="move-unstaged-edits-to-new-branch"></a></p><h3 id="我想把未暂存的内容移动到一个新分支"><a href="#我想把未暂存的内容移动到一个新分支" class="headerlink" title="我想把未暂存的内容移动到一个新分支"></a>我想把未暂存的内容移动到一个新分支</h3><pre><code class="hljs sh">$ git checkout -b my-branch</code></pre><p><a href="move-unstaged-edits-to-old-branch"></a></p><h3 id="我想把未暂存的内容移动到另一个已存在的分支"><a href="#我想把未暂存的内容移动到另一个已存在的分支" class="headerlink" title="我想把未暂存的内容移动到另一个已存在的分支"></a>我想把未暂存的内容移动到另一个已存在的分支</h3><pre><code class="hljs sh">$ git stash$ git checkout my-branch$ git stash pop</code></pre><p><a href="i-want-to-discard-my-local-uncommitted-changes"></a></p><h3 id="我想丢弃本地未提交的变化-uncommitted-changes"><a href="#我想丢弃本地未提交的变化-uncommitted-changes" class="headerlink" title="我想丢弃本地未提交的变化(uncommitted changes)"></a>我想丢弃本地未提交的变化(uncommitted changes)</h3><p>如果你只是想重置源(origin)和你本地(local)之间的一些提交(commit)，你可以：</p><pre><code class="hljs sh"><span class="hljs-comment"># one commit</span>(my-branch)$ git reset --hard HEAD^<span class="hljs-comment"># two commits</span>(my-branch)$ git reset --hard HEAD^^<span class="hljs-comment"># four commits</span>(my-branch)$ git reset --hard HEAD~4<span class="hljs-comment"># or</span>(master)$ git checkout -f</code></pre><p>重置某个特殊的文件, 你可以用文件名做为参数:</p><pre><code class="hljs sh">$ git reset filename</code></pre><p><a href="i-want-to-discard-specific-unstaged-changes"></a></p><h3 id="我想丢弃某些未暂存的内容"><a href="#我想丢弃某些未暂存的内容" class="headerlink" title="我想丢弃某些未暂存的内容"></a>我想丢弃某些未暂存的内容</h3><p>如果你想丢弃工作拷贝中的一部分内容，而不是全部。</p><p>签出(checkout)不需要的内容，保留需要的。</p><pre><code class="hljs sh">$ git checkout -p<span class="hljs-comment"># Answer y to all of the snippets you want to drop</span></code></pre><p>另外一个方法是使用 <code>stash</code>， Stash所有要保留下的内容, 重置工作拷贝, 重新应用保留的部分。</p><pre><code class="hljs sh">$ git stash -p<span class="hljs-comment"># Select all of the snippets you want to save</span>$ git reset --hard$ git stash pop</code></pre><p>或者, stash 你不需要的部分, 然后stash drop。</p><pre><code class="hljs sh">$ git stash -p<span class="hljs-comment"># Select all of the snippets you don't want to save</span>$ git stash drop</code></pre><h2 id="分支-Branches"><a href="#分支-Branches" class="headerlink" title="分支(Branches)"></a>分支(Branches)</h2><p><a name="pull-wrong-branch"></a></p><h3 id="我从错误的分支拉取了内容，或把内容拉取到了错误的分支"><a href="#我从错误的分支拉取了内容，或把内容拉取到了错误的分支" class="headerlink" title="我从错误的分支拉取了内容，或把内容拉取到了错误的分支"></a>我从错误的分支拉取了内容，或把内容拉取到了错误的分支</h3><p>这是另外一种使用 <code>git reflog</code> 情况，找到在这次错误拉(pull) 之前HEAD的指向。</p><pre><code class="hljs sh">(master)$ git reflogab7555f HEAD@&#123;0&#125;: pull origin wrong-branch: Fast-forwardc5bc55a HEAD@&#123;1&#125;: checkout: checkout message goes here</code></pre><p>重置分支到你所需的提交(desired commit):</p><pre><code class="hljs sh">$ git reset --hard c5bc55a</code></pre><p>完成。</p><p><a href="discard-local-commits"></a></p><h3 id="我想扔掉本地的提交-commit-，以便我的分支与远程的保持一致"><a href="#我想扔掉本地的提交-commit-，以便我的分支与远程的保持一致" class="headerlink" title="我想扔掉本地的提交(commit)，以便我的分支与远程的保持一致"></a>我想扔掉本地的提交(commit)，以便我的分支与远程的保持一致</h3><p>先确认你没有推(push)你的内容到远程。</p><p><code>git status</code> 会显示你领先(ahead)源(origin)多少个提交:</p><pre><code class="hljs sh">(my-branch)$ git status<span class="hljs-comment"># On branch my-branch</span><span class="hljs-comment"># Your branch is ahead of 'origin/my-branch' by 2 commits.</span><span class="hljs-comment">#   (use "git push" to publish your local commits)</span><span class="hljs-comment">#</span></code></pre><p>一种方法是:</p><pre><code class="hljs sh">(master)$ git reset --hard origin/my-branch</code></pre><p><a name="commit-wrong-branch"></a></p><h3 id="我需要提交到一个新分支，但错误的提交到了master"><a href="#我需要提交到一个新分支，但错误的提交到了master" class="headerlink" title="我需要提交到一个新分支，但错误的提交到了master"></a>我需要提交到一个新分支，但错误的提交到了master</h3><p>在master下创建一个新分支，不切换到新分支,仍在master下:</p><pre><code class="hljs sh">(master)$ git branch my-branch</code></pre><p>把master分支重置到前一个提交:</p><pre><code class="hljs sh">(master)$ git reset --hard HEAD^</code></pre><p><code>HEAD^</code> 是 <code>HEAD^1</code> 的简写，你可以通过指定要设置的<code>HEAD</code>来进一步重置。</p><p>或者, 如果你不想使用 <code>HEAD^</code>, 找到你想重置到的提交(commit)的hash(<code>git log</code> 能够完成)， 然后重置到这个hash。 使用<code>git push</code> 同步内容到远程。</p><p>例如, master分支想重置到的提交的hash为<code>a13b85e</code>:</p><pre><code class="hljs sh">(master)$ git reset --hard a13b85eHEAD is now at a13b85e</code></pre><p>签出(checkout)刚才新建的分支继续工作:</p><pre><code class="hljs sh">(master)$ git checkout my-branch</code></pre><p><a name="keep-whole-file"></a></p><h3 id="我想保留来自另外一个ref-ish的整个文件"><a href="#我想保留来自另外一个ref-ish的整个文件" class="headerlink" title="我想保留来自另外一个ref-ish的整个文件"></a>我想保留来自另外一个ref-ish的整个文件</h3><p>假设你正在做一个原型方案(原文为working spike (see note)), 有成百的内容，每个都工作得很好。现在, 你提交到了一个分支，保存工作内容:</p><pre><code class="hljs sh">(solution)$ git add -A &amp;&amp; git commit -m <span class="hljs-string">"Adding all changes from this spike into one big commit."</span></code></pre><p>当你想要把它放到一个分支里 (可能是<code>feature</code>, 或者 <code>develop</code>), 你关心是保持整个文件的完整，你想要一个大的提交分隔成比较小。</p><p>假设你有:</p><ul><li>分支 <code>solution</code>, 拥有原型方案， 领先 <code>develop</code> 分支。</li><li>分支 <code>develop</code>, 在这里你应用原型方案的一些内容。</li></ul><p>我去可以通过把内容拿到你的分支里，来解决这个问题:</p><pre><code class="hljs sh">(develop)$ git checkout solution -- file1.txt</code></pre><p>这会把这个文件内容从分支 <code>solution</code> 拿到分支 <code>develop</code> 里来:</p><pre><code class="hljs sh"><span class="hljs-comment"># On branch develop</span><span class="hljs-comment"># Your branch is up-to-date with 'origin/develop'.</span><span class="hljs-comment"># Changes to be committed:</span><span class="hljs-comment">#  (use "git reset HEAD &lt;file&gt;..." to unstage)</span><span class="hljs-comment">#</span><span class="hljs-comment">#        modified:   file1.txt</span></code></pre><p>然后, 正常提交。</p><p>Note: Spike solutions are made to analyze or solve the problem. These solutions are used for estimation and discarded once everyone gets clear visualization of the problem. ~ <a href="https://en.wikipedia.org/wiki/Extreme_programming_practices" target="_blank" rel="noopener">Wikipedia</a>.</p><p><a name="cherry-pick"></a></p><h3 id="我把几个提交-commit-提交到了同一个分支，而这些提交应该分布在不同的分支里"><a href="#我把几个提交-commit-提交到了同一个分支，而这些提交应该分布在不同的分支里" class="headerlink" title="我把几个提交(commit)提交到了同一个分支，而这些提交应该分布在不同的分支里"></a>我把几个提交(commit)提交到了同一个分支，而这些提交应该分布在不同的分支里</h3><p>假设你有一个<code>master</code>分支， 执行<code>git log</code>, 你看到你做过两次提交:</p><pre><code class="hljs sh">(master)$ git <span class="hljs-built_in">log</span>commit e3851e817c451cc36f2e6f3049db528415e3c114Author: Alex Lee &lt;alexlee@example.com&gt;Date:   Tue Jul 22 15:39:27 2014 -0400    Bug <span class="hljs-comment">#21 - Added CSRF protection</span>commit 5ea51731d150f7ddc4a365437931cd8be3bf3131Author: Alex Lee &lt;alexlee@example.com&gt;Date:   Tue Jul 22 15:39:12 2014 -0400    Bug <span class="hljs-comment">#14 - Fixed spacing on title</span>commit a13b85e984171c6e2a1729bb061994525f626d14Author: Aki Rose &lt;akirose@example.com&gt;Date:   Tue Jul 21 01:12:48 2014 -0400    First commit</code></pre><p>让我们用提交hash(commit hash)标记bug (<code>e3851e8</code> for #21, <code>5ea5173</code> for #14).</p><p>首先, 我们把<code>master</code>分支重置到正确的提交(<code>a13b85e</code>):</p><pre><code class="hljs sh">(master)$ git reset --hard a13b85eHEAD is now at a13b85e</code></pre><p>现在, 我们对 bug #21 创建一个新的分支:</p><pre><code class="hljs sh">(master)$ git checkout -b 21(21)$</code></pre><p>接着, 我们用 <em>cherry-pick</em> 把对bug #21的提交放入当前分支。 这意味着我们将应用(apply)这个提交(commit)，仅仅这一个提交(commit)，直接在HEAD上面。</p><pre><code class="hljs sh">(21)$ git cherry-pick e3851e8</code></pre><p>这时候, 这里可能会产生冲突， 参见<a href="#interactive-rebase">交互式 rebasing 章</a> <a href="#merge-conflict"><strong>冲突节</strong></a> 解决冲突.</p><p>再者， 我们为bug #14 创建一个新的分支, 也基于<code>master</code>分支</p><pre><code class="hljs sh">(21)$ git checkout master(master)$ git checkout -b 14(14)$</code></pre><p>最后, 为 bug #14 执行 <code>cherry-pick</code>:</p><pre><code class="hljs sh">(14)$ git cherry-pick 5ea5173</code></pre><p><a name="delete-stale-local-branches"></a></p><h3 id="我想删除上游-upstream-分支被删除了的本地分支"><a href="#我想删除上游-upstream-分支被删除了的本地分支" class="headerlink" title="我想删除上游(upstream)分支被删除了的本地分支"></a>我想删除上游(upstream)分支被删除了的本地分支</h3><p>一旦你在github 上面合并(merge)了一个pull request, 你就可以删除你fork里被合并的分支。 如果你不准备继续在这个分支里工作, 删除这个分支的本地拷贝会更干净，使你不会陷入工作分支和一堆陈旧分支的混乱之中。</p><pre><code class="hljs sh">$ git fetch -p</code></pre><p><a name='restore-a-deleted-branch'></a></p><h3 id="我不小心删除了我的分支"><a href="#我不小心删除了我的分支" class="headerlink" title="我不小心删除了我的分支"></a>我不小心删除了我的分支</h3><p>如果你定期推送到远程, 多数情况下应该是安全的，但有些时候还是可能删除了还没有推到远程的分支。 让我们先创建一个分支和一个新的文件:</p><pre><code class="hljs sh">(master)$ git checkout -b my-branch(my-branch)$ git branch(my-branch)$ touch foo.txt(my-branch)$ lsREADME.md foo.txt</code></pre><p>添加文件并做一次提交</p><pre><code class="hljs sh">(my-branch)$ git add .(my-branch)$ git commit -m <span class="hljs-string">'foo.txt added'</span>(my-branch)$ foo.txt added 1 files changed, 1 insertions(+) create mode 100644 foo.txt(my-branch)$ git <span class="hljs-built_in">log</span>commit 4e3cd85a670ced7cc17a2b5d8d3d809ac88d5012Author: siemiatj &lt;siemiatj@example.com&gt;Date:   Wed Jul 30 00:34:10 2014 +0200    foo.txt addedcommit 69204cdf0acbab201619d95ad8295928e7f411d5Author: Kate Hudson &lt;katehudson@example.com&gt;Date:   Tue Jul 29 13:14:46 2014 -0400    Fixes <span class="hljs-comment">#6: Force pushing after amending commits</span></code></pre><p>现在我们切回到主(master)分支，‘不小心的’删除<code>my-branch</code>分支</p><pre><code class="hljs sh">(my-branch)$ git checkout masterSwitched to branch <span class="hljs-string">'master'</span>Your branch is up-to-date with <span class="hljs-string">'origin/master'</span>.(master)$ git branch -D my-branchDeleted branch my-branch (was 4e3cd85).(master)$ <span class="hljs-built_in">echo</span> oh noes, deleted my branch!oh noes, deleted my branch!</code></pre><p>在这时候你应该想起了<code>reflog</code>, 一个升级版的日志，它存储了仓库(repo)里面所有动作的历史。</p><pre><code class="hljs angelscript">(master)$ git <span class="hljs-built_in">ref</span>log<span class="hljs-number">69204</span>cd <span class="hljs-symbol">HEAD@</span>&#123;<span class="hljs-number">0</span>&#125;: checkout: moving <span class="hljs-keyword">from</span> my-branch to master<span class="hljs-number">4e3</span>cd85 <span class="hljs-symbol">HEAD@</span>&#123;<span class="hljs-number">1</span>&#125;: commit: foo.txt added<span class="hljs-number">69204</span>cd <span class="hljs-symbol">HEAD@</span>&#123;<span class="hljs-number">2</span>&#125;: checkout: moving <span class="hljs-keyword">from</span> master to my-branch</code></pre><p>正如你所见，我们有一个来自删除分支的提交hash(commit hash)，接下来看看是否能恢复删除了的分支。</p><pre><code class="hljs sh">(master)$ git checkout -b my-branch-helpSwitched to a new branch <span class="hljs-string">'my-branch-help'</span>(my-branch-help)$ git reset --hard 4e3cd85HEAD is now at 4e3cd85 foo.txt added(my-branch-help)$ lsREADME.md foo.txt</code></pre><p>看! 我们把删除的文件找回来了。 Git的 <code>reflog</code> 在rebasing出错的时候也是同样有用的。</p><p><a name="i-want-to-delete-a-branch"></a></p><h3 id="我想删除一个分支"><a href="#我想删除一个分支" class="headerlink" title="我想删除一个分支"></a>我想删除一个分支</h3><p>删除一个远程分支:</p><pre><code class="hljs sh">(master)$ git push origin --delete my-branch</code></pre><p>你也可以:</p><pre><code class="hljs sh">(master)$ git push origin :my-branch</code></pre><p>删除一个本地分支:</p><pre><code class="hljs sh">(master)$ git branch -D my-branch</code></pre><p><a name="i-want-to-checkout-to-a-remote-branch-that-someone-else-is-working-on"></a></p><h3 id="我想从别人正在工作的远程分支签出-checkout-一个分支"><a href="#我想从别人正在工作的远程分支签出-checkout-一个分支" class="headerlink" title="我想从别人正在工作的远程分支签出(checkout)一个分支"></a>我想从别人正在工作的远程分支签出(checkout)一个分支</h3><p>首先, 从远程拉取(fetch) 所有分支:</p><pre><code class="hljs sh">(master)$ git fetch --all</code></pre><p>假设你想要从远程的<code>daves</code>分支签出到本地的<code>daves</code></p><pre><code class="hljs sh">(master)$ git checkout --track origin/davesBranch daves <span class="hljs-built_in">set</span> up to track remote branch daves from origin.Switched to a new branch <span class="hljs-string">'daves'</span></code></pre><p>(<code>--track</code> 是 <code>git checkout -b [branch] [remotename]/[branch]</code> 的简写)</p><p>这样就得到了一个<code>daves</code>分支的本地拷贝, 任何推过(pushed)的更新，远程都能看到.</p><h2 id="Rebasing-和合并-Merging"><a href="#Rebasing-和合并-Merging" class="headerlink" title="Rebasing 和合并(Merging)"></a>Rebasing 和合并(Merging)</h2><p><a name="undo-rebase"></a></p><h3 id="我想撤销rebase-merge"><a href="#我想撤销rebase-merge" class="headerlink" title="我想撤销rebase/merge"></a>我想撤销rebase/merge</h3><p>你可以合并(merge)或rebase了一个错误的分支, 或者完成不了一个进行中的rebase/merge。 Git 在进行危险操作的时候会把原始的HEAD保存在一个叫ORIG_HEAD的变量里, 所以要把分支恢复到rebase/merge前的状态是很容易的。</p><pre><code class="hljs sh">(my-branch)$ git reset --hard ORIG_HEAD</code></pre><p><a name="force-push-rebase"></a></p><h3 id="我已经rebase过-但是我不想强推-force-push"><a href="#我已经rebase过-但是我不想强推-force-push" class="headerlink" title="我已经rebase过, 但是我不想强推(force push)"></a>我已经rebase过, 但是我不想强推(force push)</h3><p>不幸的是，如果你想把这些变化(changes)反应到远程分支上，你就必须得强推(force push)。 是因你快进(Fast forward)了提交，改变了Git历史, 远程分支不会接受变化(changes)，除非强推(force push)。这就是许多人使用 merge 工作流, 而不是 rebasing 工作流的主要原因之一， 开发者的强推(force push)会使大的团队陷入麻烦。使用时需要注意，一种安全使用 rebase 的方法是，不要把你的变化(changes)反映到远程分支上, 而是按下面的做:</p><pre><code class="hljs sh">(master)$ git checkout my-branch(my-branch)$ git rebase -i master(my-branch)$ git checkout master(master)$ git merge --ff-only my-branch</code></pre><p>更多, 参见 <a href="http://stackoverflow.com/questions/11058312/how-can-i-use-git-rebase-without-requiring-a-forced-push" target="_blank" rel="noopener">this SO thread</a>.</p><p><a name="interactive-rebase"></a></p><h3 id="我需要组合-combine-几个提交-commit"><a href="#我需要组合-combine-几个提交-commit" class="headerlink" title="我需要组合(combine)几个提交(commit)"></a>我需要组合(combine)几个提交(commit)</h3><p>假设你的工作分支将会做对于 <code>master</code> 的pull-request。 一般情况下你不关心提交(commit)的时间戳，只想组合 <em>所有</em> 提交(commit) 到一个单独的里面, 然后重置(reset)重提交(recommit)。 确保主(master)分支是最新的和你的变化都已经提交了, 然后:</p><pre><code class="hljs sh">(my-branch)$ git reset --soft master(my-branch)$ git commit -am <span class="hljs-string">"New awesome feature"</span></code></pre><p>如果你想要更多的控制, 想要保留时间戳, 你需要做交互式rebase (interactive rebase):</p><pre><code class="hljs sh">(my-branch)$ git rebase -i master</code></pre><p>如果没有相对的其它分支， 你将不得不相对自己的<code>HEAD</code> 进行 rebase。 例如：你想组合最近的两次提交(commit), 你将相对于<code>HEAD~2</code> 进行rebase， 组合最近3次提交(commit), 相对于<code>HEAD~3</code>, 等等。</p><pre><code class="hljs sh">(master)$ git rebase -i HEAD~2</code></pre><p>在你执行了交互式 rebase的命令(interactive rebase command)后, 你将在你的编辑器里看到类似下面的内容:</p><pre><code class="hljs vim">pick a9c8a1d Some refactoringpick <span class="hljs-number">01</span>b2fd8 New awesome featurepick b729ad5 fixuppick e3851e8 another <span class="hljs-keyword">fix</span># Rebase <span class="hljs-number">8074</span>d12..b729ad5 onto <span class="hljs-number">8074</span>d12## Command<span class="hljs-variable">s:</span>#  <span class="hljs-keyword">p</span>, pick = use commit#  r, reword = use commit, but <span class="hljs-keyword">edit</span> the commit message#  <span class="hljs-keyword">e</span>, <span class="hljs-keyword">edit</span> = use commit, but <span class="hljs-keyword">stop</span> <span class="hljs-keyword">for</span> amending#  s, squash = use commit, but meld into <span class="hljs-keyword">previous</span> commit#  <span class="hljs-keyword">f</span>, fixup = like <span class="hljs-string">"squash"</span>, but discard this commit<span class="hljs-string">'s log message</span><span class="hljs-string">#  x, exec = run command (the rest of the line) using shell</span><span class="hljs-string">#</span><span class="hljs-string"># These lines can be re-ordered; they are executed from top to bottom.</span><span class="hljs-string">#</span><span class="hljs-string"># If you remove a line here THAT COMMIT WILL BE LOST.</span><span class="hljs-string">#</span><span class="hljs-string"># However, if you remove everything, the rebase will be aborted.</span><span class="hljs-string">#</span><span class="hljs-string"># Note that empty commits are commented out</span></code></pre><p>所有以 <code>#</code> 开头的行都是注释, 不会影响 rebase.</p><p>然后，你可以用任何上面命令列表的命令替换 <code>pick</code>, 你也可以通过删除对应的行来删除一个提交(commit)。</p><p>例如, 如果你想 <strong>单独保留最旧(first)的提交(commit),组合所有剩下的到第二个里面</strong>, 你就应该编辑第二个提交(commit)后面的每个提交(commit) 前的单词为 <code>f</code>:</p><pre><code class="hljs vim">pick a9c8a1d Some refactoringpick <span class="hljs-number">01</span>b2fd8 New awesome feature<span class="hljs-keyword">f</span> b729ad5 fixup<span class="hljs-keyword">f</span> e3851e8 another <span class="hljs-keyword">fix</span></code></pre><p>如果你想组合这些提交(commit) <strong>并重命名这个提交(commit)</strong>, 你应该在第二个提交(commit)旁边添加一个<code>r</code>，或者更简单的用<code>s</code> 替代 <code>f</code>:</p><pre><code class="hljs vim">pick a9c8a1d Some refactoringpick <span class="hljs-number">01</span>b2fd8 New awesome features b729ad5 fixups e3851e8 another <span class="hljs-keyword">fix</span></code></pre><p>你可以在接下来弹出的文本提示框里重命名提交(commit)。</p><pre><code class="hljs vim">Newer, awesomer features# Please enter the commit message <span class="hljs-keyword">for</span> your <span class="hljs-keyword">changes</span>. Lines starting# with <span class="hljs-string">'#'</span> will <span class="hljs-keyword">be</span> ignored, <span class="hljs-built_in">and</span> <span class="hljs-keyword">an</span> <span class="hljs-built_in">empty</span> message aborts the commit.# rebase in progress; onto 8074d12# You are currently editing <span class="hljs-keyword">a</span> commit <span class="hljs-keyword">while</span> rebasing branch <span class="hljs-string">'master'</span> <span class="hljs-keyword">on</span> <span class="hljs-string">'8074d12'</span>.## Changes <span class="hljs-keyword">to</span> <span class="hljs-keyword">be</span> committed:#modified:   README.md#</code></pre><p>如果成功了, 你应该看到类似下面的内容:</p><pre><code class="hljs sh">(master)$ Successfully rebased and updated refs/heads/master.</code></pre><h4 id="安全合并-merging-策略"><a href="#安全合并-merging-策略" class="headerlink" title="安全合并(merging)策略"></a>安全合并(merging)策略</h4><p><code>--no-commit</code> 执行合并(merge)但不自动提交, 给用户在做提交前检查和修改的机会。 <code>no-ff</code> 会为特性分支(feature branch)的存在过留下证据, 保持项目历史一致。</p><pre><code class="hljs sh">(master)$ git merge --no-ff --no-commit my-branch</code></pre><h4 id="我需要将一个分支合并成一个提交-commit"><a href="#我需要将一个分支合并成一个提交-commit" class="headerlink" title="我需要将一个分支合并成一个提交(commit)"></a>我需要将一个分支合并成一个提交(commit)</h4><pre><code class="hljs sh">(master)$ git merge --squash my-branch</code></pre><p><a name="rebase-unpushed-commits"></a></p><h4 id="我只想组合-combine-未推的提交-unpushed-commit"><a href="#我只想组合-combine-未推的提交-unpushed-commit" class="headerlink" title="我只想组合(combine)未推的提交(unpushed commit)"></a>我只想组合(combine)未推的提交(unpushed commit)</h4><p>有时候，在将数据推向上游之前，你有几个正在进行的工作提交(commit)。这时候不希望把已经推(push)过的组合进来，因为其他人可能已经有提交(commit)引用它们了。</p><pre><code class="hljs sh">(master)$ git rebase -i @&#123;u&#125;</code></pre><p>这会产生一次交互式的rebase(interactive rebase), 只会列出没有推(push)的提交(commit)， 在这个列表时进行reorder/fix/squash 都是安全的。</p><p><a name="check-if-all-commits-on-a-branch-are-merged"></a></p><h3 id="检查是否分支上的所有提交-commit-都合并-merge-过了"><a href="#检查是否分支上的所有提交-commit-都合并-merge-过了" class="headerlink" title="检查是否分支上的所有提交(commit)都合并(merge)过了"></a>检查是否分支上的所有提交(commit)都合并(merge)过了</h3><p>检查一个分支上的所有提交(commit)是否都已经合并(merge)到了其它分支, 你应该在这些分支的head(或任何 commits)之间做一次diff:</p><pre><code class="hljs sh">(master)$ git <span class="hljs-built_in">log</span> --graph --left-right --cherry-pick --oneline HEAD...feature/120-on-scroll</code></pre><p>这会告诉你在一个分支里有而另一个分支没有的所有提交(commit), 和分支之间不共享的提交(commit)的列表。 另一个做法可以是:</p><pre><code class="hljs sh">(master)$ git <span class="hljs-built_in">log</span> master ^feature/120-on-scroll --no-merges</code></pre><h3 id="交互式rebase-interactive-rebase-可能出现的问题"><a href="#交互式rebase-interactive-rebase-可能出现的问题" class="headerlink" title="交互式rebase(interactive rebase)可能出现的问题"></a>交互式rebase(interactive rebase)可能出现的问题</h3><p><a name="noop"></a></p><h4 id="这个rebase-编辑屏幕出现’noop’"><a href="#这个rebase-编辑屏幕出现’noop’" class="headerlink" title="这个rebase 编辑屏幕出现’noop’"></a>这个rebase 编辑屏幕出现’noop’</h4><p>如果你看到的是这样:</p><pre><code class="hljs ebnf"><span class="hljs-attribute">noop</span></code></pre><p>这意味着你rebase的分支和当前分支在同一个提交(commit)上, 或者 <em>领先(ahead)</em> 当前分支。 你可以尝试:</p><ul><li>检查确保主(master)分支没有问题</li><li>rebase  <code>HEAD~2</code> 或者更早</li></ul><p><a name="merge-conflict"></a></p><h4 id="有冲突的情况"><a href="#有冲突的情况" class="headerlink" title="有冲突的情况"></a>有冲突的情况</h4><p>如果你不能成功的完成rebase, 你可能必须要解决冲突。</p><p>首先执行 <code>git status</code> 找出哪些文件有冲突:</p><pre><code class="hljs sh">(my-branch)$ git statusOn branch my-branchChanges not staged <span class="hljs-keyword">for</span> commit:  (use <span class="hljs-string">"git add &lt;file&gt;..."</span> to update what will be committed)  (use <span class="hljs-string">"git checkout -- &lt;file&gt;..."</span> to discard changes <span class="hljs-keyword">in</span> working directory)modified:   README.md</code></pre><p>在这个例子里面, <code>README.md</code> 有冲突。 打开这个文件找到类似下面的内容:</p><pre><code class="hljs vim">&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADsome code=========some code&gt;&gt;&gt;&gt;&gt;&gt;&gt; <span class="hljs-keyword">new</span>-commit</code></pre><p>你需要解决新提交的代码(示例里, 从中间<code>==</code>线到<code>new-commit</code>的地方)与<code>HEAD</code> 之间不一样的地方.</p><p>有时候这些合并非常复杂，你应该使用可视化的差异编辑器(visual diff editor):</p><pre><code class="hljs sh">(master*)$ git mergetool -t opendiff</code></pre><p>在你解决完所有冲突和测试过后, <code>git add</code> 变化了的(changed)文件, 然后用<code>git rebase --continue</code> 继续rebase。</p><pre><code class="hljs sh">(my-branch)$ git add README.md(my-branch)$ git rebase --<span class="hljs-built_in">continue</span></code></pre><p>如果在解决完所有的冲突过后，得到了与提交前一样的结果, 可以执行<code>git rebase --skip</code>。</p><p>任何时候你想结束整个rebase 过程，回来rebase前的分支状态, 你可以做:</p><pre><code class="hljs sh">(my-branch)$ git rebase --abort</code></pre><p><a name="stashing"></a></p><h2 id="Stash"><a href="#Stash" class="headerlink" title="Stash"></a>Stash</h2><h3 id="暂存所有改动"><a href="#暂存所有改动" class="headerlink" title="暂存所有改动"></a>暂存所有改动</h3><p>暂存你工作目录下的所有改动</p><pre><code class="hljs sh">$ git stash</code></pre><p>你可以使用<code>-u</code>来排除一些文件</p><pre><code class="hljs sh">$ git stash -u</code></pre><h3 id="暂存指定文件"><a href="#暂存指定文件" class="headerlink" title="暂存指定文件"></a>暂存指定文件</h3><p>假设你只想暂存某一个文件</p><pre><code class="hljs sh">$ git stash push working-directory-path/filename.ext</code></pre><p>假设你想暂存多个文件</p><pre><code class="hljs sh">$ git stash push working-directory-path/filename1.ext working-directory-path/filename2.ext</code></pre><p><a name="stash-msg"></a></p><h3 id="暂存时记录消息"><a href="#暂存时记录消息" class="headerlink" title="暂存时记录消息"></a>暂存时记录消息</h3><p>这样你可以在<code>list</code>时看到它</p><pre><code class="hljs sh">$ git stash save &lt;message&gt;</code></pre><p>或</p><pre><code class="hljs sh">$ git stash push -m &lt;message&gt;</code></pre><p><a name="stash-apply-specific"></a></p><h3 id="使用某个指定暂存"><a href="#使用某个指定暂存" class="headerlink" title="使用某个指定暂存"></a>使用某个指定暂存</h3><p>首先你可以查看你的<code>stash</code>记录</p><pre><code class="hljs sh">$ git stash list</code></pre><p>然后你可以<code>apply</code>某个<code>stash</code></p><pre><code class="hljs sh">$ git stash apply <span class="hljs-string">"stash@&#123;n&#125;"</span></code></pre><p>此处， ‘n’是<code>stash</code>在栈中的位置，最上层的<code>stash</code>会是0</p><p>除此之外，也可以使用时间标记(假如你能记得的话)。</p><pre><code class="hljs sh">$ git stash apply <span class="hljs-string">"stash@&#123;2.hours.ago&#125;"</span></code></pre><p><a href="stage-and-keep-unstaged"></a></p><h3 id="暂存时保留未暂存的内容"><a href="#暂存时保留未暂存的内容" class="headerlink" title="暂存时保留未暂存的内容"></a>暂存时保留未暂存的内容</h3><p>你需要手动create一个<code>stash commit</code>， 然后使用<code>git stash store</code>。</p><pre><code class="hljs sh">$ git stash create$ git stash store -m <span class="hljs-string">"commit-message"</span> CREATED_SHA1</code></pre><p><a name="miscellaneous-objects"></a></p><h2 id="杂项-Miscellaneous-Objects"><a href="#杂项-Miscellaneous-Objects" class="headerlink" title="杂项(Miscellaneous Objects)"></a>杂项(Miscellaneous Objects)</h2><p><a name="clone-submodules"></a></p><h3 id="克隆所有子模块"><a href="#克隆所有子模块" class="headerlink" title="克隆所有子模块"></a>克隆所有子模块</h3><pre><code class="hljs sh">$ git <span class="hljs-built_in">clone</span> --recursive git://github.com/foo/bar.git</code></pre><p>如果已经克隆了:</p><pre><code class="hljs sh">$ git submodule update --init --recursive</code></pre><p><a name="delete-tag"></a></p><h3 id="删除标签-tag"><a href="#删除标签-tag" class="headerlink" title="删除标签(tag)"></a>删除标签(tag)</h3><pre><code class="hljs sh">$ git tag -d &lt;tag_name&gt;$ git push &lt;remote&gt; :refs/tags/&lt;tag_name&gt;</code></pre><p><a name="recover-tag"></a></p><h3 id="恢复已删除标签-tag"><a href="#恢复已删除标签-tag" class="headerlink" title="恢复已删除标签(tag)"></a>恢复已删除标签(tag)</h3><p>如果你想恢复一个已删除标签(tag), 可以按照下面的步骤: 首先, 需要找到无法访问的标签(unreachable tag):</p><pre><code class="hljs sh">$ git fsck --unreachable | grep tag</code></pre><p>记下这个标签(tag)的hash，然后用Git的 <a href="http://git-scm.com/docs/git-update-ref" target="_blank" rel="noopener">update-ref</a>:</p><pre><code class="hljs sh">$ git update-ref refs/tags/&lt;tag_name&gt; &lt;<span class="hljs-built_in">hash</span>&gt;</code></pre><p>这时你的标签(tag)应该已经恢复了。</p><p><a name="deleted-patch"></a></p><h3 id="已删除补丁-patch"><a href="#已删除补丁-patch" class="headerlink" title="已删除补丁(patch)"></a>已删除补丁(patch)</h3><p>如果某人在 GitHub 上给你发了一个pull request, 但是然后他删除了他自己的原始 fork, 你将没法克隆他们的提交(commit)或使用 <code>git am</code>。在这种情况下, 最好手动的查看他们的提交(commit)，并把它们拷贝到一个本地新分支，然后做提交。</p><p>做完提交后, 再修改作者，参见<a href="#commit-wrong-author">变更作者</a>。 然后, 应用变化, 再发起一个新的pull request。</p><h2 id="跟踪文件-Tracking-Files"><a href="#跟踪文件-Tracking-Files" class="headerlink" title="跟踪文件(Tracking Files)"></a>跟踪文件(Tracking Files)</h2><p><a href="i-want-to-change-a-file-names-capitalization-without-changing-the-contents-of-the-file"></a></p><h3 id="我只想改变一个文件名字的大小写，而不修改内容"><a href="#我只想改变一个文件名字的大小写，而不修改内容" class="headerlink" title="我只想改变一个文件名字的大小写，而不修改内容"></a>我只想改变一个文件名字的大小写，而不修改内容</h3><pre><code class="hljs sh">(master)$ git mv --force myfile MyFile</code></pre><p><a href="remove-from-git"></a></p><h3 id="我想从Git删除一个文件，但保留该文件"><a href="#我想从Git删除一个文件，但保留该文件" class="headerlink" title="我想从Git删除一个文件，但保留该文件"></a>我想从Git删除一个文件，但保留该文件</h3><pre><code class="hljs sh">(master)$ git rm --cached log.txt</code></pre><h2 id="配置-Configuration"><a href="#配置-Configuration" class="headerlink" title="配置(Configuration)"></a>配置(Configuration)</h2><p><a name="adding-command-aliases"></a></p><h3 id="我想给一些Git命令添加别名-alias"><a href="#我想给一些Git命令添加别名-alias" class="headerlink" title="我想给一些Git命令添加别名(alias)"></a>我想给一些Git命令添加别名(alias)</h3><p>在 OS X 和 Linux 下, 你的 Git的配置文件储存在 <code>~/.gitconfig</code>。我在<code>[alias]</code> 部分添加了一些快捷别名(和一些我容易拼写错误的)，如下:</p><pre><code class="hljs vim">[alias]    <span class="hljs-keyword">a</span> = <span class="hljs-built_in">add</span>    amend = commit --amend    <span class="hljs-keyword">c</span> = commit    <span class="hljs-keyword">ca</span> = commit --amend    ci = commit -<span class="hljs-keyword">a</span>    <span class="hljs-keyword">co</span> = checkout    d = diff    dc = diff --changed    <span class="hljs-keyword">ds</span> = diff --staged    <span class="hljs-keyword">f</span> = fetch    loll = <span class="hljs-built_in">log</span> --graph --decorate --pretty=oneline --abbrev-commit    <span class="hljs-keyword">m</span> = merge    one = <span class="hljs-built_in">log</span> --pretty=oneline    outstanding = rebase -i @&#123;<span class="hljs-keyword">u</span>&#125;    s = status    unpushed = <span class="hljs-built_in">log</span> @&#123;<span class="hljs-keyword">u</span>&#125;    wc = whatchanged    wip = rebase -i @&#123;<span class="hljs-keyword">u</span>&#125;    zap = fetch -<span class="hljs-keyword">p</span></code></pre><p><a name="credential-helper"></a></p><h3 id="我想缓存一个仓库-repository-的用户名和密码"><a href="#我想缓存一个仓库-repository-的用户名和密码" class="headerlink" title="我想缓存一个仓库(repository)的用户名和密码"></a>我想缓存一个仓库(repository)的用户名和密码</h3><p>你可能有一个仓库需要授权，这时你可以缓存用户名和密码，而不用每次推/拉(push/pull)的时候都输入，Credential helper能帮你。</p><pre><code class="hljs sh">$ git config --global credential.helper cache<span class="hljs-comment"># Set git to use the credential memory cache</span></code></pre><pre><code class="hljs sh">$ git config --global credential.helper <span class="hljs-string">'cache --timeout=3600'</span><span class="hljs-comment"># Set the cache to timeout after 1 hour (setting is in seconds)</span></code></pre><p><a href="#ive-no-idea-what-i-did-wrong"></a></p><h2 id="我不知道我做错了些什么"><a href="#我不知道我做错了些什么" class="headerlink" title="我不知道我做错了些什么"></a>我不知道我做错了些什么</h2><p>你把事情搞砸了：你 <code>重置(reset)</code> 了一些东西, 或者你合并了错误的分支, 亦或你强推了后找不到你自己的提交(commit)了。有些时候, 你一直都做得很好, 但你想回到以前的某个状态。</p><p>这就是 <code>git reflog</code> 的目的， <code>reflog</code> 记录对分支顶端(the tip of a branch)的任何改变, 即使那个顶端没有被任何分支或标签引用。基本上, 每次HEAD的改变, 一条新的记录就会增加到<code>reflog</code>。遗憾的是，这只对本地分支起作用，且它只跟踪动作 (例如，不会跟踪一个没有被记录的文件的任何改变)。</p><pre><code class="hljs sh">(master)$ git reflog0a2e358 HEAD@&#123;0&#125;: reset: moving to HEAD~20254ea7 HEAD@&#123;1&#125;: checkout: moving from 2.2 to masterc10f740 HEAD@&#123;2&#125;: checkout: moving from master to 2.2</code></pre><p>上面的reflog展示了从master分支签出(checkout)到2.2 分支，然后再签回。 那里，还有一个硬重置(hard reset)到一个较旧的提交。最新的动作出现在最上面以 <code>HEAD@{0}</code>标识.</p><p>如果事实证明你不小心回移(move back)了提交(commit), reflog 会包含你不小心回移前master上指向的提交(0254ea7)。</p><pre><code class="hljs sh">$ git reset --hard 0254ea7</code></pre><p>然后使用git reset就可以把master改回到之前的commit，这提供了一个在历史被意外更改情况下的安全网。</p><p>(<a href="https://www.atlassian.com/git/tutorials/rewriting-history/git-reflog" target="_blank" rel="noopener">摘自</a>).</p><h1 id="其它资源-Other-Resources"><a href="#其它资源-Other-Resources" class="headerlink" title="其它资源(Other Resources)"></a>其它资源(Other Resources)</h1><h2 id="书-Books"><a href="#书-Books" class="headerlink" title="书(Books)"></a>书(Books)</h2><ul><li><a href="https://git-scm.com/book/en/v2" target="_blank" rel="noopener">Pro Git</a> - Scott Chacon’s excellent git book</li><li><a href="https://github.com/pluralsight/git-internals-pdf" target="_blank" rel="noopener">Git Internals</a> - Scott Chacon’s other excellent git book</li></ul><h2 id="教程-Tutorials"><a href="#教程-Tutorials" class="headerlink" title="教程(Tutorials)"></a>教程(Tutorials)</h2><ul><li><a href="https://learngitbranching.js.org/" target="_blank" rel="noopener">Learn Git branching</a> 一个基于网页的交互式 branching/merging/rebasing 教程</li><li><a href="https://medium.com/@porteneuve/getting-solid-at-git-rebase-vs-merge-4fa1a48c53aa" target="_blank" rel="noopener">Getting solid at Git rebase vs. merge</a></li><li><a href="https://github.com/asmeurer/git-workflow" target="_blank" rel="noopener">git-workflow</a> - <a href="https://github.com/asmeurer" target="_blank" rel="noopener">Aaron Meurer</a>的怎么使用Git为开源仓库贡献</li><li><a href="http://hugogiraudel.com/2015/08/13/github-as-a-workflow/" target="_blank" rel="noopener">GitHub as a workflow</a> - 使用GitHub做为工作流的趣事, 尤其是空PRs</li></ul><h2 id="脚本和工具-Scripts-and-Tools"><a href="#脚本和工具-Scripts-and-Tools" class="headerlink" title="脚本和工具(Scripts and Tools)"></a>脚本和工具(Scripts and Tools)</h2><ul><li><a href="http://firstaidgit.io/" target="_blank" rel="noopener">firstaidgit.io</a> 一个可搜索的最常被问到的Git的问题</li><li><a href="https://github.com/unixorn/git-extra-commands" target="_blank" rel="noopener">git-extra-commands</a> - 一堆有用的额外的Git脚本</li><li><a href="https://github.com/tj/git-extras" target="_blank" rel="noopener">git-extras</a> - GIT 工具集 – repo summary, repl, changelog population, author commit percentages and more</li><li><a href="https://github.com/qw3rtman/git-fire" target="_blank" rel="noopener">git-fire</a> - git-fire 是一个 Git 插件，用于帮助在紧急情况下添加所有当前文件, 做提交(committing), 和推(push)到一个新分支(阻止合并冲突)。</li><li><a href="https://github.com/git-tips/tips" target="_blank" rel="noopener">git-tips</a> - Git小提示</li><li><a href="https://github.com/Originate/git-town" target="_blank" rel="noopener">git-town</a> - 通用，高级Git工作流支持！ <a href="http://www.git-town.com" target="_blank" rel="noopener">http://www.git-town.com</a></li></ul><h2 id="GUI客户端-GUI-Clients"><a href="#GUI客户端-GUI-Clients" class="headerlink" title="GUI客户端(GUI Clients)"></a>GUI客户端(GUI Clients)</h2><ul><li><a href="https://www.gitkraken.com/" target="_blank" rel="noopener">GitKraken</a> - 豪华的Git客户端 Windows, Mac &amp; Linux</li><li><a href="https://git-cola.github.io/" target="_blank" rel="noopener">git-cola</a> - 另外一个Git客户端 Windows &amp; OS X</li><li><a href="https://github.com/git-up/GitUp" target="_blank" rel="noopener">GitUp</a> - 一个新的Git客户端，在处理Git的复杂性上有自己的特点</li><li><a href="https://rowanj.github.io/gitx/" target="_blank" rel="noopener">gitx-dev</a> - 图形化的Git客户端 OS X</li><li><a href="https://www.sourcetreeapp.com/" target="_blank" rel="noopener">Source Tree</a> - 免费的图形化Git客户端 Windows &amp; OS X</li><li><a href="http://www.git-tower.com/" target="_blank" rel="noopener">Tower</a> - 图形化Git客户端 OS X(付费)</li></ul>]]></content>
    
    
    <categories>
      
      <category>Git</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP Tasks and PyTorch Data Parallel Modes</title>
    <link href="/2020/11/22/2020/201122_useful_tips/"/>
    <url>/2020/11/22/2020/201122_useful_tips/</url>
    
    <content type="html"><![CDATA[<h2 id="NLP-Tasks"><a href="#NLP-Tasks" class="headerlink" title="NLP Tasks"></a><a href="https://huggingface.co/transformers/quicktour.html" target="_blank" rel="noopener">NLP Tasks</a></h2><ul><li><strong>Sentiment analysis</strong>: is a text positive or negative?</li><li><strong>Text generation (in English)</strong>: provide a prompt and the model will generate what follows.</li><li><strong>Name entity recognition (NER)</strong>: in an input sentence, label each word with the entity it represents (person, place, etc.)</li><li><strong>Question answering</strong>: provide the model with some context and a question, extract the answer from the context.</li><li><strong>Filling masked text</strong>: given a text with masked words (e.g., replaced by [MASK]), fill the blanks.</li><li><strong>Summarization</strong>: generate a summary of a long text.</li><li><strong>Translation</strong>: translate a text in another language.</li><li><strong>Feature extraction</strong>: return a tensor representation of the text.</li></ul><h2 id="PyTorch-Data-Parallel-Modes"><a href="#PyTorch-Data-Parallel-Modes" class="headerlink" title="PyTorch Data Parallel Modes"></a><a href="https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html" target="_blank" rel="noopener">PyTorch Data Parallel Modes</a></h2><ul><li>Data Parallel (accelerator=’dp’) (multiple-gpus, 1 machine)</li><li>DistributedDataParallel (accelerator=’ddp’) (multiple-gpus across many machines (python script based)).</li><li>DistributedDataParallel (accelerator=’ddp_spawn’) (multiple-gpus across many machines (spawn based)).</li><li>DistributedDataParallel 2 (accelerator=’ddp2’) (DP in a machine, DDP across machines).</li><li>Horovod (accelerator=’horovod’) (multi-machine, multi-gpu, configured at runtime)</li><li>TPUs (tpu_cores=8|x) (tpu or TPU pod)</li></ul>]]></content>
    
    
    <categories>
      
      <category>Tips</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tips</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>htop</title>
    <link href="/2020/11/19/2020/201119_htop/"/>
    <url>/2020/11/19/2020/201119_htop/</url>
    
    <content type="html"><![CDATA[<h2 id="颜色"><a href="#颜色" class="headerlink" title="颜色"></a>颜色</h2><h3 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h3><ul><li>蓝色: 低优先级线程</li><li>绿色: 正常优先级线程</li><li>红色: 内核线程</li></ul><h3 id="Mem"><a href="#Mem" class="headerlink" title="Mem"></a>Mem</h3><ul><li>绿色: 已用内存</li><li>蓝色: 缓冲区</li><li>黄色/橙色: 缓存</li></ul>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>按关键词杀死进程</title>
    <link href="/2020/11/19/2020/201119_kill/"/>
    <url>/2020/11/19/2020/201119_kill/</url>
    
    <content type="html"><![CDATA[<h2 id="kill"><a href="#kill" class="headerlink" title="kill"></a>kill</h2><pre><code class="hljs coq">ps -ef | <span class="hljs-type">grep</span> firefox | <span class="hljs-type">awk</span> '&#123;print $<span class="hljs-number">2</span>&#125;' | <span class="hljs-type">xargs</span> kill <span class="hljs-number">-9</span></code></pre><pre><code class="hljs vim"><span class="hljs-keyword">ps</span> -ef | <span class="hljs-keyword">grep</span> <span class="hljs-symbol">&lt;key-word&gt;</span> | awk <span class="hljs-string">'&#123;print $2&#125;'</span> | xargs kill -<span class="hljs-number">9</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>跟我一起写 Makefile</title>
    <link href="/2020/11/16/2020/201116_makefile/"/>
    <url>/2020/11/16/2020/201116_makefile/</url>
    
    <content type="html"><![CDATA[<h2 id="跟我一起写-Makefile"><a href="#跟我一起写-Makefile" class="headerlink" title="跟我一起写 Makefile"></a>跟我一起写 Makefile</h2><p>突然发现<a href="https://wiki.mjrc.ml/v/Makefile_Usage" target="_blank" rel="noopener">吴老师在2010年写的 makefile 教程</a>，转载一下</p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>什么是makefile？或许很多Winodws的程序员都不知道这个东西，因为那些Windows的IDE都为你做了这个工作，但我觉得要作一个好的和professional的程序员，makefile还是要懂。这就好像现在有这么多的HTML的编辑器，但如果你想成为一个专业人士，你还是要了解HTML的标识的含义。特别在Unix下的软件编译，你就不能不自己写makefile了，会不会写makefile，从一个侧面说明了一个人是否具备完成大型工程的能力。 </p><p>因为，makefile关系到了整个工程的编译规则。一个工程中的源文件不计数，其按类型、功能、模块分别放在若干个目录中，makefile定义了一系列的规则来指定，哪些文件需要先编译，哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为makefile就像一个Shell脚本一样，其中也可以执行操作系统的命令。 </p><p>makefile带来的好处就是——“自动化编译”，一旦写好，只需要一个make命令，整个工程完全自动编译，极大的提高了软件开发的效率。make是一个命令工具，是一个解释makefile中指令的命令工具，一般来说，大多数的IDE都有这个命令，比如：Delphi的make，Visual C++的nmake，Linux下GNU的make。可见，makefile都成为了一种在工程方面的编译方法。 </p><p>现在讲述如何写makefile的文章比较少，这是我想写这篇文章的原因。当然，不同产商的make各不相同，也有不同的语法，但其本质都是在“文件依赖性”上做文章，这里，我仅对GNU的make进行讲述，我的环境是RedHat Linux 8.0，make的版本是3.80。必竟，这个make是应用最为广泛的，也是用得最多的。而且其还是最遵循于IEEE 1003.2-1992 标准的（POSIX.2）。 </p><p>在这篇文档中，将以C/C++的源码作为我们基础，所以必然涉及一些关于C/C++的编译的知识，相关于这方面的内容，还请各位查看相关的编译器的文档。这里所默认的编译器是UNIX下的GCC和CC。</p><h3 id="关于程序的编译和链接"><a href="#关于程序的编译和链接" class="headerlink" title="关于程序的编译和链接"></a>关于程序的编译和链接</h3><p>在此，我想多说关于程序编译的一些规范和方法，一般来说，无论是C、C++、还是pas，首先要把源文件编译成中间代码文件，在Windows下也就是 <code>.obj</code> 文件，UNIX下是 <code>.o</code> 文件，即 Object File，这个动作叫做编译（compile）。然后再把大量的Object File合成执行文件，这个动作叫作链接（link）。 </p><p>编译时，编译器需要的是语法的正确，函数与变量的声明的正确。对于后者，通常是你需要告诉编译器头文件的所在位置（头文件中应该只是声明，而定义应该放在C/C++文件中），只要所有的语法正确，编译器就可以编译出中间目标文件。一般来说，每个源文件都应该对应于一个中间目标文件（O文件或是OBJ文件）。 </p><p>链接时，主要是链接函数和全局变量，所以，我们可以使用这些中间目标文件（O文件或是OBJ文件）来链接我们的应用程序。链接器并不管函数所在的源文件，只管函数的中间目标文件（Object File），在大多数时候，由于源文件太多，编译生成的中间目标文件太多，而在链接时需要明显地指出中间目标文件名，这对于编译很不方便，所以，我们要给中间目标文件打个包，在Windows下这种包叫“库文件”（Library File)，也就是 <code>.lib</code> 文件，在UNIX下，是Archive File，也就是 <code>.a</code> 文件。 </p><p>总结一下，源文件首先会生成中间目标文件，再由中间目标文件生成执行文件。在编译时，编译器只检测程序语法，和函数、变量是否被声明。如果函数未被声明，编译器会给出一个警告，但可以生成Object File。而在链接程序时，链接器会在所有的Object File中找寻函数的实现，如果找不到，那到就会报链接错误码（Linker Error），在VC下，这种错误一般是：Link 2001错误，意思说是说，链接器未能找到函数的实现。你需要指定函数的Object File. </p><p>好，言归正传，GNU的make有许多的内容，闲言少叙，还是让我们开始吧。</p><h3 id="Makefile-介绍"><a href="#Makefile-介绍" class="headerlink" title="Makefile 介绍"></a>Makefile 介绍</h3><p>make命令执行时，需要一个 Makefile 文件，以告诉make命令需要怎么样的去编译和链接程序。 </p><p>首先，我们用一个示例来说明Makefile的书写规则。以便给大家一个感兴认识。这个示例来源于GNU的make使用手册，在这个示例中，我们的工程有8个C文件，和3个头文件，我们要写一个Makefile来告诉make命令如何编译和链接这几个文件。我们的规则是：<br>    1）如果这个工程没有编译过，那么我们的所有C文件都要编译并被链接。<br>    2）如果这个工程的某几个C文件被修改，那么我们只编译被修改的C文件，并链接目标程序。<br>    3）如果这个工程的头文件被改变了，那么我们需要编译引用了这几个头文件的C文件，并链接目标程序。 </p><p>只要我们的Makefile写得够好，所有的这一切，我们只用一个make命令就可以完成，make命令会自动智能地根据当前的文件修改的情况来确定哪些文件需要重编译，从而自己编译所需要的文件和链接目标程序。 </p><h4 id="一、Makefile的规则"><a href="#一、Makefile的规则" class="headerlink" title="一、Makefile的规则"></a>一、Makefile的规则</h4><p>在讲述这个Makefile之前，还是让我们先来粗略地看一看Makefile的规则。 </p><pre><code class="hljs makefile">target ... : prerequisites ...         command         ...         ...</code></pre><ol><li><code>target</code>也就是一个目标文件，可以是<code>Object File</code>，也可以是执行文件。还可以是一个标签（Label），对于标签这种特性，在后续的“伪目标”章节中会有叙述。 </li><li><code>prerequisites</code>就是，要生成那个<code>target</code>所需要的文件或是目标。 </li><li><code>command</code>也就是make需要执行的命令。（任意的Shell命令） </li></ol><p>这是一个文件的依赖关系，也就是说，target这一个或多个的目标文件依赖于prerequisites中的文件，其生成规则定义在command中。说白一点就是说，prerequisites中如果有一个以上的文件比target文件要新的话，command所定义的命令就会被执行。这就是Makefile的规则。也就是Makefile中最核心的内容。 </p><p>说到底，Makefile的东西就是这样一点，好像我的这篇文档也该结束了。呵呵。还不尽然，这是Makefile的主线和核心，但要写好一个Makefile还不够，我会以后面一点一点地结合我的工作经验给你慢慢到来。内容还多着呢。：） </p><h3 id="二、一个示例"><a href="#二、一个示例" class="headerlink" title="二、一个示例"></a>二、一个示例</h3><p>正如前面所说的，如果一个工程有3个头文件，和8个C文件，我们为了完成前面所述的那三个规则，我们的Makefile应该是下面的这个样子的。 </p><pre><code class="hljs bash">edit : main.o kbd.o command.o display.o \         insert.o search.o files.o utils.o         cc -o edit main.o kbd.o command.o display.o \                 insert.o search.o files.o utils.o</code></pre><pre><code class="hljs bash">main.o : main.c defs.h         cc -c main.c kbd.o : kbd.c defs.h command.h         cc -c kbd.c command.o : command.c defs.h command.h         cc -c command.c display.o : display.c defs.h buffer.h         cc -c display.c insert.o : insert.c defs.h buffer.h         cc -c insert.c search.o : search.c defs.h buffer.h         cc -c search.c files.o : files.c defs.h buffer.h command.h         cc -c files.c utils.o : utils.c defs.h         cc -c utils.c clean :         rm edit main.o kbd.o command.o display.o \         insert.o search.o files.o utils.o</code></pre><p>反斜杠（\）是换行符的意思。这样比较便于Makefile的易读。我们可以把这个内容保存在文件为“Makefile”或“makefile”的文件中，然后在该目录下直接输入命令“make”就可以生成执行文件edit。如果要删除执行文件和所有的中间目标文件，那么，只要简单地执行一下“make clean”就可以了。 </p><p>在这个makefile中，目标文件（target）包含：执行文件edit和中间目标文件（*.o），依赖文件（prerequisites）就是冒号后面的那些 .c 文件和 .h文件。每一个 .o 文件都有一组依赖文件，而这些 .o 文件又是执行文件 edit 的依赖文件。依赖关系的实质上就是说明了目标文件是由哪些文件生成的，换言之，目标文件是哪些文件更新的。 </p><p>在定义好依赖关系后，后续的那一行定义了如何生成目标文件的操作系统命令，一定要以一个Tab键作为开头。记住，make并不管命令是怎么工作的，他只管执行所定义的命令。make会比较targets文件和prerequisites文件的修改日期，如果prerequisites文件的日期要比targets文件的日期要新，或者target不存在的话，那么，make就会执行后续定义的命令。 </p><p>这里要说明一点的是，clean不是一个文件，它只不过是一个动作名字，有点像C语言中的lable一样，其冒号后什么也没有，那么，make就不会自动去找文件的依赖性，也就不会自动执行其后所定义的命令。要执行其后的命令，就要在make命令后明显得指出这个lable的名字。这样的方法非常有用，我们可以在一个makefile中定义不用的编译或是和编译无关的命令，比如程序的打包，程序的备份，等等。</p><h3 id="三、make是如何工作的"><a href="#三、make是如何工作的" class="headerlink" title="三、make是如何工作的"></a>三、make是如何工作的</h3><p>在默认的方式下，也就是我们只输入make命令。那么， </p><ol><li>make会在当前目录下找名字叫“Makefile”或“makefile”的文件。 </li><li>如果找到，它会找文件中的第一个目标文件（target），在上面的例子中，他会找到“edit”这个文件，并把这个文件作为最终的目标文件。 </li><li>如果edit文件不存在，或是edit所依赖的后面的 .o 文件的文件修改时间要比edit这个文件新，那么，他就会执行后面所定义的命令来生成edit这个文件。 </li><li>如果edit所依赖的.o文件也存在，那么make会在当前文件中找目标为.o文件的依赖性，如果找到则再根据那一个规则生成.o文件。（这有点像一个堆栈的过程） </li><li>当然，你的C文件和H文件是存在的啦，于是make会生成 .o 文件，然后再用 .o 文件生命make的终极任务，也就是执行文件edit了。 </li></ol><p>这就是整个make的依赖性，make会一层又一层地去找文件的依赖关系，直到最终编译出第一个目标文件。在找寻的过程中，如果出现错误，比如最后被依赖的文件找不到，那么make就会直接退出，并报错，而对于所定义的命令的错误，或是编译不成功，make根本不理。make只管文件的依赖性，即，如果在我找了依赖关系之后，冒号后面的文件还是不在，那么对不起，我就不工作啦。 </p><p>通过上述分析，我们知道，像clean这种，没有被第一个目标文件直接或间接关联，那么它后面所定义的命令将不会被自动执行，不过，我们可以显示要make执行。即命令——“make clean”，以此来清除所有的目标文件，以便重编译。 </p><p>于是在我们编程中，如果这个工程已被编译过了，当我们修改了其中一个源文件，比如file.c，那么根据我们的依赖性，我们的目标file.o会被重编译（也就是在这个依性关系后面所定义的命令），于是file.o的文件也是最新的啦，于是file.o的文件修改时间要比edit要新，所以edit也会被重新链接了（详见edit目标文件后定义的命令）。 </p><p>而如果我们改变了“command.h”，那么，kdb.o、command.o和files.o都会被重编译，并且，edit会被重链接。 </p><h3 id="四、makefile中使用变量"><a href="#四、makefile中使用变量" class="headerlink" title="四、makefile中使用变量"></a>四、makefile中使用变量</h3><p>在上面的例子中，先让我们看看edit的规则： </p><pre><code class="hljs bash">edit : main.o kbd.o command.o display.o \                 insert.o search.o files.o utils.o         cc -o edit main.o kbd.o command.o display.o \                 insert.o search.o files.o utils.o</code></pre><p>我们可以看到<code>[.o]</code>文件的字符串被重复了两次，如果我们的工程需要加入一个新的<code>[.o]</code>文件，那么我们需要在两个地方加（应该是三个地方，还有一个地方在clean中）。当然，我们的makefile并不复杂，所以在两个地方加也不累，但如果makefile变得复杂，那么我们就有可能会忘掉一个需要加入的地方，而导致编译失败。所以，为了makefile的易维护，在makefile中我们可以使用变量。makefile的变量也就是一个字符串，理解成C语言中的宏可能会更好。 </p><p>比如，我们声明一个变量，叫objects, OBJECTS, objs, OBJS, obj, 或是 OBJ，反正不管什么啦，只要能够表示obj文件就行了。我们在makefile一开始就这样定义： </p><pre><code class="hljs bash">objects = main.o kbd.o command.o display.o \         insert.o search.o files.o utils.o</code></pre><p>于是，我们就可以很方便地在我们的makefile中以“$(objects)”的方式来使用这个变量了，于是我们的改良版makefile就变成下面这个样子： </p><pre><code class="hljs bash">objects = main.o kbd.o command.o display.o \         insert.o search.o files.o utils.o</code></pre><pre><code class="hljs bash">edit : $(objects)         cc -o edit $(objects) main.o : main.c defs.h         cc -c main.c kbd.o : kbd.c defs.h command.h         cc -c kbd.c command.o : command.c defs.h command.h         cc -c command.c display.o : display.c defs.h buffer.h         cc -c display.c insert.o : insert.c defs.h buffer.h         cc -c insert.c search.o : search.c defs.h buffer.h         cc -c search.c files.o : files.c defs.h buffer.h command.h         cc -c files.c utils.o : utils.c defs.h         cc -c utils.c clean :         rm edit $(objects)</code></pre><p>于是如果有新的 .o 文件加入，我们只需简单地修改一下 objects 变量就可以了。 </p><p>关于变量更多的话题，我会在后续给你一一道来。 </p><h3 id="五、让make自动推导"><a href="#五、让make自动推导" class="headerlink" title="五、让make自动推导"></a>五、让make自动推导</h3><p>GNU的make很强大，它可以自动推导文件以及文件依赖关系后面的命令，于是我们就没必要去在每一个[.o]文件后都写上类似的命令，因为，我们的make会自动识别，并自己推导命令。 </p><p>只要make看到一个[.o]文件，它就会自动的把[.c]文件加在依赖关系中，如果make找到一个whatever.o，那么whatever.c，就会是whatever.o的依赖文件。并且 cc -c whatever.c 也会被推导出来，于是，我们的makefile再也不用写得这么复杂。我们的是新的makefile又出炉了。 </p><pre><code class="hljs bash">objects = main.o kbd.o command.o display.o \         insert.o search.o files.o utils.o edit : $(objects)         cc -o edit $(objects) main.o : defs.h kbd.o : defs.h command.h command.o : defs.h command.h display.o : defs.h buffer.h insert.o : defs.h buffer.h search.o : defs.h buffer.h files.o : defs.h buffer.h command.h utils.o : defs.h .PHONY : clean clean :         rm edit $(objects)</code></pre><p>这种方法，也就是make的“隐晦规则”。上面文件内容中，“.PHONY”表示，clean是个伪目标文件。 </p><p>关于更为详细的“隐晦规则”和“伪目标文件”，我会在后续给你一一道来。 </p><h3 id="六、另类风格的makefile"><a href="#六、另类风格的makefile" class="headerlink" title="六、另类风格的makefile"></a>六、另类风格的makefile</h3><p>即然我们的make可以自动推导命令，那么我看到那堆[.o]和[.h]的依赖就有点不爽，那么多的重复的[.h]，能不能把其收拢起来，好吧，没有问题，这个对于make来说很容易，谁叫它提供了自动推导命令和文件的功能呢？来看看最新风格的makefile吧。 </p><pre><code class="hljs bash">objects = main.o kbd.o command.o display.o \         insert.o search.o files.o utils.o edit : $(objects)         cc -o edit $(objects) $(objects) : defs.h kbd.o command.o files.o : command.h display.o insert.o search.o files.o : buffer.h .PHONY : clean clean :         rm edit $(objects)</code></pre><p>这种风格，让我们的makefile变得很简单，但我们的文件依赖关系就显得有点凌乱了。鱼和熊掌不可兼得。还看你的喜好了。我是不喜欢这种风格的，一是文件的依赖关系看不清楚，二是如果文件一多，要加入几个新的.o文件，那就理不清楚了。 </p><h3 id="七、清空目标文件的规则"><a href="#七、清空目标文件的规则" class="headerlink" title="七、清空目标文件的规则"></a>七、清空目标文件的规则</h3><p>每个Makefile中都应该写一个清空目标文件（.o和执行文件）的规则，这不仅便于重编译，也很利于保持文件的清洁。这是一个“修养”（呵呵，还记得我的《编程修养》吗）。一般的风格都是： </p><pre><code class="hljs bash">clean:         rm edit $(objects)</code></pre><p>更为稳健的做法是： </p><pre><code class="hljs bash">.PHONY : clean clean :         -rm edit $(objects)</code></pre><p>前面说过，.PHONY意思表示clean是一个“伪目标”，。而在rm命令前面加了一个小减号的意思就是，也许某些文件出现问题，但不要管，继续做后面的事。当然，clean的规则不要放在文件的开头，不然，这就会变成make的默认目标，相信谁也不愿意这样。不成文的规矩是——“clean从来都是放在文件的最后”。 </p><p>上面就是一个makefile的概貌，也是makefile的基础，下面还有很多makefile的相关细节，准备好了吗？准备好了就来。</p>]]></content>
    
    
    <categories>
      
      <category>toolkit</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开发</tag>
      
      <tag>工具</tag>
      
      <tag>效率</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neural Network Translation</title>
    <link href="/2020/11/05/2020/201105_NMT/"/>
    <url>/2020/11/05/2020/201105_NMT/</url>
    
    <content type="html"><![CDATA[<ul><li><strong>NMT</strong>: Neural Machine Translation</li><li><strong>SMT</strong>: Static Machine Translation</li><li><strong>BLUE</strong>: Bilingual Evaluation Understudy </li><li><strong>WMT-14 dataset</strong>: </li><li><strong>beam-search</strong>: beam search是对贪心策略一个改进。在每一个时间步，不再只保留当前分数最高的1个输出，而是保留num_beams个。当num_beams=1时集束搜索就退化成了贪心搜索。</li><li><strong>&lt;EOS&gt;</strong>: end-of-sentence symbol</li></ul><h2 id="Sequence-to-Sequence-Learning-with-Neural-Networks"><a href="#Sequence-to-Sequence-Learning-with-Neural-Networks" class="headerlink" title="Sequence to Sequence Learning with Neural Networks"></a>Sequence to Sequence Learning with Neural Networks</h2><p><strong>Sequential Problems: Lengths are not known a-priori.</strong></p><p>Using a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, then another deep LSTM to decode the target sequence from the vector.</p><h3 id="BLUE"><a href="#BLUE" class="headerlink" title="BLUE"></a>BLUE</h3><p>$$BLEU = BP \times exp(\sum_{n=1}^{N}w_n\log{P_n})$$</p><p>其中 $BP$ 是惩罚因子(Brevity Penalty)，为了避免评分的偏向性，所以在结果中引入。</p><h3 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h3><p>Given a sequence of inputs $(x1, \dots, x_T)$, a standard RNN computes a sequence of outputs $(y_1, \dots , y_T)$ by iterating the following equation:</p><p>$$h_t = sigm(W^{hx}x_t+W^{hh}h_{t-1})$$</p><p>$$y_t = W^{yh}h_t$$</p><p>However, it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and non-monotonic relationships.</p><p>The goal of LSTM is known to learn problems with long range temporal dependencies, and estimate the conditional probability $p(y_1, \dots, y_{T’}|x_1, \dots, x_T)$ (length $T’$ may differ from $T$)</p><ul><li>用了两种不同的lstm，一种是处理输入序列，一种是处理输出序列；</li><li>更深的lstm会比浅的lstm效果更好，所以论文选择了四层；</li><li>将输入的序列翻转之后作为输入效果更好一些。</li></ul><h3 id="Decoding-and-Rescoring"><a href="#Decoding-and-Rescoring" class="headerlink" title="Decoding and Rescoring"></a>Decoding and Rescoring</h3><p>Trained the model by maximizing the log probability of a correct translation $T$ given the source sentence $S$:</p><p>$$ 1/|\mathcal{S}| \sum_{(T,S)\in \mathcal{S}} \log p(T|\mathcal{S})$$</p>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git Tips</title>
    <link href="/2020/11/05/2020/201107_git/"/>
    <url>/2020/11/05/2020/201107_git/</url>
    
    <content type="html"><![CDATA[<p>实验室wiki上静北师兄整理的，copy一下</p><h2 id="Git-使用哲学"><a href="#Git-使用哲学" class="headerlink" title="Git 使用哲学"></a>Git 使用哲学</h2><p>简单来说，Git 的使用哲学和目的就是有条理地说清楚你都干了什么事，然后把它们一一存储成独立的改动。 这么做以后，如果你之前发现你做的某一件事可能是错的，你可以基于这个独立的改动创建一个分支，同时还保留其他改动。 同时，你还可以实现回滚到某个版本，应用其他同学的改动等等。但就核心而言，是练就你有条理地说清楚你都干了什么事的能力。</p><h2 id="Git-基本使用方法"><a href="#Git-基本使用方法" class="headerlink" title="Git 基本使用方法"></a>Git 基本使用方法</h2><h3 id="创建-Git-仓库"><a href="#创建-Git-仓库" class="headerlink" title="创建 Git 仓库"></a>创建 Git 仓库</h3><p>越来越多的实验代码都使用 Git 来管理和分享。对于一个已经存在的远程仓库，执行下述命令即可将其克隆到本地。</p><pre><code class="hljs bash">$ git <span class="hljs-built_in">clone</span> &lt;url&gt;</code></pre><p>对于一份没有被 Git 管理的本地代码，切换到代码所在文件夹，执行下述命令即可创建一个 Git 仓库。</p><pre><code class="hljs bash">$ git init</code></pre><h3 id="配置-SSH-密钥"><a href="#配置-SSH-密钥" class="headerlink" title="配置 SSH 密钥"></a>配置 SSH 密钥</h3><p><code>&lt;url&gt;</code> 一般有两种。一种是基于 HTTP 协议的，一种是基于 Git 协议的。基于 Git 协议的可以通过密钥认证来避免输密码。 绝大多数在线的 Git 仓库托管站点都支持密钥认证，从而可以避免可能要输入的密码。 这里推荐参考 <a href="https://docs.github.com/cn/free-pro-team@latest/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent" target="_blank" rel="noopener">GitHub 密钥配置文档</a> 。</p><h3 id="创建改动"><a href="#创建改动" class="headerlink" title="创建改动"></a>创建改动</h3><p>当你对 Git 仓库内的文件进行修改后，你可以通过下述命令来查看目前的改动。</p><pre><code class="hljs bash">$ git status</code></pre><p>你可以通过下述命令来暂存你的改动。</p><pre><code class="hljs bash">$ git add &lt;pattern&gt;</code></pre><p>其中 <code>&lt;pattern&gt;</code> 可以是文件路径，文件夹路径，通配符等等。 如果所有文件都需要被提交，你可以使用下述命令。</p><pre><code class="hljs bash">$ git add .</code></pre>]]></content>
    
    
    <categories>
      
      <category>Git</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Matrix Decomposition</title>
    <link href="/2020/11/04/2020/201104_matrix_factorization/"/>
    <url>/2020/11/04/2020/201104_matrix_factorization/</url>
    
    <content type="html"><![CDATA[<h2 id="Matrix-Decomposition"><a href="#Matrix-Decomposition" class="headerlink" title="Matrix Decomposition"></a>Matrix Decomposition</h2><ul><li><strong>PCA</strong>: Principle Component Analysis</li><li><strong>SVD</strong>: Singular Value Decomposition</li><li><strong>LDA</strong>: Linear Discriminant Analysis</li><li><strong>NMF</strong>: Non-negative Matrix Factorization</li><li><strong>NMF</strong>: with Sparse Constraints</li><li>Linear Sparse Coding</li></ul><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>$$A_{n \times m} = B_{n \times k}C_{k \times m}$$</p><ul><li>$B$ captures the common features in $A$</li><li>$C$ carries specific characteristics of the original samples </li><li>$n \times m \rightarrow (n+m) \times k$</li><li>$ k&lt;&lt;n $</li></ul><p><img src="/img/matrix.jpg" srcset="/img/loading.gif" alt="avatar"></p><ul><li>In PCA: $B$ is eigenvectors</li><li>In SVD: $B$ is right (column) eigenvectors </li><li>In LDA: $B$ is discriminant directions </li><li>In NMF: $B$ is local features</li></ul>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hidden Markov Model</title>
    <link href="/2020/11/02/2020/201102_hmm/"/>
    <url>/2020/11/02/2020/201102_hmm/</url>
    
    <content type="html"><![CDATA[<h2 id="隐马尔可夫模型（Hidden-Markov-Model）"><a href="#隐马尔可夫模型（Hidden-Markov-Model）" class="headerlink" title="隐马尔可夫模型（Hidden Markov Model）"></a>隐马尔可夫模型（Hidden Markov Model）</h2><p>$$\lambda = f(\boldsymbol{\pi}, \boldsymbol{A}, \boldsymbol{B})$$</p><ul><li>隐藏状态初始概率分布 $\boldsymbol{\pi}$</li><li>状态转移概率矩阵 $\boldsymbol{A}$</li><li>观测状态概率矩阵 $\boldsymbol{B}$</li></ul><h3 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h3><h4 id="1-概率计算："><a href="#1-概率计算：" class="headerlink" title="1. 概率计算："></a>1. 概率计算：</h4><p>已知系统输出 $\boldsymbol{Y}$ 及模型 $\lambda = f(\boldsymbol{\pi}, \boldsymbol{A}, \boldsymbol{B})$ 计算产生 $Y$ 的概率 $P(\boldsymbol{Y}|\lambda) $</p><h4 id="2-参数估计：训练"><a href="#2-参数估计：训练" class="headerlink" title="2. 参数估计：训练"></a>2. 参数估计：训练</h4><p>给定若干输出（训练样本）$Y$，确定模型 $\lambda = f(\boldsymbol{\pi}, \boldsymbol{A}, \boldsymbol{B})$ 的参数 </p><h4 id="3-最优状态序列搜索：识别"><a href="#3-最优状态序列搜索：识别" class="headerlink" title="3. 最优状态序列搜索：识别"></a>3. 最优状态序列搜索：识别</h4><p>已知输出 $\boldsymbol{Y}$ 及模型$\lambda = f(\boldsymbol{\pi}, \boldsymbol{A}, \boldsymbol{B})$，估计系统产生 $\boldsymbol{Y}$ 最可能的状态序列 $\boldsymbol{X}$</p><h3 id="两个假设"><a href="#两个假设" class="headerlink" title="两个假设"></a>两个假设</h3><p><strong>1. 齐次马尔可夫假设</strong>：即任意时刻的状态只依赖于前一时刻的状态，与其他时刻的状态无关（初始时刻的状态由参数 $\boldsymbol{\pi}$ 决定）</p><p><strong>2. 观测独立假设</strong>：即任意时刻的观测只依赖于该时刻的状态，与其他无关</p><h3 id="概率计算"><a href="#概率计算" class="headerlink" title="概率计算"></a>概率计算</h3><h4 id="1-前向概率"><a href="#1-前向概率" class="headerlink" title="1. 前向概率"></a>1. 前向概率</h4><p>$$\alpha_t(j)=P(o_1o_2 \dots o_{t-1}o{t}, q_t=s_j|\lambda)$$</p><h4 id="2-后向概率"><a href="#2-后向概率" class="headerlink" title="2. 后向概率"></a>2. 后向概率</h4><p>$$\beta_t(j)=P(o_1o_2 \dots o_{t-1}o{t} | q_t=s_j, \lambda)$$</p><h3 id="Viterbi-算法"><a href="#Viterbi-算法" class="headerlink" title="Viterbi 算法"></a>Viterbi 算法</h3><p>维特比算法是基于动态规划的求序列最短路径的方法</p><ul><li>输入：HMM模型 $\lambda = f(\boldsymbol{\pi}, \boldsymbol{A}, \boldsymbol{B})$，观测序列$O = o_1o_2 \dots o_T $</li><li>输出：最️有可能的隐藏状态序列</li></ul><h3 id="Baum-Welch-算法"><a href="#Baum-Welch-算法" class="headerlink" title="Baum-Welch 算法"></a>Baum-Welch 算法</h3><p>$$O = o_1o_2 \dots o_T \ \Rightarrow \ \lambda = \mathop{\arg\min}_{\lambda} P(O|\lambda)$$</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre><code class="Python"></code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/85454896" target="_blank" rel="noopener">知乎：一站式解决：隐马尔可夫模型（HMM）全过程推导及实现</a></li><li><a href="https://github.com/thuhcsi/DNN-HMM-Course" target="_blank" rel="noopener">DNN-HMM related Experiments for THUHCSI Course : Speech Signal Processing</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Trainer</title>
    <link href="/2020/11/01/research_note/17_trainer/"/>
    <url>/2020/11/01/research_note/17_trainer/</url>
    
    <content type="html"><![CDATA[<p>编写一个具有可复用性的机器学习代码模版。</p><p>当前机器学习特别是深度学习的代码大部分工作流程都是相同或是类似的。我想在 PyTorch 的基础构建一个通用的代码模板统一和规范，通过对模版的代码复用快速复现其他人的论文的结果，或是用于生产环境。</p><h2 id="PyTorch-Lightning"><a href="#PyTorch-Lightning" class="headerlink" title="PyTorch-Lightning"></a>PyTorch-Lightning</h2><p>调研了一下，我认为使用 <a href="https://pytorchlightning.ai/" target="_blank" rel="noopener">PyTorch-Lightning</a> 会是一个不错的选择。</p><h2 id="工程文件结构"><a href="#工程文件结构" class="headerlink" title="工程文件结构"></a>工程文件结构</h2><h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><ol><li>CheckPoint 和 超参数 的 save 和 reload</li><li>数据可视化(loss, T-sne)</li><li>高效率的DataLoader</li><li>分布式多 GPU 训练</li><li>自动生成文档</li><li>warmup, early stop, lr_scheduler</li></ol><h2 id="CheckPoint-和-超参数-的-save-和-reload"><a href="#CheckPoint-和-超参数-的-save-和-reload" class="headerlink" title="CheckPoint 和 超参数 的 save 和 reload"></a>CheckPoint 和 超参数 的 save 和 reload</h2><p>看了很多别人的代码，很多都是使用 yaml 文件配置超参数和模型结构，使用特定的命令去调用，例如 opennmt-py。</p><p>但这并不是我想要的， 我认为首先应该约定模型 checkpoint 的存放文件夹和超参数的存放文件名</p><h2 id="数据可视化-loss-T-sne"><a href="#数据可视化-loss-T-sne" class="headerlink" title="数据可视化(loss, T-sne)"></a>数据可视化(loss, T-sne)</h2><ul><li>wandb</li><li>TensorBoardX</li></ul><h2 id="高效率的DataLoader"><a href="#高效率的DataLoader" class="headerlink" title="高效率的DataLoader"></a>高效率的DataLoader</h2><p>基于 PyTorch 编写的很多代码模型训练速度瓶颈常在于数据 IO</p><h3 id="DataLoader-在-RNN-和-LSTM-上的不同"><a href="#DataLoader-在-RNN-和-LSTM-上的不同" class="headerlink" title="DataLoader 在 RNN 和 LSTM 上的不同"></a>DataLoader 在 RNN 和 LSTM 上的不同</h3><h2 id="分布式多GPU训练"><a href="#分布式多GPU训练" class="headerlink" title="分布式多GPU训练"></a>分布式多GPU训练</h2><ul><li>dp</li><li>ddp</li><li>ddp2</li></ul>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>笔记</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python 矩阵乘法</title>
    <link href="/2020/11/01/2020/21101_mul/"/>
    <url>/2020/11/01/2020/21101_mul/</url>
    
    <content type="html"><![CDATA[<h2 id="Test-Data"><a href="#Test-Data" class="headerlink" title="Test Data"></a>Test Data</h2><p>$$<br>A =<br> \left[<br> \begin{matrix}<br>   1 &amp; 2 \\<br>   3 &amp; 4<br>  \end{matrix}<br>  \right]<br>\ \ \ <br>B =<br> \left[<br> \begin{matrix}<br>   4 &amp; 3 \\<br>   2 &amp; 1<br>  \end{matrix}<br>  \right]<br>$$</p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npmat_a = np.matrix([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])mat_b = np.matrix([[<span class="hljs-number">4</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>]])print(<span class="hljs-string">"mat_a"</span>)print(mat_a)print(<span class="hljs-string">"mat_b"</span>)print(mat_b)print(<span class="hljs-string">"dot"</span>)print(np.dot(mat_a, mat_b))print(<span class="hljs-string">"matmul"</span>)print(np.matmul(mat_a, mat_b))print(<span class="hljs-string">"multiply"</span>)print(np.multiply(mat_a, mat_b))print(<span class="hljs-string">"*"</span>)print(mat_a * mat_b)print(<span class="hljs-string">"@"</span>)print(mat_a @ mat_b)</code></pre><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>除了<code>np.multiply</code>是$ \left[ \begin{matrix} 4 &amp; 6 \\ 6 &amp; 4 \end{matrix}\right]$ 外，其它都是$ \left[ \begin{matrix} 8 &amp; 5 \\ 20 &amp; 13 \end{matrix}\right]$</p>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RPC 与 REST</title>
    <link href="/2020/08/30/2020/200830_rpc_rest/"/>
    <url>/2020/08/30/2020/200830_rpc_rest/</url>
    
    <content type="html"><![CDATA[<ul><li>RPC面向过程，只发送 GET 和 POST 请求。GET用来查询信息，其他情况下一律用POST。请求参数是动词，直接描述动作本身。</li><li>RESTful面向资源，使用 POST、DELETE、PUT、GET 请求，分别对应增、删、改、查操作。请求参数是名词，这个名词就是“增删改查”想要操作的对象。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/66311537" target="_blank" rel="noopener">通俗易懂 RPC、REST、Dubbo、HTTP、RMI 的区别与联系</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>WEB</category>
      
    </categories>
    
    
    <tags>
      
      <tag>WEB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Parallelized Command</title>
    <link href="/2020/08/29/2020/200810_parallelize/"/>
    <url>/2020/08/29/2020/200810_parallelize/</url>
    
    <content type="html"><![CDATA[<h2 id="命令行：-尖括号-中括号的含义"><a href="#命令行：-尖括号-中括号的含义" class="headerlink" title="命令行： 尖括号 中括号的含义"></a>命令行： 尖括号 中括号的含义</h2><ol><li><code>[]</code>：内的内容意思是：可写可不写</li><li><code>{}</code>：那就必须要在{}内给出的选择里选一个。</li><li><code>&lt;&gt;</code>：表示必选</li></ol><h2 id="Kaldi-run-pl-amp-queue-pl"><a href="#Kaldi-run-pl-amp-queue-pl" class="headerlink" title="Kaldi run.pl &amp; queue.pl"></a>Kaldi run.pl &amp; queue.pl</h2><p>使用方法 <code>run.pl &lt;options&gt; &lt;log-file&gt; &lt;command&gt;</code></p><pre><code class="hljs bash">├── run.pl├── run.sh├── cmd_1.sh└── cmd_2.sh</code></pre><p><strong>run.sh</strong></p><pre><code class="hljs bash"><span class="hljs-comment"># "JOB" is core to control</span>nj=2./run.pl JOB=1:<span class="hljs-variable">$nj</span> test.JOB.log bash cmd_JOB.sh</code></pre><h2 id="wait-和-amp-命令"><a href="#wait-和-amp-命令" class="headerlink" title="wait 和 &amp; 命令"></a>wait 和 &amp; 命令</h2><ul><li>在每个进程中使用&amp;符号进行让脚本在后台运行，无需等待当前进程结束。</li><li>为了确保每个进程都执行完成，最后务必使用 wait 关键字，用来确保每一个子进程都执行完成。</li><li>&amp; + wait 方法对线程并发数不可控</li></ul><pre><code class="hljs bash">./1.sh &amp;./2.sh &amp;<span class="hljs-built_in">wait</span></code></pre><pre><code class="hljs bash"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> `seq 1 3`<span class="hljs-keyword">do</span>&#123;       <span class="hljs-built_in">echo</span> <span class="hljs-variable">$i</span>&#125;&amp;<span class="hljs-keyword">done</span></code></pre><h2 id="模拟队列"><a href="#模拟队列" class="headerlink" title="模拟队列"></a>模拟队列</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><p><a href="https://blog.csdn.net/zongza/article/details/89715095" target="_blank" rel="noopener">[ASR_THU] flac文件转wav文件</a></p></li><li><p><a href="https://blog.csdn.net/zongza/article/details/89707462" target="_blank" rel="noopener">[ASR_THU] bash 脚本实现批量化作业与并行处理</a></p></li><li><p><a href="https://blog.csdn.net/yj13811596648/article/details/102817366" target="_blank" rel="noopener">[CSDN] make_mfcc.sh</a></p></li><li><p><a href="https://blog.csdn.net/yangshangwei/article/details/87691890" target="_blank" rel="noopener">使用&amp;和wait让你的脚本并行执行</a></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>shell</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kaldi</tag>
      
      <tag>shell</tag>
      
      <tag>Linix</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The art of Command line</title>
    <link href="/2020/08/29/2020/200829_cmd_art/"/>
    <url>/2020/08/29/2020/200829_cmd_art/</url>
    
    <content type="html"><![CDATA[<h1 id="命令行的艺术"><a href="#命令行的艺术" class="headerlink" title="命令行的艺术"></a><a href="https://github.com/jlevy/the-art-of-command-line/blob/master/README-zh.md" target="_blank" rel="noopener">命令行的艺术</a></h1><ul><li><a href="#前言">前言</a></li><li><a href="#基础">基础</a></li><li><a href="#日常使用">日常使用</a></li><li><a href="#文件及数据处理">文件及数据处理</a></li><li><a href="#系统调试">系统调试</a></li><li><a href="#单行脚本">单行脚本</a></li><li><a href="#冷门但有用">冷门但有用</a></li><li><a href="#仅限-os-x-系统">仅限 OS X 系统</a></li><li><a href="#仅限-windows-系统">仅限 Windows 系统</a></li><li><a href="#更多资源">更多资源</a></li><li><a href="#免责声明">免责声明</a></li></ul><p>熟练使用命令行是一种常常被忽视，或被认为难以掌握的技能，但实际上，它会提高你作为工程师的灵活性以及生产力。本文是一份我在 Linux 上工作时，发现的一些命令行使用技巧的摘要。有些技巧非常基础，而另一些则相当复杂，甚至晦涩难懂。这篇文章并不长，但当你能够熟练掌握这里列出的所有技巧时，你就学会了很多关于命令行的东西了。</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>涵盖范围：</p><ul><li>这篇文章不仅能帮助刚接触命令行的新手，而且对具有经验的人也大有裨益。本文致力于做到<em>覆盖面广</em>（涉及所有重要的内容），<em>具体</em>（给出具体的最常用的例子），以及<em>简洁</em>（避免冗余的内容，或是可以在其他地方轻松查到的细枝末节）。在特定应用场景下，本文的内容属于基本功或者能帮助您节约大量的时间。</li><li>本文主要为 Linux 所写，但在<a href="#仅限-os-x-系统">仅限 OS X 系统</a>章节和<a href="#仅限-windows-系统">仅限 Windows 系统</a>章节中也包含有对应操作系统的内容。除去这两个章节外，其它的内容大部分均可在其他类 Unix 系统或 OS X，甚至 Cygwin 中得到应用。</li><li>本文主要关注于交互式 Bash，但也有很多技巧可以应用于其他 shell 和 Bash 脚本当中。</li><li>除去“标准的”Unix 命令，本文还包括了一些依赖于特定软件包的命令（前提是它们具有足够的价值）。</li></ul><p>注意事项：</p><ul><li>为了能在一页内展示尽量多的东西，一些具体的信息可以在引用的页面中找到。我们相信机智的你知道如何使用 Google 或者其他搜索引擎来查阅到更多的详细信息。文中部分命令需要您使用 <code>apt-get</code>，<code>yum</code>，<code>dnf</code>，<code>pacman</code>，<br><code>pip</code> 或 <code>brew</code>（以及其它合适的包管理器）来安装依赖的程序。</li><li>遇到问题的话，请尝试使用 <a href="http://explainshell.com/" target="_blank" rel="noopener">Explainshell</a> 去获取相关命令、参数、管道等内容的解释。</li></ul><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><ul><li><p>学习 Bash 的基础知识。具体地，在命令行中输入 <code>man bash</code> 并至少全文浏览一遍; 它理解起来很简单并且不冗长。其他的 shell 可能很好用，但 Bash 的功能已经足够强大并且到几乎总是可用的（ 如果你<em>只</em>学习 zsh，fish 或其他的 shell 的话，在你自己的设备上会显得很方便，但过度依赖这些功能会给您带来不便，例如当你需要在服务器上工作时）。</p></li><li><p>熟悉至少一个基于文本的编辑器。通常而言 Vim （<code>vi</code>） 会是你最好的选择，毕竟在终端中编辑文本时 Vim 是最好用的工具（甚至大部分情况下 Vim 要比 Emacs、大型 IDE 或是炫酷的编辑器更好用）。</p></li><li><p>学会如何使用 <code>man</code> 命令去阅读文档。学会使用 <code>apropos</code> 去查找文档。知道有些命令并不对应可执行文件，而是在 Bash 内置好的，此时可以使用 <code>help</code> 和 <code>help -d</code> 命令获取帮助信息。你可以用 <code>type 命令</code> 来判断这个命令到底是可执行文件、shell 内置命令还是别名。</p></li><li><p>学会使用 <code>&gt;</code> 和 <code>&lt;</code> 来重定向输出和输入，学会使用 <code>|</code> 来重定向管道。明白 <code>&gt;</code> 会覆盖了输出文件而 <code>&gt;&gt;</code> 是在文件末添加。了解标准输出 stdout 和标准错误 stderr。</p></li><li><p>学会使用通配符 <code>*</code> （或许再算上 <code>?</code> 和 <code>[</code>…<code>]</code>） 和引用以及引用中 <code>&#39;</code> 和 <code>&quot;</code> 的区别（后文中有一些具体的例子）。</p></li><li><p>熟悉 Bash 中的任务管理工具：<code>&amp;</code>，<strong>ctrl-z</strong>，<strong>ctrl-c</strong>，<code>jobs</code>，<code>fg</code>，<code>bg</code>，<code>kill</code> 等。</p></li><li><p>学会使用 <code>ssh</code> 进行远程命令行登录，最好知道如何使用 <code>ssh-agent</code>，<code>ssh-add</code> 等命令来实现基础的无密码认证登录。</p></li><li><p>学会基本的文件管理工具：<code>ls</code> 和 <code>ls -l</code> （了解 <code>ls -l</code> 中每一列代表的意义），<code>less</code>，<code>head</code>，<code>tail</code> 和 <code>tail -f</code> （甚至 <code>less +F</code>），<code>ln</code> 和 <code>ln -s</code> （了解硬链接与软链接的区别），<code>chown</code>，<code>chmod</code>，<code>du</code> （硬盘使用情况概述：<code>du -hs *</code>）。 关于文件系统的管理，学习 <code>df</code>，<code>mount</code>，<code>fdisk</code>，<code>mkfs</code>，<code>lsblk</code>。知道 inode 是什么（与 <code>ls -i</code> 和 <code>df -i</code> 等命令相关）。</p></li><li><p>学习基本的网络管理工具：<code>ip</code> 或 <code>ifconfig</code>，<code>dig</code>。</p></li><li><p>学习并使用一种版本控制管理系统，例如 <code>git</code>。</p></li><li><p>熟悉正则表达式，学会使用 <code>grep</code>／<code>egrep</code>，它们的参数中 <code>-i</code>，<code>-o</code>，<code>-v</code>，<code>-A</code>，<code>-B</code> 和 <code>-C</code> 这些是很常用并值得认真学习的。</p></li><li><p>学会使用 <code>apt-get</code>，<code>yum</code>，<code>dnf</code> 或 <code>pacman</code> （具体使用哪个取决于你使用的 Linux 发行版）来查找和安装软件包。并确保你的环境中有 <code>pip</code> 来安装基于 Python 的命令行工具 （接下来提到的部分程序使用 <code>pip</code> 来安装会很方便）。</p></li></ul><h2 id="日常使用"><a href="#日常使用" class="headerlink" title="日常使用"></a>日常使用</h2><ul><li><p>在 Bash 中，可以通过按 <strong>Tab</strong> 键实现自动补全参数，使用 <strong>ctrl-r</strong> 搜索命令行历史记录（按下按键之后，输入关键字便可以搜索，重复按下 <strong>ctrl-r</strong> 会向后查找匹配项，按下 <strong>Enter</strong> 键会执行当前匹配的命令，而按下右方向键会将匹配项放入当前行中，不会直接执行，以便做出修改）。</p></li><li><p>在 Bash 中，可以按下 <strong>ctrl-w</strong> 删除你键入的最后一个单词，<strong>ctrl-u</strong> 可以删除行内光标所在位置之前的内容，<strong>alt-b</strong> 和 <strong>alt-f</strong> 可以以单词为单位移动光标，<strong>ctrl-a</strong> 可以将光标移至行首，<strong>ctrl-e</strong> 可以将光标移至行尾，<strong>ctrl-k</strong> 可以删除光标至行尾的所有内容，<strong>ctrl-l</strong> 可以清屏。键入 <code>man readline</code> 可以查看 Bash 中的默认快捷键。内容有很多，例如 <strong>alt-.</strong> 循环地移向前一个参数，而 <strong>alt-*</strong> 可以展开通配符。</p></li></ul><ul><li><p>你喜欢的话，可以执行 <code>set -o vi</code> 来使用 vi 风格的快捷键，而执行 <code>set -o emacs</code> 可以把它改回来。</p></li><li><p>为了便于编辑长命令，在设置你的默认编辑器后（例如 <code>export EDITOR=vim</code>），<strong>ctrl-x</strong> <strong>ctrl-e</strong> 会打开一个编辑器来编辑当前输入的命令。在 vi 风格下快捷键则是 <strong>escape-v</strong>。</p></li><li><p>键入 <code>history</code> 查看命令行历史记录，再用 <code>!n</code>（<code>n</code> 是命令编号）就可以再次执行。其中有许多缩写，最有用的大概就是 <code>!$</code>， 它用于指代上次键入的参数，而 <code>!!</code> 可以指代上次键入的命令了（参考 man 页面中的“HISTORY EXPANSION”）。不过这些功能，你也可以通过快捷键 <strong>ctrl-r</strong> 和 <strong>alt-.</strong> 来实现。</p></li><li><p><code>cd</code> 命令可以切换工作路径，输入 <code>cd ~</code> 可以进入 home 目录。要访问你的 home 目录中的文件，可以使用前缀 <code>~</code>（例如 <code>~/.bashrc</code>）。在 <code>sh</code> 脚本里则用环境变量 <code>$HOME</code> 指代 home 目录的路径。</p></li><li><p>回到前一个工作路径：<code>cd -</code>。</p></li><li><p>如果你输入命令的时候中途改了主意，按下 <strong>alt-#</strong> 在行首添加 <code>#</code> 把它当做注释再按下回车执行（或者依次按下 <strong>ctrl-a</strong>， <strong>#</strong>， <strong>enter</strong>）。这样做的话，之后借助命令行历史记录，你可以很方便恢复你刚才输入到一半的命令。</p></li><li><p>使用 <code>xargs</code> （ 或 <code>parallel</code>）。他们非常给力。注意到你可以控制每行参数个数（<code>-L</code>）和最大并行数（<code>-P</code>）。如果你不确定它们是否会按你想的那样工作，先使用 <code>xargs echo</code> 查看一下。此外，使用 <code>-I{}</code> 会很方便。例如：</p><pre><code class="hljs bash">find . -name <span class="hljs-string">'*.py'</span> | xargs grep some_functioncat hosts | xargs -I&#123;&#125; ssh root@&#123;&#125; hostname</code></pre></li></ul><ul><li><p><code>pstree -p</code> 以一种优雅的方式展示进程树。</p></li><li><p>使用 <code>pgrep</code> 和 <code>pkill</code> 根据名字查找进程或发送信号（<code>-f</code> 参数通常有用）。</p></li><li><p>了解你可以发往进程的信号的种类。比如，使用 <code>kill -STOP [pid]</code> 停止一个进程。使用 <code>man 7 signal</code> 查看详细列表。</p></li><li><p>使用 <code>nohup</code> 或 <code>disown</code> 使一个后台进程持续运行。</p></li><li><p>使用 <code>netstat -lntp</code> 或 <code>ss -plat</code> 检查哪些进程在监听端口（默认是检查 TCP 端口; 添加参数 <code>-u</code> 则检查 UDP 端口）或者 <code>lsof -iTCP -sTCP:LISTEN -P -n</code> (这也可以在 OS X 上运行)。</p></li><li><p><code>lsof</code> 来查看开启的套接字和文件。</p></li><li><p>使用 <code>uptime</code> 或 <code>w</code> 来查看系统已经运行多长时间。</p></li><li><p>使用 <code>alias</code> 来创建常用命令的快捷形式。例如：<code>alias ll=&#39;ls -latr&#39;</code> 创建了一个新的命令别名 <code>ll</code>。</p></li><li><p>可以把别名、shell 选项和常用函数保存在 <code>~/.bashrc</code>，具体看下这篇<a href="http://superuser.com/a/183980/7106" target="_blank" rel="noopener">文章</a>。这样做的话你就可以在所有 shell 会话中使用你的设定。</p></li><li><p>把环境变量的设定以及登陆时要执行的命令保存在 <code>~/.bash_profile</code>。而对于从图形界面启动的 shell 和 <code>cron</code> 启动的 shell，则需要单独配置文件。</p></li><li><p>要想在几台电脑中同步你的配置文件（例如 <code>.bashrc</code> 和 <code>.bash_profile</code>），可以借助 Git。</p></li><li><p>当变量和文件名中包含空格的时候要格外小心。Bash 变量要用引号括起来，比如 <code>&quot;$FOO&quot;</code>。尽量使用 <code>-0</code> 或 <code>-print0</code> 选项以便用 NULL 来分隔文件名，例如 <code>locate -0 pattern | xargs -0 ls -al</code> 或 <code>find / -print0 -type d | xargs -0 ls -al</code>。如果 for 循环中循环访问的文件名含有空字符（空格、tab 等字符），只需用 <code>IFS=$&#39;\n&#39;</code> 把内部字段分隔符设为换行符。</p></li><li><p>在 Bash 脚本中，使用 <code>set -x</code> 去调试输出（或者使用它的变体 <code>set -v</code>，它会记录原始输入，包括多余的参数和注释）。尽可能地使用严格模式：使用 <code>set -e</code> 令脚本在发生错误时退出而不是继续运行；使用 <code>set -u</code> 来检查是否使用了未赋值的变量；试试 <code>set -o pipefail</code>，它可以监测管道中的错误。当牵扯到很多脚本时，使用 <code>trap</code> 来检测 ERR 和 EXIT。一个好的习惯是在脚本文件开头这样写，这会使它能够检测一些错误，并在错误发生时中断程序并输出信息：</p><pre><code class="hljs bash"><span class="hljs-built_in">set</span> -euo pipefail<span class="hljs-built_in">trap</span> <span class="hljs-string">"echo 'error: Script failed: see failed command above'"</span> ERR</code></pre></li><li><p>在 Bash 脚本中，子 shell（使用括号 <code>(...)</code>）是一种组织参数的便捷方式。一个常见的例子是临时地移动工作路径，代码如下：</p><pre><code class="hljs bash"><span class="hljs-comment"># do something in current dir</span>(<span class="hljs-built_in">cd</span> /some/other/dir &amp;&amp; other-command)<span class="hljs-comment"># continue in original dir</span></code></pre></li><li><p>在 Bash 中，变量有许多的扩展方式。<code>${name:?error message}</code> 用于检查变量是否存在。此外，当 Bash 脚本只需要一个参数时，可以使用这样的代码 <code>input_file=${1:?usage: $0 input_file}</code>。在变量为空时使用默认值：<code>${name:-default}</code>。如果你要在之前的例子中再加一个（可选的）参数，可以使用类似这样的代码 <code>output_file=${2:-logfile}</code>，如果省略了 $2，它的值就为空，于是 <code>output_file</code> 就会被设为 <code>logfile</code>。数学表达式：<code>i=$(( (i + 1) % 5 ))</code>。序列：<code>{1..10}</code>。截断字符串：<code>${var%suffix}</code> 和 <code>${var#prefix}</code>。例如，假设 <code>var=foo.pdf</code>，那么 <code>echo ${var%.pdf}.txt</code> 将输出 <code>foo.txt</code>。</p></li><li><p>使用括号扩展（<code>{</code>…<code>}</code>）来减少输入相似文本，并自动化文本组合。这在某些情况下会很有用，例如 <code>mv foo.{txt,pdf} some-dir</code>（同时移动两个文件），<code>cp somefile{,.bak}</code>（会被扩展成 <code>cp somefile somefile.bak</code>）或者 <code>mkdir -p test-{a,b,c}/subtest-{1,2,3}</code>（会被扩展成所有可能的组合，并创建一个目录树）。</p></li><li><p>通过使用 <code>&lt;(some command)</code> 可以将输出视为文件。例如，对比本地文件 <code>/etc/hosts</code> 和一个远程文件：</p><pre><code class="hljs sh">diff /etc/hosts &lt;(ssh somehost cat /etc/hosts)</code></pre></li><li><p>编写脚本时，你可能会想要把代码都放在大括号里。缺少右括号的话，代码就会因为语法错误而无法执行。如果你的脚本是要放在网上分享供他人使用的，这样的写法就体现出它的好处了，因为这样可以防止下载不完全代码被执行。</p><pre><code class="hljs bash">&#123;      <span class="hljs-comment"># 在这里写代码</span>&#125;</code></pre></li><li><p>了解 Bash 中的“here documents”，例如 <code>cat &lt;&lt;EOF ...</code>。</p></li><li><p>在 Bash 中，同时重定向标准输出和标准错误：<code>some-command &gt;logfile 2&gt;&amp;1</code> 或者 <code>some-command &amp;&gt;logfile</code>。通常，为了保证命令不会在标准输入里残留一个未关闭的文件句柄捆绑在你当前所在的终端上，在命令后添加 <code>&lt;/dev/null</code> 是一个好习惯。</p></li><li><p>使用 <code>man ascii</code> 查看具有十六进制和十进制值的ASCII表。<code>man unicode</code>，<code>man utf-8</code>，以及 <code>man latin1</code> 有助于你去了解通用的编码信息。</p></li><li><p>使用 <code>screen</code> 或 <a href="https://tmux.github.io/" target="_blank" rel="noopener"><code>tmux</code></a> 来使用多份屏幕，当你在使用 ssh 时（保存 session 信息）将尤为有用。而 <code>byobu</code> 可以为它们提供更多的信息和易用的管理工具。另一个轻量级的 session 持久化解决方案是 <a href="https://github.com/bogner/dtach" target="_blank" rel="noopener"><code>dtach</code></a>。</p></li><li><p>ssh 中，了解如何使用 <code>-L</code> 或 <code>-D</code>（偶尔需要用 <code>-R</code>）开启隧道是非常有用的，比如当你需要从一台远程服务器上访问 web 页面。</p></li><li><p>对 ssh 设置做一些小优化可能是很有用的，例如这个 <code>~/.ssh/config</code> 文件包含了防止特定网络环境下连接断开、压缩数据、多通道等选项：</p><pre><code class="hljs routeros"><span class="hljs-attribute">TCPKeepAlive</span>=<span class="hljs-literal">yes</span><span class="hljs-attribute">ServerAliveInterval</span>=15<span class="hljs-attribute">ServerAliveCountMax</span>=6<span class="hljs-attribute">Compression</span>=<span class="hljs-literal">yes</span>ControlMaster autoControlPath /tmp/%r@%h:%pControlPersist <span class="hljs-literal">yes</span></code></pre></li><li><p>一些其他的关于 ssh 的选项是与安全相关的，应当小心翼翼的使用。例如你应当只能在可信任的网络中启用 <code>StrictHostKeyChecking=no</code>，<code>ForwardAgent=yes</code>。</p></li><li><p>考虑使用 <a href="https://mosh.mit.edu/" target="_blank" rel="noopener"><code>mosh</code></a> 作为 ssh 的替代品，它使用 UDP 协议。它可以避免连接被中断并且对带宽需求更小，但它需要在服务端做相应的配置。</p></li><li><p>获取八进制形式的文件访问权限（修改系统设置时通常需要，但 <code>ls</code> 的功能不那么好用并且通常会搞砸），可以使用类似如下的代码：</p><pre><code class="hljs sh"><span class="hljs-built_in">stat</span> -c <span class="hljs-string">'%A %a %n'</span> /etc/timezone</code></pre></li><li><p>使用 <a href="https://github.com/mooz/percol" target="_blank" rel="noopener"><code>percol</code></a> 或者 <a href="https://github.com/junegunn/fzf" target="_blank" rel="noopener"><code>fzf</code></a> 可以交互式地从另一个命令输出中选取值。</p></li><li><p>使用 <code>fpp</code>（<a href="https://github.com/facebook/PathPicker" target="_blank" rel="noopener">PathPicker</a>）可以与基于另一个命令(例如 <code>git</code>）输出的文件交互。</p></li><li><p>将 web 服务器上当前目录下所有的文件（以及子目录）暴露给你所处网络的所有用户，使用：<br><code>python -m SimpleHTTPServer 7777</code> （使用端口 7777 和 Python 2）或<code>python -m http.server 7777</code> （使用端口 7777 和 Python 3）。</p></li><li><p>以其他用户的身份执行命令，使用 <code>sudo</code>。默认以 root 用户的身份执行；使用 <code>-u</code> 来指定其他用户。使用 <code>-i</code> 来以该用户登录（需要输入_你自己的_密码）。</p></li><li><p>将 shell 切换为其他用户，使用 <code>su username</code> 或者 <code>sudo - username</code>。加入 <code>-</code> 会使得切换后的环境与使用该用户登录后的环境相同。省略用户名则默认为 root。切换到哪个用户，就需要输入_哪个用户的_密码。</p></li><li><p>了解命令行的 <a href="https://wiki.debian.org/CommonErrorMessages/ArgumentListTooLong" target="_blank" rel="noopener">128K 限制</a>。使用通配符匹配大量文件名时，常会遇到“Argument list too long”的错误信息。（这种情况下换用 <code>find</code> 或 <code>xargs</code> 通常可以解决。）</p></li><li><p>当你需要一个基本的计算器时，可以使用 <code>python</code> 解释器（当然你要用 python 的时候也是这样）。例如：</p><pre><code class="hljs angelscript">&gt;&gt;&gt; <span class="hljs-number">2</span>+<span class="hljs-number">3</span><span class="hljs-number">5</span></code></pre></li></ul><h2 id="文件及数据处理"><a href="#文件及数据处理" class="headerlink" title="文件及数据处理"></a>文件及数据处理</h2><ul><li><p>在当前目录下通过文件名查找一个文件，使用类似于这样的命令：<code>find . -iname &#39;*something*&#39;</code>。在所有路径下通过文件名查找文件，使用 <code>locate something</code> （但注意到 <code>updatedb</code> 可能没有对最近新建的文件建立索引，所以你可能无法定位到这些未被索引的文件）。</p></li><li><p>使用 <a href="https://github.com/ggreer/the_silver_searcher" target="_blank" rel="noopener"><code>ag</code></a> 在源代码或数据文件里检索（<code>grep -r</code> 同样可以做到，但相比之下 <code>ag</code> 更加先进）。</p></li><li><p>将 HTML 转为文本：<code>lynx -dump -stdin</code>。</p></li><li><p>Markdown，HTML，以及所有文档格式之间的转换，试试 <a href="http://pandoc.org/" target="_blank" rel="noopener"><code>pandoc</code></a>。</p></li><li><p>当你要处理棘手的 XML 时候，<code>xmlstarlet</code> 算是上古时代流传下来的神器。</p></li><li><p>使用 <a href="http://stedolan.github.io/jq/" target="_blank" rel="noopener"><code>jq</code></a> 处理 JSON。</p></li><li><p>使用 <a href="https://github.com/0k/shyaml" target="_blank" rel="noopener"><code>shyaml</code></a> 处理 YAML。</p></li><li><p>要处理 Excel 或 CSV 文件的话，<a href="https://github.com/onyxfish/csvkit" target="_blank" rel="noopener">csvkit</a> 提供了 <code>in2csv</code>，<code>csvcut</code>，<code>csvjoin</code>，<code>csvgrep</code> 等方便易用的工具。</p></li><li><p>当你要处理 Amazon S3 相关的工作的时候，<a href="https://github.com/s3tools/s3cmd" target="_blank" rel="noopener"><code>s3cmd</code></a> 是一个很方便的工具而 <a href="https://github.com/bloomreach/s4cmd" target="_blank" rel="noopener"><code>s4cmd</code></a> 的效率更高。Amazon 官方提供的 <a href="https://github.com/aws/aws-cli" target="_blank" rel="noopener"><code>aws</code></a> 以及  <a href="https://github.com/donnemartin/saws" target="_blank" rel="noopener"><code>saws</code></a> 是其他 AWS 相关工作的基础，值得学习。</p></li><li><p>了解如何使用 <code>sort</code> 和 <code>uniq</code>，包括 uniq 的 <code>-u</code> 参数和 <code>-d</code> 参数，具体内容在后文单行脚本节中。另外可以了解一下 <code>comm</code>。</p></li><li><p>了解如何使用 <code>cut</code>，<code>paste</code> 和 <code>join</code> 来更改文件。很多人都会使用 <code>cut</code>，但遗忘了 <code>join</code>。</p></li><li><p>了解如何运用 <code>wc</code> 去计算新行数（<code>-l</code>），字符数（<code>-m</code>），单词数（<code>-w</code>）以及字节数（<code>-c</code>）。</p></li><li><p>了解如何使用 <code>tee</code> 将标准输入复制到文件甚至标准输出，例如 <code>ls -al | tee file.txt</code>。</p></li><li><p>要进行一些复杂的计算，比如分组、逆序和一些其他的统计分析，可以考虑使用 <a href="https://www.gnu.org/software/datamash/" target="_blank" rel="noopener"><code>datamash</code></a>。</p></li><li><p>注意到语言设置（中文或英文等）对许多命令行工具有一些微妙的影响，比如排序的顺序和性能。大多数 Linux 的安装过程会将 <code>LANG</code> 或其他有关的变量设置为符合本地的设置。要意识到当你改变语言设置时，排序的结果可能会改变。明白国际化可能会使 sort 或其他命令运行效率下降<em>许多倍</em>。某些情况下（例如集合运算）你可以放心的使用 <code>export LC_ALL=C</code> 来忽略掉国际化并按照字节来判断顺序。</p></li><li><p>你可以单独指定某一条命令的环境，只需在调用时把环境变量设定放在命令的前面，例如 <code>TZ=Pacific/Fiji date</code> 可以获取斐济的时间。</p></li><li><p>了解如何使用 <code>awk</code> 和 <code>sed</code> 来进行简单的数据处理。 参阅 <a href="#one-liners">One-liners</a> 获取示例。</p></li><li><p>替换一个或多个文件中出现的字符串：</p><pre><code class="hljs sh">perl -pi.bak -e <span class="hljs-string">'s/old-string/new-string/g'</span> my-files-*.txt</code></pre></li><li><p>使用 <a href="https://github.com/jlevy/repren" target="_blank" rel="noopener"><code>repren</code></a> 来批量重命名文件，或是在多个文件中搜索替换内容。（有些时候 <code>rename</code> 命令也可以批量重命名，但要注意，它在不同 Linux 发行版中的功能并不完全一样。）</p><pre><code class="hljs sh"><span class="hljs-comment"># 将文件、目录和内容全部重命名 foo -&gt; bar:</span>repren --full --preserve-case --from foo --to bar .<span class="hljs-comment"># 还原所有备份文件 whatever.bak -&gt; whatever:</span>repren --renames --from <span class="hljs-string">'(.*)\.bak'</span> --to <span class="hljs-string">'\1'</span> *.bak<span class="hljs-comment"># 用 rename 实现上述功能（若可用）:</span>rename <span class="hljs-string">'s/\.bak$//'</span> *.bak</code></pre></li><li><p>根据 man 页面的描述，<code>rsync</code> 是一个快速且非常灵活的文件复制工具。它闻名于设备之间的文件同步，但其实它在本地情况下也同样有用。在安全设置允许下，用 <code>rsync</code> 代替 <code>scp</code> 可以实现文件续传，而不用重新从头开始。它同时也是删除大量文件的<a href="https://web.archive.org/web/20130929001850/http://linuxnote.net/jianingy/en/linux/a-fast-way-to-remove-huge-number-of-files.html" target="_blank" rel="noopener">最快方法</a>之一：</p><pre><code class="hljs sh">mkdir empty &amp;&amp; rsync -r --delete empty/ some-dir &amp;&amp; rmdir some-dir</code></pre></li><li><p>若要在复制文件时获取当前进度，可使用 <code>pv</code>，<a href="https://github.com/dmerejkowsky/pycp" target="_blank" rel="noopener"><code>pycp</code></a>，<a href="https://github.com/Xfennec/progress" target="_blank" rel="noopener"><code>progress</code></a>，<code>rsync --progress</code>。若所执行的复制为block块拷贝，可以使用 <code>dd status=progress</code>。</p></li><li><p>使用 <code>shuf</code> 可以以行为单位来打乱文件的内容或从一个文件中随机选取多行。</p></li><li><p>了解 <code>sort</code> 的参数。显示数字时，使用 <code>-n</code> 或者 <code>-h</code> 来显示更易读的数（例如 <code>du -h</code> 的输出）。明白排序时关键字的工作原理（<code>-t</code> 和 <code>-k</code>）。例如，注意到你需要 <code>-k1，1</code> 来仅按第一个域来排序，而 <code>-k1</code> 意味着按整行排序。稳定排序（<code>sort -s</code>）在某些情况下很有用。例如，以第二个域为主关键字，第一个域为次关键字进行排序，你可以使用 <code>sort -k1，1 | sort -s -k2，2</code>。</p></li><li><p>如果你想在 Bash 命令行中写 tab 制表符，按下 <strong>ctrl-v</strong> <strong>[Tab]</strong> 或键入 <code>$&#39;\t&#39;</code> （后者可能更好，因为你可以复制粘贴它）。</p></li><li><p>标准的源代码对比及合并工具是 <code>diff</code> 和 <code>patch</code>。使用 <code>diffstat</code> 查看变更总览数据。注意到 <code>diff -r</code> 对整个文件夹有效。使用 <code>diff -r tree1 tree2 | diffstat</code> 查看变更的统计数据。<code>vimdiff</code> 用于比对并编辑文件。</p></li><li><p>对于二进制文件，使用 <code>hd</code>，<code>hexdump</code> 或者 <code>xxd</code> 使其以十六进制显示，使用 <code>bvi</code>，<code>hexedit</code> 或者 <code>biew</code> 来进行二进制编辑。</p></li><li><p>同样对于二进制文件，<code>strings</code>（包括 <code>grep</code> 等工具）可以帮助在二进制文件中查找特定比特。</p></li><li><p>制作二进制差分文件（Delta 压缩），使用 <code>xdelta3</code>。</p></li><li><p>使用 <code>iconv</code> 更改文本编码。需要更高级的功能，可以使用 <code>uconv</code>，它支持一些高级的 Unicode 功能。例如，这条命令移除了所有重音符号：</p><pre><code class="hljs sh">uconv -f utf-8 -t utf-8 -x <span class="hljs-string">'::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] &gt;; ::Any-NFC; '</span> &lt; input.txt &gt; output.txt</code></pre></li><li><p>拆分文件可以使用 <code>split</code>（按大小拆分）和 <code>csplit</code>（按模式拆分）。</p></li><li><p>操作日期和时间表达式，可以用 <a href="http://www.fresse.org/dateutils/" target="_blank" rel="noopener"><code>dateutils</code></a> 中的 <code>dateadd</code>、<code>datediff</code>、<code>strptime</code> 等工具。</p></li><li><p>使用 <code>zless</code>、<code>zmore</code>、<code>zcat</code> 和 <code>zgrep</code> 对压缩过的文件进行操作。</p></li><li><p>文件属性可以通过 <code>chattr</code> 进行设置，它比文件权限更加底层。例如，为了保护文件不被意外删除，可以使用不可修改标记：<code>sudo chattr +i /critical/directory/or/file</code></p></li><li><p>使用 <code>getfacl</code> 和 <code>setfacl</code> 以保存和恢复文件权限。例如：</p><pre><code class="hljs sh">getfacl -R /some/path &gt; permissions.txtsetfacl --restore=permissions.txt</code></pre></li><li><p>为了高效地创建空文件，请使用 <code>truncate</code>（创建<a href="https://zh.wikipedia.org/wiki/稀疏文件" target="_blank" rel="noopener">稀疏文件</a>），<code>fallocate</code>（用于 ext4，xfs，btrf 和 ocfs2 文件系统），<code>xfs_mkfile</code>（适用于几乎所有的文件系统，包含在 xfsprogs 包中），<code>mkfile</code>（用于类 Unix 操作系统，比如 Solaris 和 Mac OS）。</p></li></ul><h2 id="系统调试"><a href="#系统调试" class="headerlink" title="系统调试"></a>系统调试</h2><ul><li><p><code>curl</code> 和 <code>curl -I</code> 可以被轻松地应用于 web 调试中，它们的好兄弟 <code>wget</code> 也是如此，或者也可以试试更潮的 <a href="https://github.com/jkbrzt/httpie" target="_blank" rel="noopener"><code>httpie</code></a>。</p></li><li><p>获取 CPU 和硬盘的使用状态，通常使用使用 <code>top</code>（<code>htop</code> 更佳），<code>iostat</code> 和 <code>iotop</code>。而 <code>iostat -mxz 15</code> 可以让你获悉 CPU 和每个硬盘分区的基本信息和性能表现。</p></li><li><p>使用 <code>netstat</code> 和 <code>ss</code> 查看网络连接的细节。</p></li><li><p><code>dstat</code> 在你想要对系统的现状有一个粗略的认识时是非常有用的。然而若要对系统有一个深度的总体认识，使用 <a href="https://github.com/nicolargo/glances" target="_blank" rel="noopener"><code>glances</code></a>，它会在一个终端窗口中向你提供一些系统级的数据。</p></li><li><p>若要了解内存状态，运行并理解 <code>free</code> 和 <code>vmstat</code> 的输出。值得留意的是“cached”的值，它指的是 Linux 内核用来作为文件缓存的内存大小，而与空闲内存无关。</p></li><li><p>Java 系统调试则是一件截然不同的事，一个可以用于 Oracle 的 JVM 或其他 JVM 上的调试的技巧是你可以运行 <code>kill -3 &lt;pid&gt;</code> 同时一个完整的栈轨迹和堆概述（包括 GC 的细节）会被保存到标准错误或是日志文件。JDK 中的 <code>jps</code>，<code>jstat</code>，<code>jstack</code>，<code>jmap</code> 很有用。<a href="https://github.com/aragozin/jvm-tools" target="_blank" rel="noopener">SJK tools</a> 更高级。</p></li><li><p>使用 <a href="http://www.bitwizard.nl/mtr/" target="_blank" rel="noopener"><code>mtr</code></a> 去跟踪路由，用于确定网络问题。</p></li><li><p>用 <a href="https://dev.yorhel.nl/ncdu" target="_blank" rel="noopener"><code>ncdu</code></a> 来查看磁盘使用情况，它比寻常的命令，如 <code>du -sh *</code>，更节省时间。</p></li><li><p>查找正在使用带宽的套接字连接或进程，使用 <a href="http://www.ex-parrot.com/~pdw/iftop/" target="_blank" rel="noopener"><code>iftop</code></a> 或 <a href="https://github.com/raboof/nethogs" target="_blank" rel="noopener"><code>nethogs</code></a>。</p></li><li><p><code>ab</code> 工具（Apache 中自带）可以简单粗暴地检查 web 服务器的性能。对于更复杂的负载测试，使用 <code>siege</code>。</p></li><li><p><a href="https://wireshark.org/" target="_blank" rel="noopener"><code>wireshark</code></a>，<a href="https://www.wireshark.org/docs/wsug_html_chunked/AppToolstshark.html" target="_blank" rel="noopener"><code>tshark</code></a> 和 <a href="http://ngrep.sourceforge.net/" target="_blank" rel="noopener"><code>ngrep</code></a> 可用于复杂的网络调试。</p></li><li><p>了解 <code>strace</code> 和 <code>ltrace</code>。这俩工具在你的程序运行失败、挂起甚至崩溃，而你却不知道为什么或你想对性能有个总体的认识的时候是非常有用的。注意 profile 参数（<code>-c</code>）和附加到一个运行的进程参数 （<code>-p</code>）。</p></li><li><p>了解使用 <code>ldd</code> 来检查共享库。但是<a href="http://www.catonmat.net/blog/ldd-arbitrary-code-execution/" target="_blank" rel="noopener">永远不要在不信任的文件上运行</a>。</p></li><li><p>了解如何运用 <code>gdb</code> 连接到一个运行着的进程并获取它的堆栈轨迹。</p></li><li><p>学会使用 <code>/proc</code>。它在调试正在出现的问题的时候有时会效果惊人。比如：<code>/proc/cpuinfo</code>，<code>/proc/meminfo</code>，<code>/proc/cmdline</code>，<code>/proc/xxx/cwd</code>，<code>/proc/xxx/exe</code>，<code>/proc/xxx/fd/</code>，<code>/proc/xxx/smaps</code>（这里的 <code>xxx</code> 表示进程的 id 或 pid）。</p></li><li><p>当调试一些之前出现的问题的时候，<a href="http://sebastien.godard.pagesperso-orange.fr/" target="_blank" rel="noopener"><code>sar</code></a> 非常有用。它展示了 cpu、内存以及网络等的历史数据。</p></li><li><p>关于更深层次的系统分析以及性能分析，看看 <code>stap</code>（<a href="https://sourceware.org/systemtap/wiki" target="_blank" rel="noopener">SystemTap</a>），<a href="https://en.wikipedia.org/wiki/Perf_(Linux)" target="_blank" rel="noopener"><code>perf</code></a>，以及<a href="https://github.com/draios/sysdig" target="_blank" rel="noopener"><code>sysdig</code></a>。</p></li><li><p>查看你当前使用的系统，使用 <code>uname</code>，<code>uname -a</code>（Unix／kernel 信息）或者 <code>lsb_release -a</code>（Linux 发行版信息）。</p></li><li><p>无论什么东西工作得很欢乐（可能是硬件或驱动问题）时可以试试 <code>dmesg</code>。</p></li><li><p>如果你删除了一个文件，但通过 <code>du</code> 发现没有释放预期的磁盘空间，请检查文件是否被进程占用：<br><code>lsof | grep deleted | grep &quot;filename-of-my-big-file&quot;</code></p></li></ul><h2 id="单行脚本"><a href="#单行脚本" class="headerlink" title="单行脚本"></a>单行脚本</h2><p>一些命令组合的例子：</p><ul><li><p>当你需要对文本文件做集合交、并、差运算时，<code>sort</code> 和 <code>uniq</code> 会是你的好帮手。具体例子请参照代码后面的，此处假设 <code>a</code> 与 <code>b</code> 是两内容不同的文件。这种方式效率很高，并且在小文件和上 G 的文件上都能运用（注意尽管在 <code>/tmp</code> 在一个小的根分区上时你可能需要 <code>-T</code> 参数，但是实际上 <code>sort</code> 并不被内存大小约束），参阅前文中关于 <code>LC_ALL</code> 和 <code>sort</code> 的 <code>-u</code> 参数的部分。</p><pre><code class="hljs sh">sort a b | uniq &gt; c   <span class="hljs-comment"># c 是 a 并 b</span>sort a b | uniq -d &gt; c   <span class="hljs-comment"># c 是 a 交 b</span>sort a b b | uniq -u &gt; c   <span class="hljs-comment"># c 是 a - b</span></code></pre></li><li><p>使用 <code>grep . *</code>（每行都会附上文件名）或者 <code>head -100 *</code>（每个文件有一个标题）来阅读检查目录下所有文件的内容。这在检查一个充满配置文件的目录（如 <code>/sys</code>、<code>/proc</code>、<code>/etc</code>）时特别好用。</p></li></ul><ul><li><p>计算文本文件第三列中所有数的和（可能比同等作用的 Python 代码快三倍且代码量少三倍）：</p><pre><code class="hljs sh">awk <span class="hljs-string">'&#123; x += $3 &#125; END &#123; print x &#125;'</span> myfile</code></pre></li><li><p>如果你想在文件树上查看大小/日期，这可能看起来像递归版的 <code>ls -l</code> 但比 <code>ls -lR</code> 更易于理解：</p><pre><code class="hljs sh">find . -<span class="hljs-built_in">type</span> f -ls</code></pre></li><li><p>假设你有一个类似于 web 服务器日志文件的文本文件，并且一个确定的值只会出现在某些行上，假设一个 <code>acct_id</code> 参数在 URI 中。如果你想计算出每个 <code>acct_id</code> 值有多少次请求，使用如下代码：</p><pre><code class="hljs sh">egrep -o <span class="hljs-string">'acct_id=[0-9]+'</span> access.log | cut -d= -f2 | sort | uniq -c | sort -rn</code></pre></li><li><p>要持续监测文件改动，可以使用 <code>watch</code>，例如检查某个文件夹中文件的改变，可以用 <code>watch -d -n 2 &#39;ls -rtlh | tail&#39;</code>；或者在排查 WiFi 设置故障时要监测网络设置的更改，可以用 <code>watch -d -n 2 ifconfig</code>。</p></li><li><p>运行这个函数从这篇文档中随机获取一条技巧（解析 Markdown 文件并抽取项目）：</p><pre><code class="hljs sh"><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">taocl</span></span>() &#123;  curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README-zh.md|    pandoc -f markdown -t html |    iconv -f <span class="hljs-string">'utf-8'</span> -t <span class="hljs-string">'unicode'</span> |    xmlstarlet fo --html --dropdtd |    xmlstarlet sel -t -v <span class="hljs-string">"(html/body/ul/li[count(p)&gt;0])[<span class="hljs-variable">$RANDOM</span> mod last()+1]"</span> |    xmlstarlet unesc | fmt -80&#125;</code></pre></li></ul><h2 id="冷门但有用"><a href="#冷门但有用" class="headerlink" title="冷门但有用"></a>冷门但有用</h2><ul><li><p><code>expr</code>：计算表达式或正则匹配</p></li><li><p><code>m4</code>：简单的宏处理器</p></li><li><p><code>yes</code>：多次打印字符串</p></li><li><p><code>cal</code>：漂亮的日历</p></li><li><p><code>env</code>：执行一个命令（脚本文件中很有用）</p></li><li><p><code>printenv</code>：打印环境变量（调试时或在写脚本文件时很有用）</p></li><li><p><code>look</code>：查找以特定字符串开头的单词或行</p></li><li><p><code>cut</code>，<code>paste</code> 和 <code>join</code>：数据修改</p></li><li><p><code>fmt</code>：格式化文本段落</p></li><li><p><code>pr</code>：将文本格式化成页／列形式</p></li><li><p><code>fold</code>：包裹文本中的几行</p></li><li><p><code>column</code>：将文本格式化成多个对齐、定宽的列或表格</p></li><li><p><code>expand</code> 和 <code>unexpand</code>：制表符与空格之间转换</p></li><li><p><code>nl</code>：添加行号</p></li><li><p><code>seq</code>：打印数字</p></li><li><p><code>bc</code>：计算器</p></li><li><p><code>factor</code>：分解因数</p></li><li><p><a href="https://gnupg.org/" target="_blank" rel="noopener"><code>gpg</code></a>：加密并签名文件</p></li><li><p><code>toe</code>：terminfo 入口列表</p></li><li><p><code>nc</code>：网络调试及数据传输</p></li><li><p><code>socat</code>：套接字代理，与 <code>netcat</code> 类似</p></li><li><p><a href="https://github.com/mattthias/slurm" target="_blank" rel="noopener"><code>slurm</code></a>：网络流量可视化</p></li><li><p><code>dd</code>：文件或设备间传输数据</p></li><li><p><code>file</code>：确定文件类型</p></li><li><p><code>tree</code>：以树的形式显示路径和文件，类似于递归的 <code>ls</code></p></li><li><p><code>stat</code>：文件信息</p></li><li><p><code>time</code>：执行命令，并计算执行时间</p></li><li><p><code>timeout</code>：在指定时长范围内执行命令，并在规定时间结束后停止进程</p></li><li><p><code>lockfile</code>：使文件只能通过 <code>rm -f</code> 移除</p></li><li><p><code>logrotate</code>： 切换、压缩以及发送日志文件</p></li><li><p><code>watch</code>：重复运行同一个命令，展示结果并／或高亮有更改的部分</p></li><li><p><a href="https://github.com/joh/when-changed" target="_blank" rel="noopener"><code>when-changed</code></a>：当检测到文件更改时执行指定命令。参阅 <code>inotifywait</code> 和 <code>entr</code>。</p></li><li><p><code>tac</code>：反向输出文件</p></li><li><p><code>shuf</code>：文件中随机选取几行</p></li><li><p><code>comm</code>：一行一行的比较排序过的文件</p></li><li><p><code>strings</code>：从二进制文件中抽取文本</p></li><li><p><code>tr</code>：转换字母</p></li><li><p><code>iconv</code> 或 <code>uconv</code>：文本编码转换</p></li><li><p><code>split</code> 和 <code>csplit</code>：分割文件</p></li><li><p><code>sponge</code>：在写入前读取所有输入，在读取文件后再向同一文件写入时比较有用，例如 <code>grep -v something some-file | sponge some-file</code></p></li><li><p><code>units</code>：将一种计量单位转换为另一种等效的计量单位（参阅 <code>/usr/share/units/definitions.units</code>）</p></li><li><p><code>apg</code>：随机生成密码</p></li><li><p><code>xz</code>：高比例的文件压缩</p></li><li><p><code>ldd</code>：动态库信息</p></li><li><p><code>nm</code>：提取 obj 文件中的符号</p></li><li><p><code>ab</code> 或 <a href="https://github.com/wg/wrk" target="_blank" rel="noopener"><code>wrk</code></a>：web 服务器性能分析</p></li><li><p><code>strace</code>：调试系统调用</p></li><li><p><a href="http://www.bitwizard.nl/mtr/" target="_blank" rel="noopener"><code>mtr</code></a>：更好的网络调试跟踪工具</p></li><li><p><code>cssh</code>：可视化的并发 shell</p></li><li><p><code>rsync</code>：通过 ssh 或本地文件系统同步文件和文件夹</p></li><li><p><a href="https://wireshark.org/" target="_blank" rel="noopener"><code>wireshark</code></a> 和 <a href="https://www.wireshark.org/docs/wsug_html_chunked/AppToolstshark.html" target="_blank" rel="noopener"><code>tshark</code></a>：抓包和网络调试工具</p></li><li><p><a href="http://ngrep.sourceforge.net/" target="_blank" rel="noopener"><code>ngrep</code></a>：网络层的 grep</p></li><li><p><code>host</code> 和 <code>dig</code>：DNS 查找</p></li><li><p><code>lsof</code>：列出当前系统打开文件的工具以及查看端口信息</p></li><li><p><code>dstat</code>：系统状态查看</p></li><li><p><a href="https://github.com/nicolargo/glances" target="_blank" rel="noopener"><code>glances</code></a>：高层次的多子系统总览</p></li><li><p><code>iostat</code>：硬盘使用状态</p></li><li><p><code>mpstat</code>： CPU 使用状态</p></li><li><p><code>vmstat</code>： 内存使用状态</p></li><li><p><code>htop</code>：top 的加强版</p></li><li><p><code>last</code>：登入记录</p></li><li><p><code>w</code>：查看处于登录状态的用户</p></li><li><p><code>id</code>：用户/组 ID 信息</p></li><li><p><a href="http://sebastien.godard.pagesperso-orange.fr/" target="_blank" rel="noopener"><code>sar</code></a>：系统历史数据</p></li><li><p><a href="http://www.ex-parrot.com/~pdw/iftop/" target="_blank" rel="noopener"><code>iftop</code></a> 或 <a href="https://github.com/raboof/nethogs" target="_blank" rel="noopener"><code>nethogs</code></a>：套接字及进程的网络利用情况</p></li><li><p><code>ss</code>：套接字数据</p></li><li><p><code>dmesg</code>：引导及系统错误信息</p></li><li><p><code>sysctl</code>： 在内核运行时动态地查看和修改内核的运行参数</p></li><li><p><code>hdparm</code>：SATA/ATA 磁盘更改及性能分析</p></li><li><p><code>lsblk</code>：列出块设备信息：以树形展示你的磁盘以及磁盘分区信息</p></li><li><p><code>lshw</code>，<code>lscpu</code>，<code>lspci</code>，<code>lsusb</code> 和 <code>dmidecode</code>：查看硬件信息，包括 CPU、BIOS、RAID、显卡、USB设备等</p></li><li><p><code>lsmod</code> 和 <code>modinfo</code>：列出内核模块，并显示其细节</p></li><li><p><code>fortune</code>，<code>ddate</code> 和 <code>sl</code>：额，这主要取决于你是否认为蒸汽火车和莫名其妙的名人名言是否“有用”</p></li></ul><h2 id="仅限-OS-X-系统"><a href="#仅限-OS-X-系统" class="headerlink" title="仅限 OS X 系统"></a>仅限 OS X 系统</h2><p>以下是<em>仅限于</em> OS X 系统的技巧。</p><ul><li><p>用 <code>brew</code> （Homebrew）或者 <code>port</code> （MacPorts）进行包管理。这些可以用来在 OS X 系统上安装以上的大多数命令。</p></li><li><p>用 <code>pbcopy</code> 复制任何命令的输出到桌面应用，用 <code>pbpaste</code> 粘贴输入。</p></li><li><p>若要在 OS X 终端中将 Option 键视为 alt 键（例如在上面介绍的 <strong>alt-b</strong>、<strong>alt-f</strong> 等命令中用到），打开 偏好设置 -&gt; 描述文件 -&gt; 键盘 并勾选“使用 Option 键作为 Meta 键”。</p></li><li><p>用 <code>open</code> 或者 <code>open -a /Applications/Whatever.app</code> 使用桌面应用打开文件。</p></li><li><p>Spotlight：用 <code>mdfind</code> 搜索文件，用 <code>mdls</code> 列出元数据（例如照片的 EXIF 信息）。</p></li><li><p>注意 OS X 系统是基于 BSD UNIX 的，许多命令（例如 <code>ps</code>，<code>ls</code>，<code>tail</code>，<code>awk</code>，<code>sed</code>）都和 Linux 中有微妙的不同（ Linux 很大程度上受到了 System V-style Unix 和 GNU 工具影响）。你可以通过标题为 “BSD General Commands Manual” 的 man 页面发现这些不同。在有些情况下 GNU 版本的命令也可能被安装（例如 <code>gawk</code> 和 <code>gsed</code> 对应 GNU 中的 awk 和 sed ）。如果要写跨平台的 Bash 脚本，避免使用这些命令（例如，考虑 Python 或者 <code>perl</code> ）或者经过仔细的测试。</p></li><li><p>用 <code>sw_vers</code> 获取 OS X 的版本信息。</p></li></ul><h2 id="仅限-Windows-系统"><a href="#仅限-Windows-系统" class="headerlink" title="仅限 Windows 系统"></a>仅限 Windows 系统</h2><p>以下是<em>仅限于</em> Windows 系统的技巧。</p><h3 id="在-Winodws-下获取-Unix-工具"><a href="#在-Winodws-下获取-Unix-工具" class="headerlink" title="在 Winodws 下获取 Unix 工具"></a>在 Winodws 下获取 Unix 工具</h3><ul><li><p>可以安装 <a href="https://cygwin.com/" target="_blank" rel="noopener">Cygwin</a> 允许你在 Microsoft Windows 中体验 Unix shell 的威力。这样的话，本文中介绍的大多数内容都将适用。</p></li><li><p>在 Windows 10 上，你可以使用 <a href="https://msdn.microsoft.com/commandline/wsl/about" target="_blank" rel="noopener">Bash on Ubuntu on Windows</a>，它提供了一个熟悉的 Bash 环境，包含了不少 Unix 命令行工具。好处是它允许 Linux 上编写的程序在 Windows 上运行，而另一方面，Windows 上编写的程序却无法在 Bash 命令行中运行。</p></li><li><p>如果你在 Windows 上主要想用 GNU 开发者工具（例如 GCC），可以考虑 <a href="http://www.mingw.org/" target="_blank" rel="noopener">MinGW</a> 以及它的 <a href="http://www.mingw.org/wiki/msys" target="_blank" rel="noopener">MSYS</a> 包，这个包提供了例如 bash，gawk，make 和 grep 的工具。MSYS 并不包含所有可以与 Cygwin 媲美的特性。当制作 Unix 工具的原生 Windows 端口时 MinGW 将特别地有用。</p></li><li><p>另一个在 Windows 下实现接近 Unix 环境外观效果的选项是 <a href="https://github.com/dthree/cash" target="_blank" rel="noopener">Cash</a>。注意在此环境下只有很少的 Unix 命令和命令行可用。</p></li></ul><h3 id="实用-Windows-命令行工具"><a href="#实用-Windows-命令行工具" class="headerlink" title="实用 Windows 命令行工具"></a>实用 Windows 命令行工具</h3><ul><li><p>可以使用 <code>wmic</code> 在命令行环境下给大部分 Windows 系统管理任务编写脚本以及执行这些任务。</p></li><li><p>Windows 实用的原生命令行网络工具包括 <code>ping</code>，<code>ipconfig</code>，<code>tracert</code>，和 <code>netstat</code>。</p></li><li><p>可以使用 <code>Rundll32</code> 命令来实现<a href="http://www.thewindowsclub.com/rundll32-shortcut-commands-windows" target="_blank" rel="noopener">许多有用的 Windows 任务</a> 。</p></li></ul><h3 id="Cygwin-技巧"><a href="#Cygwin-技巧" class="headerlink" title="Cygwin 技巧"></a>Cygwin 技巧</h3><ul><li><p>通过 Cygwin 的包管理器来安装额外的 Unix 程序。</p></li><li><p>使用 <code>mintty</code> 作为你的命令行窗口。</p></li><li><p>要访问 Windows 剪贴板，可以通过 <code>/dev/clipboard</code>。</p></li><li><p>运行 <code>cygstart</code> 以通过默认程序打开一个文件。</p></li><li><p>要访问 Windows 注册表，可以使用 <code>regtool</code>。</p></li><li><p>注意 Windows 驱动器路径 <code>C:\</code> 在 Cygwin 中用 <code>/cygdrive/c</code> 代表，而 Cygwin 的 <code>/</code> 代表 Windows 中的 <code>C:\cygwin</code>。要转换 Cygwin 和 Windows 风格的路径可以用 <code>cygpath</code>。这在需要调用 Windows 程序的脚本里很有用。</p></li><li><p>学会使用 <code>wmic</code>，你就可以从命令行执行大多数 Windows 系统管理任务，并编成脚本。</p></li><li><p>要在 Windows 下获得 Unix 的界面和体验，另一个办法是使用 <a href="https://github.com/dthree/cash" target="_blank" rel="noopener">Cash</a>。需要注意的是，这个环境支持的 Unix 命令和命令行参数非常少。</p></li><li><p>要在 Windows 上获取 GNU 开发者工具（比如 GCC）的另一个办法是使用 <a href="http://www.mingw.org/" target="_blank" rel="noopener">MinGW</a> 以及它的 <a href="http://www.mingw.org/wiki/msys" target="_blank" rel="noopener">MSYS</a> 软件包，该软件包提供了 bash、gawk、make、grep 等工具。然而 MSYS 提供的功能没有 Cygwin 完善。MinGW 在创建 Unix 工具的 Windows 原生移植方面非常有用。</p></li></ul><h2 id="更多资源"><a href="#更多资源" class="headerlink" title="更多资源"></a>更多资源</h2><ul><li><a href="https://github.com/alebcay/awesome-shell" target="_blank" rel="noopener">awesome-shell</a>：一份精心组织的命令行工具及资源的列表。</li><li><a href="https://github.com/herrbischoff/awesome-osx-command-line" target="_blank" rel="noopener">awesome-osx-command-line</a>：一份针对 OS X 命令行的更深入的指南。</li><li><a href="http://redsymbol.net/articles/unofficial-bash-strict-mode/" target="_blank" rel="noopener">Strict mode</a>：为了编写更好的脚本文件。</li><li><a href="https://github.com/koalaman/shellcheck" target="_blank" rel="noopener">shellcheck</a>：一个静态 shell 脚本分析工具，本质上是 bash／sh／zsh 的 lint。</li><li><a href="http://www.dwheeler.com/essays/filenames-in-shell.html" target="_blank" rel="noopener">Filenames and Pathnames in Shell</a>：有关如何在 shell 脚本里正确处理文件名的细枝末节。</li><li><a href="http://datascienceatthecommandline.com/#tools" target="_blank" rel="noopener">Data Science at the Command Line</a>：用于数据科学的一些命令和工具，摘自同名书籍。</li></ul>]]></content>
    
    
    <categories>
      
      <category>shell</category>
      
    </categories>
    
    
    <tags>
      
      <tag>shell</tag>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Grep Tips</title>
    <link href="/2020/08/21/2020/200821_grep/"/>
    <url>/2020/08/21/2020/200821_grep/</url>
    
    <content type="html"><![CDATA[<h2 id="单个目录示例"><a href="#单个目录示例" class="headerlink" title="单个目录示例"></a>单个目录示例</h2><pre><code class="hljs bash">grep -E <span class="hljs-string">"http"</span>  ./ -R --exclude-dir=.git</code></pre><h2 id="多个目录示例"><a href="#多个目录示例" class="headerlink" title="多个目录示例"></a>多个目录示例</h2><pre><code class="hljs bash">grep -E <span class="hljs-string">"http"</span>  . -R --exclude-dir=&#123;.git,res,bin&#125;</code></pre><h2 id="多个文件示例"><a href="#多个文件示例" class="headerlink" title="多个文件示例"></a>多个文件示例</h2><pre><code class="hljs bash">grep -E <span class="hljs-string">"http"</span>  . -R --exclude=*.&#123;java,js&#125;</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://blog.51cto.com/xoyabc/1652616" target="_blank" rel="noopener">grep搜索排除某些目录</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>shell</category>
      
    </categories>
    
    
    <tags>
      
      <tag>shell</tag>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Probilistic Linear Discriminant Analysis</title>
    <link href="/2020/08/17/2020/200817_plda/"/>
    <url>/2020/08/17/2020/200817_plda/</url>
    
    <content type="html"><![CDATA[<p>Probabilistic Linear Discriminant Analysis(PLDA) 最初是在人脸识别任务中提出的，被验证效果良好。说话人识别和人脸识别同属于生物信息识别范畴，PLDA 随后被用于说话人识别。  </p><p>考虑某样本 $x$，可以把 $x$ 看作一段语音的 i-vector, 或是一幅图像等。 样本 $x$ 的分布由协方差矩阵正定的一个GMM定义，如果已知 $x$ 属于某个高斯分量，且该高斯分量的均值点为 $y$，那么有:</p><p>$$P(x|y) = N(x|y,\Phi_{w})$$</p><p>上式中, 是正定的协方差矩阵，$y$</p>]]></content>
    
    
    <categories>
      
      <category>概率论与线性代数</category>
      
    </categories>
    
    
    <tags>
      
      <tag>概率论与线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IO 概念区分</title>
    <link href="/2020/08/15/2020/200815_os_io/"/>
    <url>/2020/08/15/2020/200815_os_io/</url>
    
    <content type="html"><![CDATA[<ul><li>同步（Synchronous）</li><li>异步 (Asynchronous)</li><li>阻塞 (Blocking)</li><li>非阻塞 (Nonblocking)</li></ul><h2 id="同步（Synchronous）"><a href="#同步（Synchronous）" class="headerlink" title="同步（Synchronous）"></a>同步（Synchronous）</h2><h2 id="异步-Asynchronous"><a href="#异步-Asynchronous" class="headerlink" title="异步 (Asynchronous)"></a>异步 (Asynchronous)</h2><h2 id="阻塞-Blocking"><a href="#阻塞-Blocking" class="headerlink" title="阻塞 (Blocking)"></a>阻塞 (Blocking)</h2><h2 id="非阻塞-Nonblocking"><a href="#非阻塞-Nonblocking" class="headerlink" title="非阻塞 (Nonblocking)"></a>非阻塞 (Nonblocking)</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://www.zhihu.com/question/19732473/answer/241673170" target="_blank" rel="noopener">[知乎] 怎样理解阻塞非阻塞与同步异步的区别</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>WEB</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tips</tag>
      
      <tag>shell</tag>
      
      <tag>WEB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kaldi Split data &amp; Concatenate scp</title>
    <link href="/2020/08/11/2020/200811_kaldi_cat_ark/"/>
    <url>/2020/08/11/2020/200811_kaldi_cat_ark/</url>
    
    <content type="html"><![CDATA[<h2 id="Split"><a href="#Split" class="headerlink" title="Split"></a>Split</h2><pre><code class="hljs bash">scp=<span class="hljs-variable">$data</span>/wav.scp<span class="hljs-comment"># prepare split_scps</span>split_scps=<span class="hljs-string">""</span><span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> $(seq <span class="hljs-variable">$nj</span>); <span class="hljs-keyword">do</span>    split_scps=<span class="hljs-string">"<span class="hljs-variable">$split_scps</span> <span class="hljs-variable">$logdir</span>/wav_<span class="hljs-variable">$&#123;name&#125;</span>.<span class="hljs-variable">$n</span>.scp"</span><span class="hljs-keyword">done</span><span class="hljs-comment"># split scp</span>utils/split_scp.pl <span class="hljs-variable">$scp</span> <span class="hljs-variable">$split_scps</span> || <span class="hljs-built_in">exit</span> 1;</code></pre><p>tip: <code>split_scps=&quot;1.scp 2.scp&quot;</code></p><h2 id="Concatenate"><a href="#Concatenate" class="headerlink" title="Concatenate"></a>Concatenate</h2><pre><code class="hljs bash"><span class="hljs-comment"># concatenate the .scp files together.</span><span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> $(seq <span class="hljs-variable">$nj</span>); <span class="hljs-keyword">do</span>  cat <span class="hljs-variable">$mfccdir</span>/raw_mfcc_<span class="hljs-variable">$name</span>.<span class="hljs-variable">$n</span>.scp || <span class="hljs-built_in">exit</span> 1;<span class="hljs-keyword">done</span> &gt; <span class="hljs-variable">$data</span>/feats.scp || <span class="hljs-built_in">exit</span> 1</code></pre>]]></content>
    
    
    <categories>
      
      <category>shell</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kaldi</tag>
      
      <tag>shell</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL CLI Tips</title>
    <link href="/2020/08/11/2020/200811_mysql_cli/"/>
    <url>/2020/08/11/2020/200811_mysql_cli/</url>
    
    <content type="html"><![CDATA[<h2 id="MySQL-Installation"><a href="#MySQL-Installation" class="headerlink" title="MySQL Installation"></a>MySQL Installation</h2><h2 id="CLI-Tips"><a href="#CLI-Tips" class="headerlink" title="CLI Tips"></a>CLI Tips</h2><h3 id="bash-CLI"><a href="#bash-CLI" class="headerlink" title="bash CLI"></a>bash CLI</h3><pre><code class="hljs bash"><span class="hljs-comment"># start</span>mysql.server start<span class="hljs-comment"># login</span>mysql -u root -pmysql -u <span class="hljs-variable">$user_name</span> -p<span class="hljs-comment"># status</span>systemctl status mysql.service<span class="hljs-comment"># run sql script</span><span class="hljs-built_in">source</span> <span class="hljs-variable">$sql_path</span></code></pre><h3 id="MySQL-CLI"><a href="#MySQL-CLI" class="headerlink" title="MySQL CLI"></a>MySQL CLI</h3><pre><code class="hljs SQL"><span class="hljs-comment"># create database</span><span class="hljs-keyword">create</span> <span class="hljs-keyword">database</span> <span class="hljs-keyword">test</span> <span class="hljs-built_in">character</span> <span class="hljs-keyword">set</span> utf8mb4;<span class="hljs-keyword">create</span> <span class="hljs-keyword">database</span> $database_name <span class="hljs-built_in">character</span> <span class="hljs-keyword">set</span> utf8mb4;<span class="hljs-comment"># show</span><span class="hljs-keyword">show</span> <span class="hljs-keyword">databases</span>;<span class="hljs-keyword">show</span> <span class="hljs-keyword">tables</span>;</code></pre><h2 id="mycli"><a href="#mycli" class="headerlink" title="mycli"></a>mycli</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><p><a href="https://www.jianshu.com/p/4fc53d7d7620" target="_blank" rel="noopener">Mac 上安装 MySQL</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/101697361" target="_blank" rel="noopener">mycli，一款让你忘记mysql-client的命令行客户端神器</a></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>WEB</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tips</tag>
      
      <tag>shell</tag>
      
      <tag>WEB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Django Tips &amp; Reference</title>
    <link href="/2020/08/08/2020/200808_django/"/>
    <url>/2020/08/08/2020/200808_django/</url>
    
    <content type="html"><![CDATA[<h2 id="MVT-模式"><a href="#MVT-模式" class="headerlink" title="MVT 模式"></a>MVT 模式</h2><ol><li><strong>Model</strong>：负责业务对象与数据库的对象(ORM)</li><li><strong>Template</strong>：负责如何把页面展示给用户</li><li><strong>View</strong>：负责业务逻辑，并在适当的时候调用 Model 和 Template</li></ol><h2 id="CLI-Tips"><a href="#CLI-Tips" class="headerlink" title="CLI Tips"></a>CLI Tips</h2><pre><code class="hljs bash"><span class="hljs-comment"># create Django projects</span>django-admin startproject <span class="hljs-variable">$project_name</span><span class="hljs-comment"># run server</span>python manage.py runserver　　<span class="hljs-comment">#default port is 8000</span>python -u manage.py runserver 0.0.0.0:4000<span class="hljs-comment"># create app</span>python manage.py startapp <span class="hljs-variable">$app_name</span><span class="hljs-comment"># admin</span>python manage.py migratepython manage.py createsuperuser<span class="hljs-comment"># make migrations</span>python manange.py makeigrations <span class="hljs-variable">$app_name</span></code></pre><h2 id="Model-Tips"><a href="#Model-Tips" class="headerlink" title="Model Tips"></a>Model Tips</h2><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> myapp_person (    <span class="hljs-string">"id"</span> <span class="hljs-built_in">serial</span> <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span> PRIMARY <span class="hljs-keyword">KEY</span>,    <span class="hljs-string">"first_name"</span> <span class="hljs-built_in">varchar</span>(<span class="hljs-number">30</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span>,    <span class="hljs-string">"last_name"</span> <span class="hljs-built_in">varchar</span>(<span class="hljs-number">30</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-literal">NULL</span>);</code></pre><pre><code class="hljs python"><span class="hljs-keyword">from</span> django.db <span class="hljs-keyword">import</span> models <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span><span class="hljs-params">(models.Model)</span>:</span>    first_name = models.CharField(max_length=<span class="hljs-number">30</span>)    last_name = models.CharField(max_length=<span class="hljs-number">30</span>)</code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><p><a href="https://www.cnblogs.com/maple-shaw/p/9029086.html" target="_blank" rel="noopener">Django 简介</a></p></li><li><p><a href="https://www.cnblogs.com/maple-shaw/articles/9333821.html" target="_blank" rel="noopener">Django 模版</a></p></li><li><p><a href="https://www.cnblogs.com/maple-shaw/articles/9285269.html" target="_blank" rel="noopener">Django 视图</a></p></li><li><p><a href="https://www.cnblogs.com/maple-shaw/articles/9282718.html" target="_blank" rel="noopener">Django 路由</a></p></li><li><p><a href="https://www.cnblogs.com/maple-shaw/p/9029086.html" target="_blank" rel="noopener">Django 模型</a></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>WEB</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tips</tag>
      
      <tag>WEB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【毕业要紧】Aduio Toolkit Ref</title>
    <link href="/2020/08/03/2020/200803_aduio_toolkit_ref/"/>
    <url>/2020/08/03/2020/200803_aduio_toolkit_ref/</url>
    
    <content type="html"><![CDATA[<h2 id="sox"><a href="#sox" class="headerlink" title="sox"></a>sox</h2><pre><code class="hljs bash"><span class="hljs-comment"># 查看音频信息</span>soxi test.wavsox test.wav -n <span class="hljs-built_in">stat</span><span class="hljs-comment"># 切割音频</span>sox in.wav out.wav trim 0 10<span class="hljs-comment"># 改变音量</span>sox -v 0.5 input.wav output.wav <span class="hljs-comment"># 音频放大0.5倍</span><span class="hljs-comment"># 以 sox foo.wav -n stat -v 命令返回的数字作为放大倍数，将最大化 foo.wav 的音量而不至于出现削波</span>sox foo.wav -n <span class="hljs-built_in">stat</span> -v 2&gt; vcsox -v `cat vc` foo.wav foo-maxed.wavsox --norm=-1 &lt;inputfile&gt; &lt;outputfile&gt; <span class="hljs-comment"># 归一化音频响度</span></code></pre><h2 id="librosa"><a href="#librosa" class="headerlink" title="librosa"></a>librosa</h2><pre><code class="hljs Python"><span class="hljs-keyword">import</span> librosa<span class="hljs-keyword">import</span> librosa.display<span class="hljs-comment"># load wavfile</span>waveform, sample_rate = librosa.load(filepath)<span class="hljs-comment"># MFCCs</span>MFCCs = librosa.feature.mfcc(y=waveform, sr=sample_rate, n_mfcc=<span class="hljs-number">24</span>)</code></pre><h2 id="torchadudio"><a href="#torchadudio" class="headerlink" title="torchadudio"></a>torchadudio</h2><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torchaudio<span class="hljs-comment"># load wavefile</span>waveform, sample_rate = torchaudio.load(filepath)<span class="hljs-comment"># MFCCs</span>MFCCc = torchaudio.transforms.MFCC()(waveform)<span class="hljs-comment"># Specgram</span>Specgram = torchaudio.transforms.Spectrogram()(waveform)</code></pre><h2 id="Kaldi"><a href="#Kaldi" class="headerlink" title="Kaldi"></a>Kaldi</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://www.cnblogs.com/LXP-Never/p/11561355.html" target="_blank" rel="noopener">[博客园] librosa语音信号处理</a></p><p>[2] <a href="https://librosa.org/doc/latest/index.html" target="_blank" rel="noopener">[librosa] Document</a></p><p>[3] <a href="https://www.jianshu.com/p/be9e16aeb101" target="_blank" rel="noopener">[简书] SOX常用命令</a></p><p>[4] <a href="https://www.cnblogs.com/zhuminghui/p/11971311.html" target="_blank" rel="noopener">[博客园] Linux 对音频万能处理的命令——SOX</a></p>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【毕业要紧】 CPC &amp; APC</title>
    <link href="/2020/07/31/2020/200714_cpc/"/>
    <url>/2020/07/31/2020/200714_cpc/</url>
    
    <content type="html"><![CDATA[<p>Predictive coding is one of the oldest techniques in signal processing for data compression</p><p>One of the most common strategies for unsupervised learning has been to predict future, missing or contextual information.</p><h2 id="Mutual-Information"><a href="#Mutual-Information" class="headerlink" title="Mutual Information"></a>Mutual Information</h2><ol><li>data: $x$</li><li>context: $c$</li></ol><p>$$I(x; c) = \sum_{x,c}p(x, c)log\frac{p(x|c)}{p(x)}$$</p><h2 id="CPC"><a href="#CPC" class="headerlink" title="CPC"></a>CPC</h2><p>Contrastive Predictive Coding (CPC) aims to learn representations that separates the target future frame $x_{i+n}$ and randomly sampled negative frames $\tilde{x}$, given a context $h_i = (x_1, x_2, …, x_i)$</p><h2 id="APC"><a href="#APC" class="headerlink" title="APC"></a>APC</h2><p>The methodology of the APC model is largely inspired by language models (LMs) for text, which are typically a probability distribution over sequences of $N$ tokens $(t_1, t_2, …, t_N )$. Given such a sequence, an LM assigns a probability $P(t_1, t_2, …, t_N)$ to the whole sequence by modeling the probability of token $t_k$ given the history $(t_1, t_2, …, t_{k−1})$</p><p>$$P(t_1,t_2,…,t_N)=\prod_{k=1}^{N}P(t_k|t_1,t_2,…,t_{k-1})$$</p><p>It is trained by minimizing the negative log-likelihood:</p><p>$$\sum_{k=1}^{N}-logP(t_1,t_2,…,t_{k-1};\theta_{t},\theta_{rnn},\theta_{s})$$</p><p>where the parameters to be optimized are $\theta_{t}$, $\theta_{rnn}$ and $\theta_{rnn}$ is a look-up table that maps each token into a vector of fixed dimensionality. $\theta_{rnn}$ is a Recurrent Neural Network (RNN) used to summarize the sequence history up to the current time step. $\theta_s$ is a Softmax layer appended at the output of each RNN time step for estimating probability distribution over the tokens. Language modeling is a general task that requires the understanding of many aspects in language in order to perform well.</p><p>In other words, given an utterance represented as a sequence of acoustic feature vectors $(x_1, x_2, …, x_T)$, the RNN processes each sequence element $x_t$ one at a time and outputs a prediction $y_t$, where $x_t$ and $y_t$ have the same dimensionality.</p>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>JavaScript tips</title>
    <link href="/2020/07/29/2020/200729_js/"/>
    <url>/2020/07/29/2020/200729_js/</url>
    
    <content type="html"><![CDATA[<h2 id="浏览器"><a href="#浏览器" class="headerlink" title="浏览器"></a>浏览器</h2><ul><li><strong>渲染引擎</strong>:</li><li><strong>JS 引擎</strong>:</li></ul><h2 id="NodeJS"><a href="#NodeJS" class="headerlink" title="NodeJS"></a>NodeJS</h2><h2 id="VUE"><a href="#VUE" class="headerlink" title="VUE"></a>VUE</h2>]]></content>
    
    
    <categories>
      
      <category>WEB</category>
      
    </categories>
    
    
    <tags>
      
      <tag>JavaScript</tag>
      
      <tag>Tips</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【毕业要紧】 Kaldi Tips</title>
    <link href="/2020/07/16/2020/200716_kaldi_tips/"/>
    <url>/2020/07/16/2020/200716_kaldi_tips/</url>
    
    <content type="html"><![CDATA[<h2 id="ark-ark-t-scp"><a href="#ark-ark-t-scp" class="headerlink" title="ark ark,t scp"></a>ark ark,t scp</h2><p><code>ark</code> is an archive format to save any <code>Kaldi objects</code>. ark can be flushed to and from unix pipe.</p><pre><code class="hljs bash">cat test.ark | copy-feats ark:- ark,t:- | less <span class="hljs-comment"># Show the contents in the ark</span></code></pre><p><strong><code>-</code> indicates standard input stream or output stream.</strong></p><h2 id="s-cs-p"><a href="#s-cs-p" class="headerlink" title="s, cs, p"></a>s, cs, p</h2><ol><li><code>s</code>：keys是有序的</li><li><code>cs</code>：按顺序访问数据 （程序不满足会崩溃） </li><li><code>p</code> ：忽略错误</li></ol><h2 id="FM-amp-FV"><a href="#FM-amp-FV" class="headerlink" title="FM &amp; FV"></a>FM &amp; FV</h2><p>Kaldi has two major types: Matrix and Vector. </p><ul><li>Binary/Text - Float/Double Matrix: FM, DM</li><li>Binary/Text - Float/Double Vector: FV, DV</li></ul><p>As such, features are often stored in one of these two file types. For instance, when you extract i-vectors, they are stored as a matrix of floats (FM) and if you extract x-vectors, they are stored as vectors of float (FV). Often it may be required to convert features stored as FV to FM and vice-versa.</p><p>convert from FV to FM:</p><pre><code class="hljs bash">copy-vector --binary=<span class="hljs-literal">false</span> scp:exp/xvectors/xvector.scp ark,t:- | \  copy-matrix ark,t:- ark,scp:exp/xvectors/xvector_mat.ark,exp/xvectors/xvector_mat.scp</code></pre><p>convert from FM to FV:</p><pre><code class="hljs bash">copy-matrix --binary=<span class="hljs-literal">false</span> scp:exp/ivectors/ivector.scp ark,t:- | \  copy-vector ark,t:- ark,scp:exp/ivectors/ivector_vec.ark,exp/ivectors/ivector_vec.scp</code></pre><h2 id="Reference-and-Other-Tips"><a href="#Reference-and-Other-Tips" class="headerlink" title="Reference and Other Tips"></a>Reference and Other Tips</h2><p>[1] <a href="https://desh2608.github.io/2019-03-27-kaldi-tricks/" target="_blank" rel="noopener">desh2608 kaldi-tricks</a></p>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
      <tag>Kaldi</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【毕业要紧】 Facebook AI Research wav2vec</title>
    <link href="/2020/07/09/2020/200709_wav2vec/"/>
    <url>/2020/07/09/2020/200709_wav2vec/</url>
    
    <content type="html"><![CDATA[<h2 id="Unknown"><a href="#Unknown" class="headerlink" title="Unknown"></a>Unknown</h2><ul><li><strong>character-based speech recognition</strong> : what’s character-based?</li></ul><h2 id="Some-Tips"><a href="#Some-Tips" class="headerlink" title="Some Tips"></a>Some Tips</h2><p>We explore unsupervised pre-training for speech recognition by learning representations of raw audio. </p><p><strong>wav2vec</strong> is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. </p><p><strong>We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task.</strong></p><p>Our experiments on WSJ reduce WER of a strong character-based <strong>log-mel filterbank baseline</strong> by up to 36% when only a few hours of transcribed data is available. Our approach achieves 2.43% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using two orders of magnitude less labeled training data</p><p>Our model, wav2vec, is a convolutional neural network that takes raw audio as input and computes a general representation that can be input to a speech recognition system.</p><p><strong>The objective is a contrastive loss that requires distinguishing a true future audio sample from negatives.</strong></p><p>Different to previous work (van den Oord et al., 2018), we move beyond frame-wise phoneme classification and apply the learned representations to improve strong supervised ASR systems. wav2vec relies on a fully convolutional architecture which can be easily parallelized over time on modern hardware compared to recurrent models used in previous work.</p><p>We introduce wav2vec, the first application of unsupervised pre-training to speech recognition with a fully convolutional model. Our approach achieves 2.43% WER on the test set of WSJ, a result that outperforms the next best known character-based speech recognition model in the literature (Amodei et al., 2016) while using two orders of magnitude less transcribed training data. </p><ul><li>improves resource-poor setups</li><li>settings where all WSJ training data is used</li></ul><h2 id="Objective"><a href="#Objective" class="headerlink" title="Objective"></a>Objective</h2><p>Our model takes raw audio signal as input and then applies two networks.</p><ul><li><strong>encoder network</strong> : embeds the audio signal in a latent space</li><li><strong>context network</strong> : combines multiple time-steps of the encoder to obtain contextualized representations</li></ul><p>Given raw audio samples $x_i \in \mathcal{X}$, we apply the encoder network $f : \mathcal{X} → \mathcal{Z}$ parameterized as a five-layer convolutional network</p><p>Next, we apply the context network $g : \mathcal{Z} → \mathcal{C}$ to the output of the encoder network to mix multiple latent representations $z_i…z_{i-v}$ into a single contextualized tensor $c_{i}= g(z_{i}…z_{i−v})$ for a receptive field size $v$.</p><p><strong>The objective is a contrastive loss that requires distinguishing a true future audio sample from negatives.</strong></p><p>We train the model to distinguish a sample $z_{i+k}$ that is $k$ steps in the future from distractor samples $\widetilde{z}$ drawn from a proposal distribution $p_n$, by minimizing the contrastive loss for each step $k = 1,…,K$:</p><p>$$L_k = -\sum_{i=1}^{T-k}(log\sigma(z_{i+k}^{T}h_k(c_i))+\lambda\mathbb{E}[log\sigma(-\widetilde{z}^{T}h_k(c_i))])$$</p><p>$$\mathcal{L} =\sum_{k=1}^{K}\mathcal{L}_k$$</p><ul><li>$\sigma(x)=1/(1+exp(-x))$ : sigmoid</li><li>$\sigma(z_{i+k}^{T}h_{k}(c_i))$ : the probability of $z_{i+k}$ being the true sample</li></ul><p>We consider a step-specific <strong>affine transformation</strong>(<a href="https://www.cnblogs.com/happystudyeveryday/p/10547316.html" target="_blank" rel="noopener">仿射变换</a>) $h_k(c_i) = W_{k}c_{i}+b_{k}$ for each step $k$, that is applied to $c_i$(van den Oord et al., 2018). We optimize the loss $\mathcal{L} =\sum_{k=1}^{K}\mathcal{L}_k$, summing (1) over different step sizes. In practice, we approximate the expectation by sampling ten negatives examples by uniformly choosing distractors from each audio sequence, i.e., $p_n(z) = \frac{1}{T}$, where $T$ is the sequence length and we set $\lambda$ to the number of negatives</p><h2 id="Code-Example"><a href="#Code-Example" class="headerlink" title="Code Example"></a>Code Example</h2><h3 id="Pre-Train-model-Useage"><a href="#Pre-Train-model-Useage" class="headerlink" title="Pre Train model Useage"></a>Pre Train model Useage</h3><ul><li>The encoder network embeds the audio signal in a latent space</li><li>the context network combines multiple time-steps of the encoder to obtain contextualized representations</li></ul><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">from</span> fairseq.models.wav2vec <span class="hljs-keyword">import</span> Wav2VecModelcp = torch.load(<span class="hljs-string">'/path/to/wav2vec.pt'</span>)model = Wav2VecModel.build_model(cp[<span class="hljs-string">'args'</span>], task=<span class="hljs-literal">None</span>)model.load_state_dict(cp[<span class="hljs-string">'model'</span>])model.eval()wav_input_16khz = torch.randn(<span class="hljs-number">1</span>,<span class="hljs-number">10000</span>)z = model.feature_extractor(wav_input_16khz)c = model.feature_aggregator(z)</code></pre>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【毕业要紧】 Self Supervised Learning in Speech Resources</title>
    <link href="/2020/07/05/2020/200708_self_supervised_learning/"/>
    <url>/2020/07/05/2020/200708_self_supervised_learning/</url>
    
    <content type="html"><![CDATA[<h2 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h2><p>Given: </p><ul><li>a dataset $\mathcal{D} = {(x, y)}_i^N$</li><li>a loss function $\mathcal{l}$</li></ul><p>Goal:</p><p>$$min_{\theta}\mathbb{E}_{(x, y)}$$</p><ul><li>Works well when labeled data is abundant</li><li><strong>Learn useful representation with the supervision</strong></li></ul><h2 id="Yann-LeCun"><a href="#Yann-LeCun" class="headerlink" title="Yann LeCun"></a>Yann LeCun</h2><blockquote><p>in self-supervised learning, the systerm learns to predict part of its input from other parts of it input.</p></blockquote><ul><li>Goal: Learning to represent the world before learning tasks.</li><li>Predict any part of the input from any other part</li><li>Predict the <strong>future</strong> from the <strong>recent past</strong></li><li>Predict the <strong>past</strong> from the <strong>present</strong></li><li>Predict the <strong>top</strong> from the <strong>bottom</strong></li></ul><h2 id="Interspeech-2020-New-Trends-in-self-supervised-Speech-Processing"><a href="#Interspeech-2020-New-Trends-in-self-supervised-Speech-Processing" class="headerlink" title="Interspeech 2020: New Trends in self-supervised Speech Processing"></a>Interspeech 2020: New Trends in self-supervised Speech Processing</h2><p>Over the past decade, supervised deep learning models led to great strides in performance for speech processing technologies and applications. </p><p>However, unlike humans who are capable of self-learning through experiences and interactions, current real-world speech applications are heavily reliant on large volumes of human annotations. </p><p>For the next generation of speech processing systems to exhibit similar levels of cognitive intelligence as humans, they should be designed to exploit unlabeled, partially labeled, contextual and distant supervision data from multiple concurrent modalities, e.g., text and video, and learning signals from corrective user follow-ups in conversational applications. </p><p><strong>The main motivation for self-supervised learning is rooted in our need to improve ASR systems when there is a limited amount of labeled data.</strong></p><p>Self-supervised learning methods [LeCun 2016] construct proxy predictive tasks to train models for downstream scenarios by exploiting large amounts of unlabeled audio data, unpaired text and audio data in the same domain, or speech data with distant unrelated labels, e.g. A text summary or slides of an audio lecture. </p><p>Through these invented proxy tasks, models learn high-level representations that generalize well across different languages, domains, and deployment scenarios with very few in-domain labeled examples. </p><p>Self-supervised learning methods achieved major successes in Natural Language Processing (NLP) [Peters 2018, Devlin 2018, Radford 2019, Raffel 2019, Lewis 2019] and Computer Vision (CV) [Sun 2019, He 2019, Xie 2019, Misra 2019].</p><p>There is a recent surge in speech processing research work introducing predictive proxy tasks for model training, and achieving impressive results in downstream applications like ASR and speaker recognition. These self-supervised approaches include, but not limited to:</p><ul><li><strong>Future prediction</strong>: Learning an autoregressive model that generates distant future audio features from historical ones [Oard 2018, Chung 2019, Schneider 2019].</li><li><strong>Mask prediction</strong>: Learning a model that predicts masked parts of the input audio signal [Liu 2019, Song 2019, Baevski 2019a, Baevski 2019b].</li><li><strong>Generating contextual data</strong>: Learning a model to predict semantically-related contextual information that accompany the speech signal, e.g. Using social media title and comments as input audio labels [Singh 2019, Pascual 2019].</li><li><strong>Chaining ASR and TTS</strong>: Using unpaired audio and text data to train an ASR system and a TTS system jointly, where one is generating training paired data for the other [Tjandra 2019, Hori 2019, Baskar 2019]. This family of self-supervised methods can be viewed as auto-encoders of speech signals through latent text representations. Effective use of external language models falls into this category to regularize the text representations.</li></ul><h2 id="Workshop-ICML-2019-Self-Supervised-Learning"><a href="#Workshop-ICML-2019-Self-Supervised-Learning" class="headerlink" title="Workshop - ICML 2019 Self-Supervised Learning"></a>Workshop - ICML 2019 Self-Supervised Learning</h2><p>Big data has driven a revolution to many domains of machine learning thanks to modern high-capacity models, but the standard approaches – supervised learning from labels, or reinforcement learning from a reward function – have become a bottleneck. </p><p>Even when data is abundant, getting the labels or rewards that specify exactly what the model must do is often intractable. Collecting simple category labels for classification is prohibitive for millions of billions of examples, and structured outputs (scene interpretations, interactions, demonstrations) are far worse, especially when the data distribution is non-stationary.</p><p><strong>Self-supervised learning</strong> is a promising alternative where proxy tasks are developed that allow models and agents to learn without explicit supervision in a way that helps with downstream performance on tasks of interest. One of the major benefits of self-supervised learning is increasing data efficiency: achieving comparable or better performance with less labeled data or fewer environment steps (in Reinforcement learning / Robotics).</p><p>The field of self-supervised learning (SSL) is rapidly evolving, and the performance of these methods is creeping closer to the fully supervised approaches. However, many of these methods are still developed in domain-specific sub-communities, such as Vision, RL and NLP, even though many similarities exist between them. While SSL is an emerging topic and there is great interest in these techniques, there are currently few workshops, tutorials or other scientific events dedicated to this topic.</p><p>This workshop aims to bring together experts with different backgrounds and applications areas to share inter-domain ideas and increase cross-pollination, tackle current shortcomings and explore new directions. The focus will be on the machine learning point of view rather than the domain side.</p><h2 id="ICML-2020-Self-supervision-in-Audio-and-Speech"><a href="#ICML-2020-Self-supervision-in-Audio-and-Speech" class="headerlink" title="ICML 2020 Self-supervision in Audio and Speech"></a>ICML 2020 Self-supervision in Audio and Speech</h2><p>The ongoing success of deep learning techniques depends on the quality of the representations automatically discovered from data. </p><p><strong>These representations must capture important underlying structures from the raw input, e.g., intermediate concepts, features, or latent variables that are useful for the downstream task</strong>. </p><p>While supervised learning using large annotated corpora can leverage useful representations, collecting large amounts of annotated examples is costly, time-consuming, and not always feasible. </p><p>This is particularly problematic for a large variety of applications. In the speech domain, for instance, there are many low-resource languages, where the progress is dramatically slower than in high-resource languages such as English. Moreover, annotations are often underspecified for many potential downstream applications, and the related supervised representations might be biased towards the task they are trained on, limiting their exportability to other applications 2.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://docs.google.com/presentation/d/1qq8t8a3decJfJyA6t9wCRJfNDkvAnbg1WxTFvx0hG4g/edit?usp=sharing" target="_blank" rel="noopener">NTU 2020 Spring ML Self-Supervised Learning slide, vpn</a></p><p>[2] <a href="https://self-supervised-sp.github.io/Interspeech2020-Special-Session" target="_blank" rel="noopener">interspeech2020: New Trends in self-supervised speech processing</a></p><p>[3] <a href="https://icml-sas.gitlab.io/" target="_blank" rel="noopener">ICML 2020 Self-supervision in Audio and Speech</a></p><p>[4] <a href="https://sites.google.com/view/self-supervised-icml2019" target="_blank" rel="noopener">Workshop - ICML 2019 Self-Supervised Learning</a></p><p>[5] <a href="https://www.bilibili.com/video/BV1oD4y1D7V2" target="_blank" rel="noopener">[CVPR2020 Tutorial] Talk#5 Self-supervised Learning by Licheng, Yen-Chun, Linjie</a></p>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【毕业要紧】 An overview of voice conversion systems</title>
    <link href="/2020/07/05/research_note/012_VC/"/>
    <url>/2020/07/05/research_note/012_VC/</url>
    
    <content type="html"><![CDATA[<h2 id="VC-systerm"><a href="#VC-systerm" class="headerlink" title="VC systerm"></a>VC systerm</h2><p><img src="https://wx2.sbimg.cn/2020/07/07/CCSsO.png" srcset="/img/loading.gif" alt="avatar"></p>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>makefile &amp; cmake &amp; pybind11</title>
    <link href="/2020/07/04/2020/200704_makefile/"/>
    <url>/2020/07/04/2020/200704_makefile/</url>
    
    <content type="html"><![CDATA[<h2 id="Makefile"><a href="#Makefile" class="headerlink" title="Makefile"></a>Makefile</h2><h3 id="makefile-的好处"><a href="#makefile-的好处" class="headerlink" title="makefile 的好处"></a>makefile 的好处</h3><p>Makefile 带来的好处就是——“自动化编译”，一旦写好，只需要一个make 命令，整个工程完全自动编译，极大的提高了软件开发的效率。make 是一个命令工具，是一个解释 Makefile 中指令的命令工具。</p><p>Makefile 关系到了整个工程的编译规则。一个工程中的源文件不计其数，并且按类型、功能、模块分别放在若干个目录中，makefile 定义了一系列的规则来指定，哪些文件需要先编译，哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为 Makefile 就像一个Shell 脚本一样，其中也可以执行操作系统的命令。</p><h3 id="rules"><a href="#rules" class="headerlink" title="rules"></a>rules</h3><p>源文件首先会生成中间目标文件，再由中间目标文件生成执行文件</p><pre><code class="hljs bash"><span class="hljs-comment"># work pipline</span><span class="hljs-comment"># 写代码-&gt; *.cpp -&gt; 编译 -&gt; *.o -&gt; 链接 -&gt; *.a / *.so</span><span class="hljs-comment"># Makefile rules</span>target ... : prerequisites ...    <span class="hljs-built_in">command</span>    ...    ...</code></pre><h3 id="exmaple"><a href="#exmaple" class="headerlink" title="exmaple"></a>exmaple</h3><h3 id="about-cc"><a href="#about-cc" class="headerlink" title="about cc"></a><a href="https://blog.csdn.net/dddd0216/article/details/51152234" target="_blank" rel="noopener">about <code>cc</code></a></h3><ul><li>cc是unix下面用的编译命令；</li><li>gcc是linux下面用的编译命令；</li></ul><p>很多makefile文件是在Unix下面写的，编译用的是 cc,但是现在很多人喜欢用linux来编译，总不能把makefile文件中所有的cc改成gcc吧，多麻烦啊！</p><p>最后某个大神想到了用连接的方法把cc连接的gcc命令上，运行cc就是运行gcc。</p><h3 id="resource"><a href="#resource" class="headerlink" title="resource"></a>resource</h3><ol><li><a href="https://github.com/seisman/how-to-write-makefile" target="_blank" rel="noopener">Github how to write makefile</a></li></ol><h2 id="Cmake"><a href="#Cmake" class="headerlink" title="Cmake"></a>Cmake</h2><p>CMake is an open-source, <strong>cross-platform</strong> family of tools designed to build, test and package software. CMake is used to control the software compilation process using simple platform and compiler independent configuration files, and generate native makefiles and workspaces that can be used in the compiler environment of your choice. The suite of CMake tools were created by Kitware in response to the need for a powerful, cross-platform build environment for open-source projects such as ITK and VTK.</p><h3 id="installation"><a href="#installation" class="headerlink" title="installation"></a>installation</h3><h3 id="rules-1"><a href="#rules-1" class="headerlink" title="rules"></a>rules</h3><h3 id="example"><a href="#example" class="headerlink" title="example"></a>example</h3><h3 id="resource-1"><a href="#resource-1" class="headerlink" title="resource"></a>resource</h3><ul><li><a href="https://github.com/Akagi201/learning-cmake" target="_blank" rel="noopener">learning-cmake</a></li><li><a href="docs/cmake-practice.pdf">CMake Practice</a></li><li><a href="docs/cmake-rules.pdf">CMake rules</a></li><li><a href="docs/mastering-cmake.pdf">Mastering CMake</a></li><li><a href="https://cgold.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">CGold: The Hitchhiker’s Guide to the CMake</a></li><li><a href="https://cmake.org/cmake/help/latest/index.html" target="_blank" rel="noopener">Latest Official CMake doc</a></li><li><a href="https://github.com/ttroy50/cmake-examples" target="_blank" rel="noopener">CMake Example</a></li></ul><h2 id="Pybind11"><a href="#Pybind11" class="headerlink" title="Pybind11"></a>Pybind11</h2><p>pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. Its goals and syntax are similar to the excellent Boost.Python library by David Abrahams: to minimize boilerplate code in traditional extension modules by inferring type information using compile-time introspection.</p><h3 id="installation-1"><a href="#installation-1" class="headerlink" title="installation"></a>installation</h3><h3 id="examples"><a href="#examples" class="headerlink" title="examples"></a>examples</h3><h3 id="resource-2"><a href="#resource-2" class="headerlink" title="resource"></a>resource</h3><ol><li><a href="https://pybind11.readthedocs.io/en/stable/" target="_blank" rel="noopener">pybind11 doc</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>toolkit</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开发</tag>
      
      <tag>工具</tag>
      
      <tag>效率</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【毕业要紧】 Evaluate Metric</title>
    <link href="/2020/07/04/2020/200705_eval_metric/"/>
    <url>/2020/07/04/2020/200705_eval_metric/</url>
    
    <content type="html"><![CDATA[<h3 id="Accuracy-Acc"><a href="#Accuracy-Acc" class="headerlink" title="Accuracy, Acc"></a>Accuracy, Acc</h3><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【毕业要紧】 flow 模型</title>
    <link href="/2020/07/04/research_note/002_flow/"/>
    <url>/2020/07/04/research_note/002_flow/</url>
    
    <content type="html"><![CDATA[<p>A good representation is one in which the data has a distribution that is easy to model</p><h2 id="基础数学"><a href="#基础数学" class="headerlink" title="基础数学"></a>基础数学</h2><h3 id="生成模型要解决的问题"><a href="#生成模型要解决的问题" class="headerlink" title="生成模型要解决的问题"></a>生成模型要解决的问题</h3><p>给定两组数据 $z$ 和 $x$ ，其中 $z$ 服从已知的简单先验分布 $\pi(z)$ ，通常是高斯分布，$x$ 服从复杂的分布 $p(x)$ ,即训练数据代表的分布，现在我们想要找到一个变换函数 $f$ ，它能建立一种 $z$ 到 $x$ 的映射，使得每对于 $\pi(z)$ 中的一个采样点，都能在 $p(x)$ 中有一个（新）样本点与之对应。</p><p>$$ p_g(x) = \int_z p(x|z)p(z)dz\ $$</p><p>其中，$ p(x|z) $ - $ the \  probability \  of \  x \  given \  z$</p><h3 id="概率分布的变换数学例题"><a href="#概率分布的变换数学例题" class="headerlink" title="概率分布的变换数学例题"></a>概率分布的变换数学例题</h3><p>设随机变量X具有概率密度</p><p>求随机变量 $Y=2X+8$ 的概率密度</p><h3 id="Jacobian-Matrix"><a href="#Jacobian-Matrix" class="headerlink" title="Jacobian Matrix"></a>Jacobian Matrix</h3><p>$$<br>\left[<br>    \begin{array}{ccc}<br>        \frac{\partial f_1 }{\partial x_1 } &amp; \cdots &amp; \frac{\partial f_1 }{\partial x_n } \\<br>        \vdots\quad &amp; \ddots &amp; \vdots\quad \\<br>        \frac{\partial f_n }{\partial x_1 } &amp; \cdots &amp; \frac{\partial f_n }{\partial x_n } \\<br>    \end{array}<br>\right]<br>$$</p><p>$$J_{ij}=\frac{\partial f_i }{\partial x_j }$$</p><h3 id="Determinant"><a href="#Determinant" class="headerlink" title="Determinant"></a>Determinant</h3><h3 id="耦合层（Coupling-Layer）"><a href="#耦合层（Coupling-Layer）" class="headerlink" title="耦合层（Coupling Layer）"></a>耦合层（Coupling Layer）</h3><h2 id="NICE"><a href="#NICE" class="headerlink" title="NICE"></a><a href="">NICE</a></h2><p>We propose a deep learning framework for modeling complex high-dimensional densities called <strong>Non-linear Independent Component Estimation</strong> (NICE). It is based on the idea that <strong>a good representation is one in which the data has a distribution that is easy to model</strong>. </p><p>For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to <strong>make the transformed data conform to a factorized distribution</strong>, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the <strong>determinant of the Jacobian</strong> and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact <strong>log-likelihood</strong>, which is tractable. </p><p>Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.</p><h2 id="RealNVP"><a href="#RealNVP" class="headerlink" title="RealNVP"></a><a href="">RealNVP</a></h2><p>Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. </p><p>We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful, stably invertible, and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact and efficient sampling, exact and efficient inference of latent variables, and an interpretable latent space. </p><p>We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation, and latent variable manipulations.</p><p>“The advantage of Real NVP compared to MAF and IAF is that it can both generate data and estimate densities with one forward pass only, whereas MAF would need D passes to generate data and IAF would need D passes to estimate densities.”</p><h2 id="Glow"><a href="#Glow" class="headerlink" title="Glow"></a><a href="">Glow</a></h2><p>Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. </p><p>In this paper we propose Glow, a simple type of generative flow using an invertible $1 \times 1$ convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realisticlooking synthesis and manipulation of large images.</p><h2 id="Normalizing-Flows-for-Probabilistic-Modeling-and-Inference"><a href="#Normalizing-Flows-for-Probabilistic-Modeling-and-Inference" class="headerlink" title="Normalizing Flows for Probabilistic Modeling and Inference"></a><a href="">Normalizing Flows for Probabilistic Modeling and Inference</a></h2><p>Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective(<a href="https://www.cnblogs.com/wanghetao/archive/2012/03/16/2400619.html">双射</a>) transformations. </p><p>There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of <strong>a unified perspective</strong>. </p><p><strong>In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference.</strong></p><p>We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.</p><h2 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h2><p><a href="https://arxiv.org/abs/1410.8516" target="_blank" rel="noopener">NICE: Non-linear Independent Components Estimation, Dinh et al. 2014</a></p><p><a href="https://arxiv.org/abs/1505.05770" target="_blank" rel="noopener">Variational Inference with Normalizing Flows, Rezende and Mohamed 2015</a></p><p><a href="https://arxiv.org/abs/1605.08803" target="_blank" rel="noopener">Density estimation using Real NVP, Dinh et al. May 2016</a></p><p><a href="https://arxiv.org/abs/1606.04934" target="_blank" rel="noopener">Improved Variational Inference with Inverse Autoregressive Flow, Kingma et al June 2016</a></p><p><a href="https://arxiv.org/abs/1705.07057" target="_blank" rel="noopener">Masked Autoregressive Flow for Density Estimation, Papamakarios et al. May 2017</a></p><p><a href="https://arxiv.org/abs/1807.03039" target="_blank" rel="noopener">Glow: Generative Flow with Invertible 1x1 Convolutions, Kingma and Dhariwal, Jul 2018</a></p><p><a href="https://arxiv.org/abs/1912.02762" target="_blank" rel="noopener">Normalizing Flows for Probabilistic Modeling and Inference. 2019</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>笔记</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【毕业要紧】 Speech 领域专业名词（持续更新）</title>
    <link href="/2020/07/03/research_note/001_speech_note/"/>
    <url>/2020/07/03/research_note/001_speech_note/</url>
    
    <content type="html"><![CDATA[<h3 id="16bit-脉冲编码调制-16-bit-Pulse-code-Modulation-PCM"><a href="#16bit-脉冲编码调制-16-bit-Pulse-code-Modulation-PCM" class="headerlink" title="16bit-脉冲编码调制 16 bit Pulse-code Modulation, PCM"></a>16bit-脉冲编码调制 16 bit Pulse-code Modulation, PCM</h3><p>16bit 代表这个⾳频将会按照信号的振幅取离散的 $2^{16}$ 个值，即 -32768～+32767，振幅越⼤，声⾳的强度越⼤。⾳频信号的另⼀个重要参数是采样率，⽐如采样率为 44.1 kHz，意思就是每秒采样 441000 个点。</p><p>如果需要提取⾳频信号中的信息，就需要对 <code>采样时间 × 采样率数⽬的点</code> 做预处理。<br>⾸先需要把振幅从整数转变为-1～+1 之间的浮点数，即对所有的振幅先加 32768，除以 32768，然后减去 1.0。我们可以画出振幅随着时间的变化。</p><h3 id="预加重"><a href="#预加重" class="headerlink" title="预加重"></a>预加重</h3><p>在数学上，预加重相当于⼀个⾼通滤波器，起到了提⾼⾼频信号分量、滤除低频噪声的作⽤。在实际应⽤中，预加重是⼀个可选的预处理步骤，对于信号中噪声的滤除有⼀定的效果。</p><ol><li>有限长单位冲激响应滤波器 Finite Impulse Response, <strong>FIR</strong></li><li>递归滤波器，无限脉冲响应数字滤波器 Infinite Impulse Response, <strong>IIR</strong></li></ol><h3 id="频率-Frequency-amp-幅度-Magnitude-amp-相位-Phase"><a href="#频率-Frequency-amp-幅度-Magnitude-amp-相位-Phase" class="headerlink" title="频率 Frequency &amp; 幅度 Magnitude &amp; 相位 Phase"></a>频率 Frequency &amp; 幅度 Magnitude &amp; 相位 Phase</h3><h3 id="音高-Pitch-amp-音量-Loudness-amp-音色-Timbre"><a href="#音高-Pitch-amp-音量-Loudness-amp-音色-Timbre" class="headerlink" title="音高 Pitch &amp; 音量 Loudness &amp; 音色 Timbre"></a>音高 Pitch &amp; 音量 Loudness &amp; 音色 Timbre</h3><h3 id="分贝-dB"><a href="#分贝-dB" class="headerlink" title="分贝 dB"></a>分贝 dB</h3><h3 id="语音特征"><a href="#语音特征" class="headerlink" title="语音特征"></a>语音特征</h3><ul><li><strong>Spectrogram</strong></li><li><strong>Fbanks</strong></li><li><strong>MFCC</strong></li><li><strong>PLP</strong>  </li><li><strong>Pitch</strong></li><li><strong>∆/∆∆</strong> </li></ul><h3 id="模数转换-Analog-to-Digital-Conversion，ADC"><a href="#模数转换-Analog-to-Digital-Conversion，ADC" class="headerlink" title="模数转换 Analog to Digital Conversion，ADC"></a>模数转换 Analog to Digital Conversion，ADC</h3><h3 id="音素-Phoneme"><a href="#音素-Phoneme" class="headerlink" title="音素 Phoneme"></a>音素 Phoneme</h3><p>描述一种语言的基本单位被称为音素Phoneme，例如BRYAN这个词就可以看做是由B, R, AY, AX, N五个音素构成的。英语中大概有50多个音素，可以用50几个HMM state来表示这些音素。</p><h3 id="掩蔽效应-Masking-Effects"><a href="#掩蔽效应-Masking-Effects" class="headerlink" title="掩蔽效应 Masking Effects"></a>掩蔽效应 Masking Effects</h3><h3 id="频谱图-Spectrogram"><a href="#频谱图-Spectrogram" class="headerlink" title="频谱图 Spectrogram"></a>频谱图 Spectrogram</h3><h3 id="声学模型-Acoustic-model"><a href="#声学模型-Acoustic-model" class="headerlink" title="声学模型 Acoustic model"></a>声学模型 Acoustic model</h3><p>语音中很多发音都是连在一起的，很难区分，所以一般用左中右三个HMM state来描述一个音素。举例来说BRYAN这个词中的R音素就变成了用B-R, R, R-AY三个HMM state来表示。这样BRYAN这个词根据上下文就需要15个state了，根据所有单词的上下文总共大概需要几千个HMM state，这种方式属于context dependent模型中的三音素triphone模式。这个HMM state的个数在各家语音识别系统中都不一样，是一个需要调的参数。所以声学模型就是如何设置HMM state。</p><h3 id="语音模型-Language-Model-LM"><a href="#语音模型-Language-Model-LM" class="headerlink" title="语音模型 Language Model, LM"></a>语音模型 Language Model, LM</h3><h3 id="Lexicon"><a href="#Lexicon" class="headerlink" title="Lexicon"></a>Lexicon</h3><h3 id="帧-Frame"><a href="#帧-Frame" class="headerlink" title="帧 Frame"></a>帧 Frame</h3><h3 id="Chunk"><a href="#Chunk" class="headerlink" title="Chunk"></a>Chunk</h3><h3 id="Mean-Opinion-Score-MOS"><a href="#Mean-Opinion-Score-MOS" class="headerlink" title="Mean Opinion Score, MOS"></a>Mean Opinion Score, MOS</h3><p>MOS测试的主要原理是让测评人根据5个等级划分对测试语音的主观感受进行打分，它既可以用于对语音质量进行主观评价，也可以用于对说话人特征相似度的评价。MOS 分是对所有测试语句和所有测评人的综合平均结果。</p><p>Shuang Z, Bakis R, Qin Y. IBM voice conversion systems for 2007 TC⁃STAR evaluation‌[J]. Tsinghua Science &amp; Technology, 2008, 13(4): 510⁃514.</p><h3 id="An-ABX-Blind-Test-ABX"><a href="#An-ABX-Blind-Test-ABX" class="headerlink" title="An ABX Blind Test, ABX"></a>An ABX Blind Test, ABX</h3><p>盲听测试（ABX测试）主要针对转换后语音的说话人特征相似度进行转换效果评价，借鉴了说话人识别的原理。测试过程中，测评人分别测听3段语音A、B和X，并判断在语音的个性特征方面语音A还是B更接近于X。其中，X是转换后得到的语音，而A和B分别为源语音和目标语音。最后统计所有测评人员的判决结果，计算出听起来像目标语音的百分比。</p><h3 id="Text-to-Speech-TTS"><a href="#Text-to-Speech-TTS" class="headerlink" title="Text to Speech, TTS"></a>Text to Speech, TTS</h3><h3 id="Voice-Conversion-VC"><a href="#Voice-Conversion-VC" class="headerlink" title="Voice Conversion, VC"></a>Voice Conversion, VC</h3><p>Voice conversion (VC) is a technique to modify the speech from source speaker to make it sound like being uttered by target speaker while keeping the linguistic content unchanged</p><h3 id="声码器-vocoder"><a href="#声码器-vocoder" class="headerlink" title="声码器 vocoder"></a>声码器 vocoder</h3><p>声码器（vocoder)语音信号某种模型的语音分析合成系统。在传输中只利用模型参数，在编译码时利用模型参数估计和语音合成技术的语音信号编译码器，一种对话音进行分析和合成的编、译码器，也称话音分析合成系统或话音频带压缩系统。它是压缩通信频带和进行保密通信的有力工具。</p><h3 id="隐马尔可夫模型-Hidden-Markov-Model-HMM"><a href="#隐马尔可夫模型-Hidden-Markov-Model-HMM" class="headerlink" title="隐马尔可夫模型 Hidden Markov Model, HMM"></a>隐马尔可夫模型 Hidden Markov Model, HMM</h3><h3 id="强制对齐"><a href="#强制对齐" class="headerlink" title="强制对齐"></a>强制对齐</h3><p>训练DNN的时候，需要知道每一帧对应的是什么音素（甚至HMM状态）。而一般语音数据的标注，只有音素串，并不知道每个音素（或HMM状态）的起止时间。“强制对齐”就是利用一个GMM-HMM模型，求出每个音素（或HMM状态）的起止时间。</p><p>基于联结时序分类(CTC)的声学模型不再需要对训练的音频序列和文本序列进行强制对齐</p><h3 id="Linguistic"><a href="#Linguistic" class="headerlink" title="Linguistic"></a>Linguistic</h3><h3 id="双语者-Bilingual"><a href="#双语者-Bilingual" class="headerlink" title="双语者 Bilingual"></a>双语者 Bilingual</h3><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://mp.weixin.qq.com/s/_jK4eTdboU9A-E785FUBJg" target="_blank" rel="noopener">CSLT微信公众号-语音识别基础：（一）语音是什么</a></p><p>[2] <a href="https://mp.weixin.qq.com/s/XRTCCcEdmQTk4XitVo8U2Q" target="_blank" rel="noopener">CSLT微信公众号-语音识别基础：（二）语音识别方法</a></p><p>[3] <a href="https://mp.weixin.qq.com/s/e4bO-koOmIrAfqC-zwOvwg" target="_blank" rel="noopener">语音识别基础：（三）完整的语音识别实验</a></p><p>[4] <a href="http://sjcj.nuaa.edu.cn/sjcjycl/article/html/201905001" target="_blank" rel="noopener">陆军工程大学-语音转换技术研究现状及展望(2019)</a></p>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【毕业要紧】 Tacotron &amp; WaveNet Resources</title>
    <link href="/2020/07/03/research_note/008_tacotron_wavenet/"/>
    <url>/2020/07/03/research_note/008_tacotron_wavenet/</url>
    
    <content type="html"><![CDATA[<ol><li>衔接式 <strong>Concatenative TTS</strong>：这里首先记住单个的语音片段，然后需要重新生成时，再把这些声音片段联合起来。但是，这种方法不能扩展，因为它只能重现记忆中的声音片段，并且如果没有从开始记忆语音片段，就不能生成新的讲话人或不同类型的语音。</li><li>参数式 <strong>Parametric TTS</strong>：这里创建一个模型，用以存储所有要合成语音的音频特征。在 WaveNet 之前，使用参数式 TTS 生成的音频比衔接式 TTS 更不自然。WaveNet 通过直接对音频的产生过程建模改进了当前最先进的技术，而不是使用以前的中间信号处理算法。</li></ol><h2 id="WaveNet"><a href="#WaveNet" class="headerlink" title="WaveNet"></a>WaveNet</h2><p>Wavenet由DeepMind提出，是一种利用神经网络系统对原始音频波形（Raw SoundWave）建模的技术，生成的音频质量优于所有的文本转语音（Text-to-Speech，TTS）系统，将计算机输出音频与人类自然语音差距缩小50%，被称为世界最佳。</p>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【毕业要紧】 TTS &amp; VC Resources</title>
    <link href="/2020/07/03/research_note/009_tts_vc_tips/"/>
    <url>/2020/07/03/research_note/009_tts_vc_tips/</url>
    
    <content type="html"><![CDATA[<h2 id="TTS-vs-VC"><a href="#TTS-vs-VC" class="headerlink" title="TTS vs VC"></a>TTS vs VC</h2><p><strong>Voice Conversion, VC</strong>: 语音转换处理的问题是输入一段声音，输出另外一段声音，但这两段声音有些不同，一般我们希望保留声音的内容，改变说话者的音色</p><p>Voice conversion (VC) is a technique to modify the speech from source speaker to make it sound like being uttered by target speaker while keeping the linguistic content unchanged</p><p><strong>Text to Speech, TTS</strong>: 文字转语音系统是将一般语言的文字转换为语音。输入一段文本，输出一段语音</p><h2 id="Toolkits"><a href="#Toolkits" class="headerlink" title="Toolkits"></a>Toolkits</h2><h3 id="Pytorch-WaveNet-vocoder"><a href="#Pytorch-WaveNet-vocoder" class="headerlink" title="Pytorch WaveNet vocoder"></a><a href="https://github.com/r9y9/wavenet_vocoder" target="_blank" rel="noopener">Pytorch WaveNet vocoder</a></h3><p>The goal of the repository is to provide an implementation of the WaveNet vocoder, which can generate high quality raw speech samples conditioned on linguistic or acoustic features.</p><h3 id="Pytorch-tacotron2"><a href="#Pytorch-tacotron2" class="headerlink" title="Pytorch tacotron2"></a><a href="https://github.com/NVIDIA/tacotron2" target="_blank" rel="noopener">Pytorch tacotron2</a></h3><p>PyTorch implementation of Natural TTS Synthesis By Conditioning Wavenet On Mel Spectrogram Predictions.</p><p>This implementation includes distributed and automatic mixed precision support and uses the LJSpeech dataset.</p><p>Distributed and Automatic Mixed Precision support relies on NVIDIA’s Apex and AMP.</p><h3 id="Pytorch-waveglow"><a href="#Pytorch-waveglow" class="headerlink" title="Pytorch waveglow"></a><a href="https://github.com/NVIDIA/waveglow/tree/master" target="_blank" rel="noopener">Pytorch waveglow</a></h3><p>WaveGlow combines insights from Glow and WaveNet in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression.</p><h3 id="Pytorch-MelGan-official"><a href="#Pytorch-MelGan-official" class="headerlink" title="Pytorch MelGan (official)"></a><a href="https://github.com/descriptinc/melgan-neurips" target="_blank" rel="noopener">Pytorch MelGan (official)</a></h3><p>Train GANs reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. </p><h2 id="VC-articles-amp-Papers"><a href="#VC-articles-amp-Papers" class="headerlink" title="VC articles &amp; Papers"></a>VC articles &amp; Papers</h2><h3 id="陆军工程大学-语音转换技术研究现状及展望-2019"><a href="#陆军工程大学-语音转换技术研究现状及展望-2019" class="headerlink" title="陆军工程大学-语音转换技术研究现状及展望(2019)"></a><a href="http://sjcj.nuaa.edu.cn/sjcjycl/article/html/201905001" target="_blank" rel="noopener">陆军工程大学-语音转换技术研究现状及展望(2019)</a></h3><p>语音转换通常是指将一个人的声音个性化特征通过“修改变换”，使之听起来像另外一个人的声音，同时保持说话内容信息不变。近年来，随着信息处理和机器学习技术的快速发展，语音转换技术也得到了突飞猛进的进步。为此，在简要介绍语音转换基本概念的基础上，重点综述了近几年语音转换的典型模型和方法，分析了语音转换的关键技术，列举了语音转换技术的主要应用场景，梳理了目前语音转换中仍存在的若干技术问题，并展望了语音转换研究的发展方向。</p><h3 id="An-overview-of-voice-conversion-systems-2017"><a href="#An-overview-of-voice-conversion-systems-2017" class="headerlink" title="An overview of voice conversion systems(2017)"></a><a href="https://www.sciencedirect.com/science/article/abs/pii/S0167639315300698" target="_blank" rel="noopener">An overview of voice conversion systems(2017)</a></h3><p>Voice transformation (VT) aims to change one or more aspects of a speech signal while preserving lin- guistic information. A subset of VT, Voice conversion (VC) specifically aims to change a source speaker’s speech in such a way that the generated output is perceived as a sentence uttered by a target speaker. Despite many years of research, VC systems still exhibit deficiencies in accurately mimicking a target speaker spectrally and prosodically, and simultaneously maintaining high speech quality. In this work we provide an overview of real-world applications, extensively study existing systems proposed in the literature, and discuss remaining challenges.</p><h3 id="Maigo-语音转换技术综述-2019"><a href="#Maigo-语音转换技术综述-2019" class="headerlink" title="Maigo-语音转换技术综述(2019)"></a><a href="https://zhuanlan.zhihu.com/p/89763124" target="_blank" rel="noopener">Maigo-语音转换技术综述(2019)</a></h3><p>i-vector 与 PLDA 的数学推导是出了名的复杂，我曾在 2011 年的夏天推导过一个多星期。在这里，我尽可能避开数学细节，用最简洁的语言引入相关概念和方法。</p><h4 id="几种传统的语音转换方法"><a href="#几种传统的语音转换方法" class="headerlink" title="几种传统的语音转换方法"></a>几种传统的语音转换方法</h4><ul><li>高斯混合模型（Gaussian mixture models, GMM）</li><li>频率弯折法（frequency warping）</li><li>基于样例的方法（exemplar-based method）</li></ul><h4 id="几种现代的语音转换方法"><a href="#几种现代的语音转换方法" class="headerlink" title="几种现代的语音转换方法"></a>几种现代的语音转换方法</h4><ul><li>生成对抗式网络（generative adversarial networks, GAN）</li><li>i-vector + PLDA（probabilistic linear discriminant analysis）</li><li>自编码器（autoencoders）</li></ul><h2 id="TTS-articles-amp-Papers"><a href="#TTS-articles-amp-Papers" class="headerlink" title="TTS articles &amp; Papers"></a>TTS articles &amp; Papers</h2><h3 id="清华大学王东老师-TTS"><a href="#清华大学王东老师-TTS" class="headerlink" title="清华大学王东老师-TTS"></a><a href="http://cslt.riit.tsinghua.edu.cn/mediawiki/images/a/a8/Tts.pdf" target="_blank" rel="noopener">清华大学王东老师-TTS</a></h3>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【毕业要紧】 Speaker Embedding Resources</title>
    <link href="/2020/07/03/research_note/011_speaker_embedding/"/>
    <url>/2020/07/03/research_note/011_speaker_embedding/</url>
    
    <content type="html"><![CDATA[<h2 id="Eval-Metrics"><a href="#Eval-Metrics" class="headerlink" title="Eval Metrics"></a>Eval Metrics</h2><h3 id="ROC-amp-EER"><a href="#ROC-amp-EER" class="headerlink" title="ROC &amp; EER"></a>ROC &amp; EER</h3><h2 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h2><h3 id="i-vector"><a href="#i-vector" class="headerlink" title="i-vector"></a>i-vector</h3><h3 id="x-vector"><a href="#x-vector" class="headerlink" title="x-vector"></a>x-vector</h3><h2 id="Backends"><a href="#Backends" class="headerlink" title="Backends"></a>Backends</h2>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【毕业要紧】 Word Embedding Resources</title>
    <link href="/2020/07/03/research_note/010_NLP/"/>
    <url>/2020/07/03/research_note/010_NLP/</url>
    
    <content type="html"><![CDATA[<p>one-hot 编码存在的⼀个主要问题是它⽆法表示出词汇间的相似度。</p>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>毕业要紧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性代数复习</title>
    <link href="/2020/06/30/research_note/006_solver/"/>
    <url>/2020/06/30/research_note/006_solver/</url>
    
    <content type="html"><![CDATA[<h2 id="solver"><a href="#solver" class="headerlink" title="solver"></a>solver</h2><ul><li>SVD: Singular value decomposition</li><li>LSQR: Least squares solution, can be combined with shrinkage</li><li>EIGEN: Eigenvalue decomposition, can be combined with shrinkage</li></ul><h2 id="线性空间"><a href="#线性空间" class="headerlink" title="线性空间"></a>线性空间</h2><h3 id="什么是线性空间"><a href="#什么是线性空间" class="headerlink" title="什么是线性空间"></a>什么是线性空间</h3><ul><li>空集 $\varnothing$</li><li>有理数集 $\mathbb{Q}$</li><li>实数集合 $\mathbb{R}$</li><li>复数集 $\mathbb{C}$</li></ul><p><strong>数域</strong>：对加、减、乘、除四则运算封闭的非空数集</p><h3 id="线性空间的性质"><a href="#线性空间的性质" class="headerlink" title="线性空间的性质"></a>线性空间的性质</h3>]]></content>
    
    
    <categories>
      
      <category>概率论与线性代数</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>美化 Python 异常信息</title>
    <link href="/2020/06/17/2020/200617_pyerrors/"/>
    <url>/2020/06/17/2020/200617_pyerrors/</url>
    
    <content type="html"><![CDATA[<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>Github链接<a href="https://github.com/onelivesleft/PrettyErrors" target="_blank" rel="noopener">PrettyErrors</a></p><pre><code class="hljs bash">python -m pip install pretty_errorspython -m pretty_errors</code></pre><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://zhuanlan.zhihu.com/p/100234650" target="_blank" rel="noopener">知乎-一行代码简化Python异常信息</a></p>]]></content>
    
    
    <categories>
      
      <category>toolkit</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开发</tag>
      
      <tag>工具</tag>
      
      <tag>效率</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机网络复习</title>
    <link href="/2020/06/17/2020/200621_computer_network/"/>
    <url>/2020/06/17/2020/200621_computer_network/</url>
    
    <content type="html"><![CDATA[<h2 id="工作装备"><a href="#工作装备" class="headerlink" title="工作装备"></a>工作装备</h2><ol><li>物理层：中继器，集线器</li><li>数据链路：网桥，交换机</li><li>网络层：路由器 </li><li>传输层： 网关</li></ol><h2 id="网络协议"><a href="#网络协议" class="headerlink" title="网络协议"></a>网络协议</h2><ol><li>物理层： RJ45 、 CLOCK 、 IEEE802.3</li><li>数据链路： PPP 、 FR 、 HDLC 、 VLAN 、 MAC</li><li>网络层： IP 、 ICMP 、 ARP 、 RARP 、 OSPF 、 IPX 、 RIP 、 IGRP</li><li>传输层： TCP 、 UDP 、 SPX</li><li>会话层： NFS 、 SQL 、 NETBIOS 、 RPC</li><li>表示层： JPEG 、 MPEG 、 ASII</li><li>应用层： FTP 、 DNS 、 Telnet 、 SMTP 、 HTTP 、 WWW 、 NFS</li></ol>]]></content>
    
    
    <categories>
      
      <category>offer</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DNF 模型</title>
    <link href="/2020/06/16/research_note/003_DNF/"/>
    <url>/2020/06/16/research_note/003_DNF/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>flows</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>笔记</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>flow 模型的Pytorch实现</title>
    <link href="/2020/06/16/research_note/004_flow_implement/"/>
    <url>/2020/06/16/research_note/004_flow_implement/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>flows</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>笔记</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DNF 模型的Pytorch实现</title>
    <link href="/2020/06/16/research_note/005_DNF_implement/"/>
    <url>/2020/06/16/research_note/005_DNF_implement/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>flows</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>笔记</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linear Discriminant Analysis</title>
    <link href="/2020/06/14/2020/200614_Linear_Discriminant_Analysis/"/>
    <url>/2020/06/14/2020/200614_Linear_Discriminant_Analysis/</url>
    
    <content type="html"><![CDATA[<p>线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的线性分类方法。LDA是一种有监督的线性分类算法。LDA的基本思想是将数据投影到低维空间后，使得：<strong>同一类数据尽可能接近，不同类数据尽可能疏远</strong>。在对新样本进行分类时，将其投影到同样的低维空间上，再根据投影点的位置来确定新样本的类别。</p><p><img src="https://wx1.sbimg.cn/2020/06/14/lda.png" srcset="/img/loading.gif" alt="avatar"></p><p>值得一提的是，LDA 可从贝时斯决策理论的角度来阐释，并可证明，当两类数据同先验、满足高斯分布且协方差相等时，LDA 可达到最优分类。</p><h2 id="Fisher-discriminant-criterion"><a href="#Fisher-discriminant-criterion" class="headerlink" title="Fisher discriminant criterion"></a>Fisher discriminant criterion</h2><ol><li>$X_i$ 表示 $i$ 类示例的集合</li><li>$N$ 表示有 $N$ 类，且第 $i$ 类的样本数为 $N_i$</li><li>$n$ 表示总共有 $n$ 个数据</li><li>$\mu_i$ 表示 $i$ 类示例的均值向量，$\mu$ 表示总的均值向量</li><li>$\Sigma_i$ 表示 $i$ 类示例的协方差矩阵</li><li>$S_w$ 表示类内散度矩阵（within-class scatter matrix）</li><li>$S_b$ 表示类间散度矩阵（between-class scatter matrix）</li><li>$S_t$ 表示全局散度矩阵</li><li>$w$ 为变换矩阵</li><li>$J(w)$ 是最大目标Fisher判别准则</li></ol><p>$$\boldsymbol{\mu_i} = \frac{1}{N_i} \sum_{\boldsymbol{x} \in X_i}\boldsymbol{x}$$</p><p>$$\Sigma_i = \sum_{\boldsymbol{x} \in X_i}(\boldsymbol{x}-\boldsymbol{\mu_i})(\boldsymbol{x}-\boldsymbol{\mu_i})^T$$</p><p>$$S_w = \sum^N_{i=1}S_{w_i} = \sum^N_{i=1}\Sigma_i = \sum^N_{i=1}\sum_{\boldsymbol{x} \in X_i}(\boldsymbol{x}-\boldsymbol{\mu_i})(\boldsymbol{x}-\boldsymbol{\mu_i})^T$$</p><p>$$S_b = S_t - S_w = \sum^N_{i=1}N_i(\boldsymbol{\mu_i}-\boldsymbol{\mu})(\boldsymbol{\mu_i}-\boldsymbol{\mu})^T$$</p><p>$$S_t = S_b + S_w =\sum_{i=1}^{n}(\boldsymbol{x_i}-\boldsymbol{\mu})(\boldsymbol{x_i}-\boldsymbol{\mu})^T$$</p><p>$$J(w) = \frac{w^TS_bw}{w^TS_ww}$$</p><p>优化目标推导过程</p><p>$$ \frac{\mathrm{d}J(w)}{\mathrm{d}w} = 0 $$</p><p>$$<br>\frac{\mathrm{d}J(w)}{\mathrm{d}w} = \frac{\mathrm{d}}{\mathrm{d}w}(\frac{w^TS_bw}{w^TS_ww}) =<br>$$</p><p><img src="https://wx2.sbimg.cn/2020/06/14/lda_s.png" srcset="/img/loading.gif" alt="avatar"></p><p>多分类 LDA 可以有多种实现方法：使用 $S_b$, $S_w$ , $S_t$ 三者中的任何两个即可</p><h2 id="LDA-算法的训练流程"><a href="#LDA-算法的训练流程" class="headerlink" title="LDA 算法的训练流程"></a>LDA 算法的训练流程</h2><ol><li>计算类内散度矩阵 $S_w$</li><li>计算类间散度矩阵 $S_b$</li><li>计算矩阵 $S_w^{-1}S_b$</li><li>计算矩阵 $S_w^{-1}S_b$ 的特征值与特征向量，按从小到大的顺序选取前 $d$ 个特征值和对应的 $d$ 个特征向量，得到投影矩阵 $w$</li></ol><h2 id="sklearn包-LDA-的使用"><a href="#sklearn包-LDA-的使用" class="headerlink" title="sklearn包 LDA 的使用"></a>sklearn包 LDA 的使用</h2><pre><code class="hljs Python"><span class="hljs-keyword">from</span> sklearn.discriminant_analysis <span class="hljs-keyword">import</span> LinearDiscriminantAnalysis <span class="hljs-keyword">as</span> LDAlda = LDA(solver=<span class="hljs-string">'svd'</span>, n_components=LDA_components)<span class="hljs-comment"># fit</span>lda.fit(train_data, train_label)<span class="hljs-comment"># transform</span>train_data = lda.transform(train_data)test_data = lda.transform(test_data)</code></pre><h2 id="LDA-Python-代码实现"><a href="#LDA-Python-代码实现" class="headerlink" title="LDA Python 代码实现"></a>LDA Python 代码实现</h2><h2 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="http://wangd.cslt.org/teach/speech-processing/3.2-SA-LDA.ppt" target="_blank" rel="noopener">CSLT-THU王东老师PPT-Static Analysis: LDA</a></p><p>[2] <a href="">西瓜书-线性判别分析</a></p><p>[3] <a href="http://166.111.134.19:7777/mlbook/" target="_blank" rel="noopener">CSLT-THU王东老师-现代机器学习导论</a></p><p>[4] <a href="https://mp.weixin.qq.com/s/PZpEtcdiPUxIv6M3sGOaGA" target="_blank" rel="noopener">机器学习实验室微信公众号-数学推导LDA线性判别分析</a></p><p>[5] <a href="https://www.cnblogs.com/timlong/p/11403709.html" target="_blank" rel="noopener">博客园-LDA</a></p><p>[6] <a href="https://zhuanlan.zhihu.com/p/79696530" target="_blank" rel="noopener">知乎-线性判别分析LDA原理及推导过程（非常详细）</a></p><p>[7] <a href="https://www.bilibili.com/video/BV15t411v7Pg?p=7" target="_blank" rel="noopener">THU袁博老师数据挖掘课程-数据预处理PPT</a></p><p>[8] <a href="https://zhuanlan.zhihu.com/p/33742983" target="_blank" rel="noopener">知乎-Fisher判别分析(Fisher Discriminant Analysis)</a></p>]]></content>
    
    
    <categories>
      
      <category>backends</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多个 PDF 文件的快速合并</title>
    <link href="/2020/06/10/2020/200610_pdf_merge/"/>
    <url>/2020/06/10/2020/200610_pdf_merge/</url>
    
    <content type="html"><![CDATA[<p>毕设论文需要合并多个pdf文件，记录一下pdf文件的合并过程</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><pre><code class="hljs bash">sudo apt-get install poppler-utils</code></pre><h2 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h2><pre><code class="hljs bash"><span class="hljs-comment"># 全部</span>pdfunite *.pdf all.pdf<span class="hljs-comment"># 顺序</span>pdfunite 1.pdf 2.pdf 3.pdf 4.pdf all-1234.pdf</code></pre><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://blog.csdn.net/a912952381/article/details/80682669" target="_blank" rel="noopener">linux bash合并PDF文件</a></p>]]></content>
    
    
    <categories>
      
      <category>toolkit</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开发</tag>
      
      <tag>工具</tag>
      
      <tag>效率</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Softmax &amp; Cross-Entropy &amp; KLD</title>
    <link href="/2020/06/09/2020/200609_softmax/"/>
    <url>/2020/06/09/2020/200609_softmax/</url>
    
    <content type="html"><![CDATA[<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>$ Softmax $ 的作用是把一个序列$\boldsymbol{a}$，变成概率。</p><p>$$ Softmax(\boldsymbol{a}) = \frac{e^{a_i}}{\sum_j e^{a_j}} $$</p><p>从概率的角度解释 $Softmax$ 的话，就是 $S_i = \frac{e^{a_i}}{\sum_{k=1}^Ne^{a_k}} = P(y=i|a)$</p><p>其中，$LR$ 是 $Softmax$ 的类别数为 2 时 $Softmax$ 的特殊形式</p><h2 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross-Entropy"></a>Cross-Entropy</h2><p>给定两个概率分布： $p$ （理想结果即正确标签向量）和 $q$ （神经网络输出结果即经过 $softmax$ 转换后的结果向量），则通过 $q$ 来表示 $p$ 的交叉熵为</p><p>$$ H(p, q) = -\sum_x p(x)log(q(x)) $$</p><p>例如</p><p>$$H(p=[1,0,0], q=[0.5,0.4,0.1])  = -(1 \cdot log(0.5) + 0 \cdot log(0.4) + 0 \cdot log(0.1)) $$</p><h2 id="nll-loss-negative-log-likelihood-loss"><a href="#nll-loss-negative-log-likelihood-loss" class="headerlink" title="nll_loss (negative log likelihood loss)"></a>nll_loss (negative log likelihood loss)</h2>]]></content>
    
    
    <categories>
      
      <category>概率论与线性代数</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>概率论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch Data Loader</title>
    <link href="/2020/06/09/2020/200609_pytorch_data_loader/"/>
    <url>/2020/06/09/2020/200609_pytorch_data_loader/</url>
    
    <content type="html"><![CDATA[<h2 id="GIL"><a href="#GIL" class="headerlink" title="GIL"></a>GIL</h2><p>Python 自带 Gobal Interpreter Lock (GIL)，任何时候，Python只能运行一个线程</p><h2 id="DotaLoader的构建"><a href="#DotaLoader的构建" class="headerlink" title="DotaLoader的构建"></a><code>DotaLoader</code>的构建</h2><pre><code class="hljs Python">DataLoader(dataset, batch_size=<span class="hljs-number">200</span>, shuffle=<span class="hljs-literal">False</span>, sampler=<span class="hljs-literal">None</span>,           batch_sampler=<span class="hljs-literal">None</span>, num_workers=<span class="hljs-number">0</span>, collate_fn=<span class="hljs-literal">None</span>,           pin_memory=<span class="hljs-literal">False</span>, drop_last=<span class="hljs-literal">False</span>, timeout=<span class="hljs-number">0</span>,           worker_init_fn=<span class="hljs-literal">None</span>)</code></pre><h2 id="官方模板"><a href="#官方模板" class="headerlink" title="官方模板"></a>官方模板</h2><p>PyTorch官方为我们提供了自定义数据读取的标准化代码代码模块。</p><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CustomDataset</span><span class="hljs-params">(Dataset)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, ...)</span>:</span>        <span class="hljs-keyword">pass</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span><span class="hljs-params">(self, index)</span>:</span>        <span class="hljs-keyword">return</span> (img, label)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-comment"># return examples size</span>        <span class="hljs-keyword">return</span> count</code></pre><ol><li><code>__init__()</code>函数用于初始化数据读取逻辑，比如读取包含标签和图片地址的csv文件、定义transform组合等。</li><li><code>__getitem__()</code>函数用来返回数据和标签。目的上是为了能够被后续的dataloader所调用。</li><li><code>__len__()</code>函数则用于返回样本数量。</li></ol><p>其中，<code>__getitem__()</code> 和<code>__len__()</code>用于构建Map-style datasets；<code>__iter__()</code>用于构建Iterable-style datasets（一般不太用）</p><h2 id="训练集和验证集的划分"><a href="#训练集和验证集的划分" class="headerlink" title="训练集和验证集的划分"></a>训练集和验证集的划分</h2><p>如果需要对数据划分训练集和验证集，torch的Dataset对象也提供了<code>random_split</code>函数作为数据划分工具，且划分结果可直接供后续的DataLoader使用。</p><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> random_splittrainset, valset = random_split(dataset, [len_dataset*<span class="hljs-number">0.7</span>, len_dataset*<span class="hljs-number">0.3</span>])</code></pre><h2 id="Pytorch-并行化"><a href="#Pytorch-并行化" class="headerlink" title="Pytorch 并行化"></a>Pytorch 并行化</h2><ol><li>Data Parallel, DP: 数据并行化</li><li>Distributed Data Parallel, DDP: 分布式数据并行化</li></ol><h2 id="Mnist-Dataset-的实现"><a href="#Mnist-Dataset-的实现" class="headerlink" title="Mnist Dataset 的实现"></a>Mnist Dataset 的实现</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://pytorch.org/docs/stable/data.html" target="_blank" rel="noopener">官方文档 torch.utils.data</a></p><p>[2] <a href="https://mp.weixin.qq.com/s/5fXsPCpI_eYeEC12CbF17g" target="_blank" rel="noopener">夕小瑶的卖萌屋-PyTorch数据Pipeline标准化代码模板</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>概率论复习</title>
    <link href="/2020/06/08/research_note/007_bayes/"/>
    <url>/2020/06/08/research_note/007_bayes/</url>
    
    <content type="html"><![CDATA[<h2 id="加法公式"><a href="#加法公式" class="headerlink" title="加法公式"></a>加法公式</h2><p>$$ P(A \cup B) = P(A) + P(B) - P(A\cap B) $$</p><h2 id="乘法公式"><a href="#乘法公式" class="headerlink" title="乘法公式"></a>乘法公式</h2><p>$$ P(AB) = P(A)P(B|A) $$</p><h2 id="协方差矩阵-与-散度矩阵"><a href="#协方差矩阵-与-散度矩阵" class="headerlink" title="协方差矩阵 与 散度矩阵"></a>协方差矩阵 与 散度矩阵</h2><p>将协方差矩阵乘以系数 $n-1$ 就得到了散度矩阵，矩阵的大小由特征维数 $d$ 决定，是一个为 $d×d$ 的半正定矩阵</p><h2 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h2><p>贝叶斯公式用于描述两个<strong>条件概率</strong>之间的关系，比如 $P(A|B)$ 和 $P(B|A)$。</p><p>$$ P(A|B) = P(A) \frac{P(B|A)}{P(B)} $$</p><ol><li>$P(A|B)$ ：后验概率</li><li>$P(A)$：先验概率</li></ol><p>$$ P(A_i|B) = \frac{P(A_i)P(B|A_i)}{\sum^n_{j=1} P(A_j)P(B|A_j)} $$</p><h2 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h2><p>$$ P(B) = \sum^n_{i=1} P(A_i)P(B|A_i) $$</p><p>事件 $B$ 总伴随着某个 $A_i$ 出现</p><h2 id="贝叶斯公式与全概率公式之间的关系"><a href="#贝叶斯公式与全概率公式之间的关系" class="headerlink" title="贝叶斯公式与全概率公式之间的关系"></a>贝叶斯公式与全概率公式之间的关系</h2><p>$P(A_i|B)$ 后验概率（新信息 $B$ 出现后 $A$ 发生的概率） = $P(A)$ 先验概率（ $A$ 发生的概率） $ｘ$ 可能性函数（新信息带出现来的调整）</p><h2 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h2><p>条件概率是指事件 $A$ 在另外一个事件 $B$ 已经发生条件下的发生概率。条件概率表示为：$P(A|B)$，读作“在B的条件下A的概率”或是the probability of x given z</p><h2 id="边缘概率"><a href="#边缘概率" class="headerlink" title="边缘概率"></a>边缘概率</h2><h2 id="贝叶斯估计和极大似然估计"><a href="#贝叶斯估计和极大似然估计" class="headerlink" title="贝叶斯估计和极大似然估计"></a>贝叶斯估计和极大似然估计</h2><ol><li>MLE, Maxium Likelihood Estimator：极大似然估计</li><li>MAP, Maxium a Posterior：最大后验概率</li></ol><p>最大似然估计和贝叶斯估计最大区别便在于估计的<strong>参数</strong>不同。</p><p>最大似然估计是一种确定模型参数值的方法。确定参数值的过程，是找到能最大化模型产生真实观察数据可能性的那一组参数。要估计的参数 $θ$ 被当作是固定形式的一个未知变量，然后我们结合真实数据通过最大化似然函数来求解这个固定形式的未知变量。</p><p>贝叶斯估计则是将参数视为是有某种已知先验分布的随机变量，意思便是这个参数他不是一个固定的未知数，而是符合一定先验分布如：随机变量θ符合正态分布等！那么在贝叶斯估计中除了类条件概率密度 $p(x|w)$ 符合一定的先验分布，参数 $θ$ 也符合一定的先验分布。我们通过贝叶斯规则将参数的先验分布转化成后验分布进行求解。</p><p>在贝叶斯模型使用过程中，贝叶斯估计用的是后验概率，而最大似然估计直接使用的是类条件概率密度。</p><blockquote><p>最大似然估计（和其他类似方法）把待估计的参数看作是确定性的量，只是其取值未知。最佳估计就是使得产生已观测到的样本（即训练样本）的概率为最大的那个值。</p><p>与此不同的是，贝叶斯估计则把待估计的参数看成是符合某种先验分布的随机变量。对样本进行观测的过程，就是把先验概率密度转化为后验概率密度，这样就利用样本的信息修正了对参数的初始估计值。</p></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://www.bilibili.com/video/BV1a4411B7B4" target="_blank" rel="noopener">B站-「一个模型」教你搞定贝叶斯和全概率公式</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/32803109" target="_blank" rel="noopener">知乎-从最大似然估计开始，你需要打下的机器学习基石</a></p><p>[3] <a href="https://www.bilibili.com/video/BV1C7411c7bs?p=1" target="_blank" rel="noopener">B站-MLE(极大似然)和MAP(最大后验)</a></p>]]></content>
    
    
    <categories>
      
      <category>概率论与线性代数</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>概率论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线段树</title>
    <link href="/2020/05/31/2020/200531_segment_tree/"/>
    <url>/2020/05/31/2020/200531_segment_tree/</url>
    
    <content type="html"><![CDATA[<h2 id="线段树的功能"><a href="#线段树的功能" class="headerlink" title="线段树的功能"></a>线段树的功能</h2><p>线段树用来处理数组的<strong>区间查询</strong>（range query）和<strong>元素更新</strong>（update）操作。可以进行<strong>区间最大值</strong>，<strong>区间最小值</strong>或者<strong>区间异或值</strong>的查询。对应于树状数组，线段树进行更新（update）的操作为<code>O(logn)</code>，进行区间查询（range query）的操作也为<code>O(logn)</code>。</p><h2 id="线段树的初始化"><a href="#线段树的初始化" class="headerlink" title="线段树的初始化"></a>线段树的初始化</h2><h2 id="更新（update）"><a href="#更新（update）" class="headerlink" title="更新（update）"></a>更新（update）</h2><h2 id="区间查询（range-query）"><a href="#区间查询（range-query）" class="headerlink" title="区间查询（range query）"></a>区间查询（range query）</h2><h2 id="完整的板子"><a href="#完整的板子" class="headerlink" title="完整的板子"></a>完整的板子</h2><h2 id="参考练习题"><a href="#参考练习题" class="headerlink" title="参考练习题"></a>参考练习题</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2>]]></content>
    
    
    <categories>
      
      <category>板子</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>算法</tag>
      
      <tag>板子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FFT 板子</title>
    <link href="/2020/05/31/2020/200601_FFT/"/>
    <url>/2020/05/31/2020/200601_FFT/</url>
    
    <content type="html"><![CDATA[<h2 id="傅里叶变换-VS-离散傅里叶变换"><a href="#傅里叶变换-VS-离散傅里叶变换" class="headerlink" title="傅里叶变换 VS 离散傅里叶变换"></a>傅里叶变换 VS 离散傅里叶变换</h2>]]></content>
    
    
    <categories>
      
      <category>板子</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>算法</tag>
      
      <tag>板子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络结构</title>
    <link href="/2020/05/31/2020/200531_nnet/"/>
    <url>/2020/05/31/2020/200531_nnet/</url>
    
    <content type="html"><![CDATA[<h2 id="多层感知机（MLP-Multilayer-Perceptron）"><a href="#多层感知机（MLP-Multilayer-Perceptron）" class="headerlink" title="多层感知机（MLP, Multilayer Perceptron）"></a>多层感知机（MLP, Multilayer Perceptron）</h2><p>Fully Connected Layer</p><h2 id="时延神经网络-TDNN-Time-Delay-Neural-Networks"><a href="#时延神经网络-TDNN-Time-Delay-Neural-Networks" class="headerlink" title="时延神经网络(TDNN, Time-Delay Neural Networks)"></a>时延神经网络(TDNN, Time-Delay Neural Networks)</h2><p>TDNN是一个常用于语音信号处理领域卷积神经网络，使用 FFT 预处理的语音信号作为输入，其隐含层由2个一维卷积核组成，以提取频率域上的平移不变特征。</p><h2 id="卷积神经网络（CNN-Convolutional-Neural-Networks）"><a href="#卷积神经网络（CNN-Convolutional-Neural-Networks）" class="headerlink" title="卷积神经网络（CNN, Convolutional Neural Networks）"></a>卷积神经网络（CNN, Convolutional Neural Networks）</h2><h3 id="内积-dot-product-scalar-product"><a href="#内积-dot-product-scalar-product" class="headerlink" title="内积 (dot product, scalar product)"></a>内积 (dot product, scalar product)</h3><h3 id="Architect"><a href="#Architect" class="headerlink" title="Architect"></a>Architect</h3><ol><li>Convolution</li><li>Pooling</li><li>Flatten</li></ol><h3 id="Property"><a href="#Property" class="headerlink" title="Property"></a>Property</h3><ol><li>Some of patterns are much smaller than the whole image</li><li>The smae pattterns appear in different regions</li><li>Subsampling the pixels will not change the object</li></ol><h3 id="1-x-1-Convolution"><a href="#1-x-1-Convolution" class="headerlink" title="1 x 1 Convolution"></a>1 x 1 Convolution</h3><p>1 x 1 的卷积核用来实现降通道数的操作</p><h2 id="循环神经网络（RNN-Recurrent-Neural-Networks）"><a href="#循环神经网络（RNN-Recurrent-Neural-Networks）" class="headerlink" title="循环神经网络（RNN, Recurrent Neural Networks）"></a>循环神经网络（RNN, Recurrent Neural Networks）</h2><h2 id="长短期记忆网络（LSTM-Long-Short-Term-Memorys）"><a href="#长短期记忆网络（LSTM-Long-Short-Term-Memorys）" class="headerlink" title="长短期记忆网络（LSTM, Long Short-Term Memorys）"></a>长短期记忆网络（LSTM, Long Short-Term Memorys）</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://www.bilibili.com/video/BV1Lb411b7BS?from=search&seid=17888903676501609442" target="_blank" rel="noopener">B站-李宏毅讲解卷积神经网络（带字幕</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>笔记</tag>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>生成式模型 VS 判别式模型</title>
    <link href="/2020/05/30/2020/200530_Difference_between_generative_model_and_discriminative_model/"/>
    <url>/2020/05/30/2020/200530_Difference_between_generative_model_and_discriminative_model/</url>
    
    <content type="html"><![CDATA[<h2 id="生成式模型-VS-判别式模型"><a href="#生成式模型-VS-判别式模型" class="headerlink" title="生成式模型 VS 判别式模型"></a>生成式模型 VS 判别式模型</h2><p><strong>Discriminative models</strong> learn the classification (hard or soft) boundary between classes. A discriminative model learns the conditional probability distribution p(y|x) - which you should read as “the probability of y given x”.</p><p><strong>Generative models</strong> model the distribution of individual classes. A generative model learns the joint probability distribution p(x,y)</p><p>生成模型是模拟这个结果是如何产生的,然后算出产生各个结果的概率。判别模型是发现各个结果之间的不同,不关心产生结果的过程。</p><p><img src="https://wx1.sbimg.cn/2020/05/30/diff_g_d.png" srcset="/img/loading.gif" alt="avatar"></p><h2 id="典型代表模型"><a href="#典型代表模型" class="headerlink" title="典型代表模型"></a>典型代表模型</h2><h3 id="生成式模型"><a href="#生成式模型" class="headerlink" title="生成式模型"></a>生成式模型</h3><ol><li>朴素贝叶斯</li><li>K紧邻（KNN）</li><li>混合高斯模型</li><li>隐马尔科夫模型（HMM）</li><li>贝叶斯网络</li><li>Sigmoid Belief Networks </li><li>马尔科夫随机场（Markov Random Fields）</li><li>深度信念网络（DBN）</li></ol><h3 id="判别式模型"><a href="#判别式模型" class="headerlink" title="判别式模型"></a>判别式模型</h3><ol><li>线性回归（Linear Regression）</li><li>逻辑斯蒂回归（Logistic Regression）</li><li>神经网络（NN）</li><li>支持向量机（SVM）</li><li>高斯过程（Gaussian Process）</li><li>条件随机场（CRF）</li><li>CART(Classification and Regression Tree)</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf" target="_blank" rel="noopener">On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes</a></p><p>[2] <a href="https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-a-discriminative-algorithm/879591#879591" target="_blank" rel="noopener">The difference between a generative and a discriminative algorithm?</a></p><p>[3] <a href="https://www.cnblogs.com/yejintianming00/p/9378810.html" target="_blank" rel="noopener">判别式模型与生成式模型</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>生成式模型</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>优化器</title>
    <link href="/2020/05/30/2020/200530_optimizer/"/>
    <url>/2020/05/30/2020/200530_optimizer/</url>
    
    <content type="html"><![CDATA[<p>深度学习中的优化算法主要围绕梯度下降算法展开，其主要思想是：选取一定的训练样本，按照一定的步长（学习率）沿着梯度的方向调整更新参数，优化模型的目标函数。</p><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p><strong>随机梯度下降法</strong>是每次使用一批数据进行梯度的计算，而非计算全部数据的梯度，因为如果每次计算全部数据的梯度，会导致运算量加大，运算时间变长，容易陷入局部最优解，而随机梯度下降可能每次不是朝着真正最小的方向，这样反而可以跳出局部的最优解。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>笔记</tag>
      
      <tag>优化器</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>My First Blog</title>
    <link href="/2020/05/27/2020/200527_first/"/>
    <url>/2020/05/27/2020/200527_first/</url>
    
    <content type="html"><![CDATA[<p>欢迎来到zyz的个人网站。这是我的第一篇博客。</p>]]></content>
    
    
    
    <tags>
      
      <tag>test</tag>
      
      <tag>博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weekly</title>
    <link href="/2000/10/31/research_note/16_new_flow/"/>
    <url>/2000/10/31/research_note/16_new_flow/</url>
    
    <content type="html"><![CDATA[<h2 id="2020-10-31"><a href="#2020-10-31" class="headerlink" title="2020-10-31"></a>2020-10-31</h2><p>当前很多网上开源的 Normalizing Flow 模型都是在一些 toy 数据测试的，例如使用 <code>np.random.randn()</code> 函数随机生成的 2d 数据。本周我尝试复现了一下toy数据集的结果，下周我想抽时间编写和实现 NF 模型的多 GPU 训练代码（基于 PyTorch 框架）, 换 mnist 数据集测试一下。</p><p><img src="/img/nf/toy_data.png" srcset="/img/loading.gif" alt="avatar"></p>]]></content>
    
    
    <categories>
      
      <category>毕业要紧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>笔记</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Contrastive Learning for Speaker Recogition</title>
    <link href="/2000/01/01/research_note/014_pretrain/"/>
    <url>/2000/01/01/research_note/014_pretrain/</url>
    
    <content type="html"><![CDATA[<ul><li>We don’t know something is blue until we see red.</li><li>We don’t know something is music until we hear noise.</li></ul><p>在过去，无监督学习以例如 VAE、GAN 这样的生成式模型比较多，生成式模型期望利用数据表征重构完整数据。但最近判别式模型逐渐走进了人们的视野。对比学习是何恺明、Hinton、LeCun等大佬最近关注的研究重点之一。对比学习通过数据之间的对比进行表示学习，期望数据表征包含足够多信息即可。近日，唐杰老师对 Self-Supervised Learning 发表了一篇综述性的文章。</p><p>CN-Celeb 数据集的发布展现出当前说话人识别系统在真实环境下的性能表现可能并不理想，系统的鲁棒性还有待增强。<strong>我认为 SimCLR 为代表的对比学习方法可以更好的提高当前说话人识别系统的鲁棒性，特别是可以提高在 CN-Celeb 数据集上的表现。</strong></p><h2 id="Contrastive-Learning"><a href="#Contrastive-Learning" class="headerlink" title="Contrastive Learning"></a>Contrastive Learning</h2><p>对比学习的思路可以用一句话来概括 “We don’t know something is blue until we see red”。它通过数据之间的对比进行表示学习，从而让像的样本所得表示差异小，让不像的样本所得表示差异大。“对比学习”中的“对比”是 positive 样本 和 negative 样本的对比。在学习到的“表示空间”内，增大某样本与其 positive 样本之间的相似度，减少与negative 样本的相似度：</p><p>$$\textrm{score}(f(x),f(x^{+})) &gt;&gt; \textrm{score}(f(x), f(x^{-}))$$</p><p>其中，互信息(Mutual Information, MI)在对比学习中是一个非常重要的概念，是指一个随机变量由于已知另一个随机变量而减少的不确定性，通过最大化互信息进行表征学习，从而期望数据表征包含足够多信息。 $MI(x;c) = \sum_{x,c}p(x,c)log\frac{p(x|c)}{p(x)}$ 。很多大佬也提出了一系列对比学习的 Loss，其中比较有代表性的是：</p><ul><li>InfoNCE (InfoNCE 是 MI 的下界)</li><li>NCE</li><li>Margin Triplet</li></ul><p>在对比学习中，我认为有 3 个非常典型的学习框架可以应用于说话人识别这样的语音信号处理领域中，分别如下:</p><h3 id="1-CPC"><a href="#1-CPC" class="headerlink" title="1. CPC"></a>1. CPC</h3><ul><li>Anchor: 自回归模型历史时序</li><li>Positive: 同一时序未来时刻的信息</li><li>Negative: 不同时序未来时刻的信息</li></ul><h3 id="2-MoCo"><a href="#2-MoCo" class="headerlink" title="2. MoCo"></a>2. MoCo</h3><p>MoCo 是由何恺明一作提出用于 CV 领域的方法。它的网络训练的方法和我之前用 Pytorch 写的 flow 的训练模式很像</p><h3 id="3-SimCLR"><a href="#3-SimCLR" class="headerlink" title="3. SimCLR"></a>3. SimCLR</h3><p>SimCLR 是由 Hinton 参与提出的一个 Constractive Learning 通用学习框架，我认为它的核心是<strong>Data Argument</strong>。</p><h2 id="CN-Celeb-Pretrain-Experiment"><a href="#CN-Celeb-Pretrain-Experiment" class="headerlink" title="CN-Celeb Pretrain Experiment"></a>CN-Celeb Pretrain Experiment</h2><p>CN-Celeb 数据集的发布展现出当前说话人识别系统在真实环境下的性能表现可能并不理想，系统的鲁棒性还有待增强。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://amitness.com/2020/03/illustrated-simclr/" target="_blank" rel="noopener">[Amit Chaudhary] illustrated-simclr</a></p><p>[2] <a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/" target="_blank" rel="noopener">[Amit Chaudhary] SSL</a></p><p>[3] <a href="https://sthalles.github.io/simple-self-supervised-learning/" target="_blank" rel="noopener">[sthalles] simple-self-supervised-learning</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>CPC Work Plan</title>
    <link href="/2000/01/01/research_note/15_cpc_asv_work/"/>
    <url>/2000/01/01/research_note/15_cpc_asv_work/</url>
    
    <content type="html"><![CDATA[<p>实验分为两个阶段，</p><ol><li>搭建 基于MFCC 的 baseline;</li><li>之后是引入 wav2vec 获取语音的 CPC feature 进行实验。</li></ol><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>使用 LibSpeech 训练说话人识别模型和做语音预训练，使用 sitw 和 CN-Celeb 测试 ASV 系统性能</p><ul><li>LibSpeech (Pretain &amp; Dev)</li><li>sitw (Test)</li><li>CN-Celeb (Test)</li></ul><h3 id="Toolkit"><a href="#Toolkit" class="headerlink" title="Toolkit"></a>Toolkit</h3><ul><li>Kaldi</li><li>fariseq-wav2vec</li></ul><h2 id="Sitw-dev"><a href="#Sitw-dev" class="headerlink" title="Sitw-dev"></a>Sitw-dev</h2><p><strong>i-vector</strong></p><table><thead><tr><th align="center">Equel Error Rate</th><th align="center">MFCC</th><th align="center">wav2vec c</th><th align="center">wav2vec z</th></tr></thead><tbody><tr><td align="center">cosine</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">PLDA</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">LDA PLDA</td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><p><strong>x-vector</strong></p><table><thead><tr><th align="center">Equel Error Rate</th><th align="center">MFCC</th><th align="center">wav2vec c</th><th align="center">wav2vec z</th></tr></thead><tbody><tr><td align="center">cosine</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">PLDA</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">LDA PLDA</td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><h2 id="Sitw-eval"><a href="#Sitw-eval" class="headerlink" title="Sitw-eval"></a>Sitw-eval</h2><p><strong>i-vector</strong></p><table><thead><tr><th align="center">Equel Error Rate</th><th align="center">MFCC</th><th align="center">wav2vec c</th><th align="center">wav2vec z</th></tr></thead><tbody><tr><td align="center">cosine</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">PLDA</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">LDA PLDA</td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><p><strong>x-vector</strong></p><table><thead><tr><th align="center">Equel Error Rate</th><th align="center">MFCC</th><th align="center">wav2vec c</th><th align="center">wav2vec z</th></tr></thead><tbody><tr><td align="center">cosine</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">PLDA</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">LDA PLDA</td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><h2 id="CN-Celeb"><a href="#CN-Celeb" class="headerlink" title="CN-Celeb"></a>CN-Celeb</h2><p><strong>i-vector</strong></p><table><thead><tr><th align="center">Equel Error Rate</th><th align="center">MFCC</th><th align="center">wav2vec c</th><th align="center">wav2vec z</th></tr></thead><tbody><tr><td align="center">cosine</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">PLDA</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">LDA PLDA</td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><p><strong>x-vector</strong></p><table><thead><tr><th align="center">Equel Error Rate</th><th align="center">MFCC</th><th align="center">wav2vec c</th><th align="center">wav2vec z</th></tr></thead><tbody><tr><td align="center">cosine</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">PLDA</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">LDA PLDA</td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>【CSLT-THU cvss SELF】</title>
    <link href="/2000/01/01/research_note/014_self_cvss/"/>
    <url>/2000/01/01/research_note/014_self_cvss/</url>
    
    <content type="html"><![CDATA[<p>Self-Supervised Pretrain speech representation for Speaker Recogntion</p><p>The ongoing success of deep learning techniques depends on the quality of the representations automatically discovered from data. What is a good representation?</p><ol><li>high-performing on a diverse set of downstream tasks using simple models.</li><li>useful in transfer learning with small amounts of data for a new task.</li></ol><p>[1] Introduction</p><p>Recently, Self-Supervised Learning(SSL) shows a promising approach for data representation. Self-Supervised Learning is a version of unsupervised learning where data provides the supervision, and the goal of SSL is learning represent the world before learning tasks. For instance, BERT is a well-know NLP model developed by Google for pre-training language representations. It leverages an enormous amount of plain text data publicly available on the web and is trained in an unsupervised manner, and it can map a variable-length sentence or word to a fixed-length vector for many NLP downstream tasks.</p><p>In speech processing filed, the extraction and selection of the best parametric representation of acoustic signals is an important task in the design of any speech recognition system or speaker recognition sysytem. However, the characteristics of the speakers in speech signal are poorly captured by the traditional acoustic features, such as the amplitudes of a wave signal, log Mel spectrograms, Mel frequency cepstral coefficients(MFCCs), or Filter banks(Fbanks). </p><p>The goal of Self-Supervised speech representation is to leverages an enormous amount of unlabeled speech data publicly available on the web and is trained in an unsupervised manner and find a transformation from the surface features that makes high-level properties of speech more accessible to downstream tasks, such as speech recognition and speaker recognition.</p><p>Therefore, inspired by the idea of Self-Supervised Learning, I did some experiments about Self-Supervised Speech Representation Feature for Speaker Recongnition task.</p><p>[2] Experiment</p><ol><li><p>Toolkits</p><ul><li>fairseq: fairseq is powerful seq2seq model toolkit developed by Facebook AI research. wav2vec is a subset project in fairseq, which is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training.</li><li>Kaldi: Kaldi is a powerful toolkit for speech signal processing, speech recognition and speaker recognition.</li></ul></li><li><p>Dataset</p><ul><li>P100: a small speech command dataset which contains 100 speakers.</li><li>P80(dev): for ASV system training, subset of P100 which contains 80 speakers</li><li>P20(enroll &amp; test): for ASV system evalutation, subset of P100 which contains 20 speakers</li></ul></li><li><p>Config</p></li></ol><p>Group 1: Average Pooling</p><pre><code>- baseline MFCC average: MFCC feature with average pooling- wav2vec c average: wav2vec pretrain feature c with average pooling- wav2vec z average: wav2vec pretrain feature z with average pooling</code></pre><p>Group 2: i-vector</p><pre><code>- baseline MFCC i-vector: MFCC feature with i-vector ASV system- wav2vec c i-vector: wav2vec pretrain feature c with i-vector ASV system- wav2vec z i-vector: wav2vec pretrain feature z with i-vector ASV system</code></pre><p>Group 3: x-vector</p><pre><code>- baseline MFCC x-vector: MFCC feature with x-vector ASV system- wav2vec c x-vector: wav2vec pretrain feature c with x-vector ASV system- wav2vec z x-vector: wav2vec pretrain feature z with x-vector ASV system</code></pre><ol start="4"><li><p>Result</p></li><li><p>Analyze</p></li></ol><p>The Result show wav2vec pretrain Speech feature.</p><p>See <a href="">Yang Zhang’s Chinese Blog</a> for more detail.</p><hr><p>Contrastive Learning for ASV</p><p>Contrastive learning has recently shown encouraging progress in Self-Supervised Learning, e.g., in Momentum Contrast (MoCo) and SimCLR.</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Self-Supervised Pretrain Speech Representation for Speaker Recongnition</title>
    <link href="/2000/01/01/research_note/014_self/"/>
    <url>/2000/01/01/research_note/014_self/</url>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>使用预训练模型提取语音声学特征从而提高当前说话人识别系统的性能。</p><p>Self-Supervised Pretrain speech representation for Speaker Recogntion</p><p>The goal of speech representation learning is to find a transformation from the surface features that makes high-level properties of speech more accessible to downstream tasks.</p><p><img src="https://wx2.sbimg.cn/2020/07/12/CuUHw.png" srcset="/img/loading.gif" alt="avatar"></p><h3 id="Backgroud"><a href="#Backgroud" class="headerlink" title="Backgroud"></a>Backgroud</h3><p>在说话人识别任务中，第一步大多都是对语音进行<strong>声学特征提取</strong>，从语音信号中提取例如 mfcc, fbank 这样的声学特征。通过提取声学特征，一帧高维度的语音信号(waveform)可以号用一个12~40维向量简洁地表示出来；一整段语音信号，就被表示为这种向量的一个序列。例如，<strong>梅尔刻度 (Mel scale)</strong>是一种基于人耳对等距的音高(pitch)变化的感官判断而定的非线性频率刻度，它与频率关系如下:</p><p>$$M(f) = 2595 \cdot log_{10}(1+\frac{f}{700})$$</p><p>MFCC 的性能表现可能会受到如下因素的影响: </p><ol><li>the number of filters</li><li>the shape of filters </li><li>the way that filters are spaced</li><li>the way that the power spectrum is warped</li></ol><p>这些传统的声学特征是根据人耳的感知机理，例如掩蔽效应，人为设计的特征提取方法。这类方法可能并不适用于计算机的模式识别，使用传统的声学特征用于说话人识别可能存在一定的不合理性。例如:</p><ol><li>声学特征提取过程中可能会存在一定的信息损失，导致识别性能降低。</li><li>声学特征参数的选取往往依赖于人的经验。不同的参数设置，会导致不同的系统识别的性能。</li><li>提取特征的过程中可能没有考虑人说话的上下文 context 信息。</li></ol><h3 id="What-is-self-supervised-learning"><a href="#What-is-self-supervised-learning" class="headerlink" title="What is self-supervised learning?"></a>What is self-supervised learning?</h3><p>最近，self-supervised learning 逐渐成为科研工作者们的关注重点。self-supervised learning 是无监督学习里的一类方法，主要是希望能够<strong>学习到一种通用的特征表达用于下游任务</strong></p><p>Yann LeCun in AAAI2020</p><blockquote><p>in self-supervised learning, the systerm learns to predict part of its input from other parts of it input.</p></blockquote><ul><li>Goal: Learning to represent the world before learning tasks.</li><li>Predict any part of the input from any other part</li><li>Predict the <strong>future</strong> from the <strong>recent past</strong></li><li>Predict the <strong>past</strong> from the <strong>present</strong></li><li>Predict the <strong>top</strong> from the <strong>bottom</strong></li></ul><p>什么是一个好的 Representation ?</p><ol><li>high-performing on a diverse set of downstream tasks using simple models</li><li>useful in transfer learning with small amounts of data for a new task</li></ol><p>自监督学习已经在 CV 和 NLP 领域取得了极大的成功，以大名鼎鼎的 BERT 模型为例，它利用大规模无标注语料训练、获得文本的包含丰富语义信息的 Representation，然后将文本的语义表示在特定 NLP 任务中作微调后应用于该任务上。</p><p>目前self-supervised learning的主要有如下方法：</p><p><img src="https://wx1.sbimg.cn/2020/07/12/Cubk8.png" srcset="/img/loading.gif" alt="avatar"></p><p>在 speech 领域，也有很多大佬进行了一系列的探索。从 interspeech2020 和 ICML2020 两个学术会议中，我们可以看出未来应该会有更多的大佬加入 self supervised learning 在 speech 领域的研究。</p><ol><li><a href="https://self-supervised-sp.github.io/Interspeech2020-Special-Session" target="_blank" rel="noopener">Interspeech2020 Special Session: New Trends in self-supervised speech processing</a></li><li><a href="https://icml-sas.gitlab.io/" target="_blank" rel="noopener">ICML2020: Self-supervision in Audio and Speech</a></li></ol><h3 id="Contrastive-Predictive-Coding"><a href="#Contrastive-Predictive-Coding" class="headerlink" title="Contrastive Predictive Coding"></a>Contrastive Predictive Coding</h3><p>Contrastive Predictive Coding (CPC) 是由 Google DeepMind 出品的非常有代表性的自监督学习方法，可以用于语音、图片、文本以及强化学习。<strong>CPC的主要思想就是基于 context 信息的未来数据预测，以及通过采样的方式进行训练。</strong></p><p><img src="https://wx2.sbimg.cn/2020/07/20/CfofV.png" srcset="/img/loading.gif" alt="avatar"></p><p>CPC 的出发点是最大化context vector $c$ 和数据输入 $x$ 之间的<strong>互信息（Mutual Information）</strong>，使得 $c$ 包含足够的原始数据信息，因而可以作为新的特征表示。互信息定义如下：</p><p>$$MI(x;c) = \sum_{x,c}p(x,c)log\frac{p(x|c)}{p(x)}$$</p><p>“对比学习”中的“对比”是 positive 样本 和 negative 样本的对比。在学习到的“表示空间”内，增大某样本与其 positive 样本之间的相似度，减少与negative 样本的相似度。CPC 目标是学习一个 encoder $f$</p><p>$$\textrm{score}(f(x),f(x^{+})) &gt;&gt; \textrm{score}(f(x), f(x^{-}))$$</p><p>score 函数是相似度的度量，例如可以设置为 $\textrm{score}(a, b) = a^Tb$。一般在一个训练批量的 $N$ 个样本中，使用 1 个 positive 样本和 $N-1$ 个 negative 样本。</p><p>如果大量无监督但数据采样训练，最大化互信息，我们可以得到数据新的表征。非常值得一提的是，论文中对不同说话人表征在 $t-SNE$ 可视化图中展现出了不错的区分性。</p><p><img src="https://wx2.sbimg.cn/2020/07/15/CZBTR.md.png" srcset="/img/loading.gif" alt="avatar"></p><h3 id="WAV2VEC"><a href="#WAV2VEC" class="headerlink" title="WAV2VEC"></a>WAV2VEC</h3><p>依据 CPC 论文的思路，FaceBook FAIR 团队提出了 wav2vec 模型利用大量未标记的音频数据预训练的方法用于提高 ASR 系统的性能。预训练模型是在 Fairseq 工具包中的 PyTorch 中实现的。</p><p><img src="https://wx2.sbimg.cn/2020/07/15/CZqK2.png" srcset="/img/loading.gif" alt="avatar"></p><p>模型由两个卷积神经网络组成:</p><ul><li><strong>Encoder Network</strong> $x \rightarrow z$ : embeds the audio signal in a latent space</li><li><strong>Context Network</strong> $z \rightarrow c$ : combines multiple time-steps of the encoder to obtain contextualized representations</li></ul><p>与 CPC 原论文里略有不同的是，在 Context Network 上，wav2vec 采用的是卷积神经网络，而 CPC 采用的 Autoregressive Model 自回归模型，因此在 infer 的速度上 wav2vec 会更快。</p><p>预训练结束后，将上下文网络 $c_i$ 生成的表示形式作为声学特征输入，而不是使用 log-mel 滤波器组特征。</p><h3 id="Self-Supervised-Feature-for-Speaker-Recongnition"><a href="#Self-Supervised-Feature-for-Speaker-Recongnition" class="headerlink" title="Self-Supervised Feature for Speaker Recongnition"></a>Self-Supervised Feature for Speaker Recongnition</h3><p>我想使用 self supervised learning 的方法改进当前说话人识别系统。创新点在于利用了大量无说话人 label 的语音数据去提高当前说话人识别系统的性能</p><p>首先通过大量没有任何标注的语音数据，依据语音时序上的上下文 context 信息，使用神经网络去重新学一个更加合理 speech 的 representation。这类 self-supervised features 对语音信号前后关系有更好地建模。可以对于说话人长时性描述更强。之后我们通过预训练好的模型，对语音进行特征提取，新的声学特征将取代 mfcc 和 fbank 这类传统的声学特征，作为说话人模型对输入。我们利用这种新的特征，和传统 i-vector 和 x-vector 表征学习相互结合。</p><p>总结来说就是使用预训练模型提取语音声学特征取代 mfcc 和 fbank 从而提高当前说话人识别系统的性能。预期可能取得如下结果:</p><ol><li>进一步提高当前的说话人识别系统的性能，成为新的STOA。</li><li>说话人训练数据在 low resources 的情况下取得更好的性能。例如 self supervised feature＋100个spk数据集训练出来的模型性能约等于甚至好于 mfcc＋500个spk数据集训练出的模型。</li><li>降低当前说话人识别模型的参数量，即 x-vector 模型的网络结构不需要很深很复杂也可以达到不错的说话人识别效果。</li><li>进一步提高当前 anti-spoofing 系统的性能。</li></ol><h2 id="My-Experiment"><a href="#My-Experiment" class="headerlink" title="My Experiment"></a>My Experiment</h2><h3 id="Toolkits"><a href="#Toolkits" class="headerlink" title="Toolkits"></a>Toolkits</h3><p>我的实验主要使用了如下两个工具:</p><ol><li><strong><a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">fairseq</a></strong>: 它是一个由 facebook AI research 团队维护的 seq2seq 模型工具包，wav2vec 是 fairseq 内的一个子项目，其中同时开源了相关的一些预训练模型</li><li><strong><a href="https://github.com/kaldi-asr/kaldi" target="_blank" rel="noopener">Kaldi</a></strong>: 它是一个主要由 Danney Povey 等人主要维护的语音工具包，其中包含了经典的说话人识别模型 (i-vector, x-vector等) 的全部 pipline 代码</li></ol><h3 id="Dataset-amp-Configs"><a href="#Dataset-amp-Configs" class="headerlink" title="Dataset &amp; Configs"></a>Dataset &amp; Configs</h3><p>为了快速验证这个想法，我在一个含有100个说话人的小规模的命令词数据集上，分别使用了几个不同的声学特征提取方法进行了3组实验。我们使用80个说话人进行说话人模型的训练，另外20个人用于测试。具体配置如下：</p><h4 id="Group-1-Average-Pooling"><a href="#Group-1-Average-Pooling" class="headerlink" title="Group 1: Average Pooling"></a>Group 1: Average Pooling</h4><ul><li><strong>baseline MFCC average</strong>: MFCC feature with average pooling</li><li><strong>wav2vec c average</strong>: wav2vec pretrain feature c with average pooling</li><li><strong>wav2vec z average</strong>: wav2vec pretrain feature z with average pooling</li></ul><h4 id="Group-2-i-vector"><a href="#Group-2-i-vector" class="headerlink" title="Group 2: i-vector"></a>Group 2: i-vector</h4><ul><li><strong>baseline MFCC i-vector</strong>: MFCC feature with i-vector ASV system</li><li><strong>wav2vec c i-vector</strong>: wav2vec pretrain feature c with i-vector ASV system</li><li><strong>wav2vec z i-vector</strong>: wav2vec pretrain feature z with i-vector ASV system</li></ul><h4 id="Group-3-x-vector"><a href="#Group-3-x-vector" class="headerlink" title="Group 3: x-vector"></a>Group 3: x-vector</h4><ul><li><strong>baseline MFCC x-vector</strong>: MFCC feature with x-vector ASV system</li><li><strong>wav2vec c x-vector</strong>: wav2vec pretrain feature c with x-vector ASV system</li><li><strong>wav2vec z x-vector</strong>: wav2vec pretrain feature z with x-vector ASV system</li></ul><h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><h4 id="Group-1-Average-Pooling-1"><a href="#Group-1-Average-Pooling-1" class="headerlink" title="Group 1: Average Pooling"></a>Group 1: Average Pooling</h4><table><thead><tr><th align="center">Equel Error Rate</th><th align="center">MFCC average</th><th align="center">wav2vec c average</th><th align="center">wav2vec z average</th></tr></thead><tbody><tr><td align="center">cosine</td><td align="center"><strong>24.59 %</strong></td><td align="center">33.52 %</td><td align="center">27.51 %</td></tr><tr><td align="center">PLDA</td><td align="center">11.68 %</td><td align="center">8.697 %</td><td align="center"><strong>5.92 %</strong></td></tr><tr><td align="center">LDA PLDA</td><td align="center">13.25 %</td><td align="center">8.614 %</td><td align="center"><strong>5.981 %</strong></td></tr></tbody></table><h4 id="Group-2-i-vector-1"><a href="#Group-2-i-vector-1" class="headerlink" title="Group 2: i-vector"></a>Group 2: i-vector</h4><table><thead><tr><th align="center">Equel Error Rate</th><th align="center">MFCC i-vector</th><th align="center">wav2vec c i-vector</th><th align="center">wav2vec z i-vector</th></tr></thead><tbody><tr><td align="center">cosine</td><td align="center">19.2 %</td><td align="center">22.23 %</td><td align="center"></td></tr><tr><td align="center">PLDA</td><td align="center">10.45 %</td><td align="center">10.35 %</td><td align="center"></td></tr><tr><td align="center">LDA PLDA</td><td align="center">9.972 %</td><td align="center">10.4 %</td><td align="center"></td></tr></tbody></table><h4 id="Group-3-x-vector-1"><a href="#Group-3-x-vector-1" class="headerlink" title="Group 3: x-vector"></a>Group 3: x-vector</h4><table><thead><tr><th align="center">Equel Error Rate</th><th align="center">MFCC x-vector</th><th align="center">wav2vec c x-vector</th><th align="center">wav2vec z x-vector</th></tr></thead><tbody><tr><td align="center">cosine</td><td align="center">12.23 %</td><td align="center">17.32 %</td><td align="center"><strong>11.71 %</strong></td></tr><tr><td align="center">PLDA</td><td align="center">12.04 %</td><td align="center">15.53 %</td><td align="center"><strong>8.926 %</strong></td></tr><tr><td align="center">LDA PLDA</td><td align="center">11.48 %</td><td align="center">11.71 %</td><td align="center"><strong>8.784 %</strong></td></tr></tbody></table><h4 id="T-SNE"><a href="#T-SNE" class="headerlink" title="T-SNE"></a>T-SNE</h4><p><img src="https://wx1.sbimg.cn/2020/07/24/DEeCM.png" srcset="/img/loading.gif" alt="avatar"></p><h3 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h3><ol><li><strong>Dataset</strong>: VoxCeleb dataset</li><li><strong>Dataset</strong>: VAD &amp; Data Arguement</li><li><strong>Pretrain</strong>: center loss for pretrain</li><li><strong>Pretrain</strong>: triplet loss for pretrain</li></ol><h2 id="Related-Research"><a href="#Related-Research" class="headerlink" title="Related Research"></a>Related Research</h2><h3 id="Speech-Pretrain-Papers"><a href="#Speech-Pretrain-Papers" class="headerlink" title="Speech Pretrain Papers"></a>Speech Pretrain Papers</h3><ol><li><a href="https://arxiv.org/abs/1904.05862" target="_blank" rel="noopener">[Facebook] wav2vec: Unsupervised Pre-Training for Speech Recognition, 2019</a></li><li><a href="https://arxiv.org/abs/2002.12764" target="_blank" rel="noopener">[Google] Towards Learning a Universal Non-Semantic Representation of Speech, 2020</a></li><li><a href="https://arxiv.org/abs/1807.03748" target="_blank" rel="noopener">[DeepMind] Representation Learning with Contrastive Predictive Coding, 2019</a></li><li><a href="https://arxiv.org/abs/1904.03240" target="_blank" rel="noopener">[MIT] An Unsupervised Autoregressive Model for Speech Representation Learning, 2019</a></li><li><a href="https://arxiv.org/abs/1912.01679" target="_blank" rel="noopener">[Amazon] Deep Contextualized Acoustic Representations for Semi-Supervised Speech Recognition, 2020</a></li><li><a href="https://arxiv.org/abs/1910.12638" target="_blank" rel="noopener">[NTU] MOCKINGJAY: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders, 2020</a></li></ol><h3 id="Speaker-Related-Papers"><a href="#Speaker-Related-Papers" class="headerlink" title="Speaker Related Papers"></a>Speaker Related Papers</h3><ol><li><a href="https://arxiv.org/abs/1812.00271" target="_blank" rel="noopener">[Yoshua Bengio] Learning Speaker Representations with Mutual Information, 2018</a></li><li><a href="https://arxiv.org/abs/2002.12764" target="_blank" rel="noopener">[Google] Towards Learning a Universal Non-Semantic Representation of Speech, 2020</a></li><li><a href="https://arxiv.org/abs/1904.03240" target="_blank" rel="noopener">[MIT] An Unsupervised Autoregressive Model for Speech Representation Learning, 2019</a></li></ol><h3 id="Pretrain-Related-Toolkits-amp-Code"><a href="#Pretrain-Related-Toolkits-amp-Code" class="headerlink" title="Pretrain Related Toolkits &amp; Code"></a>Pretrain Related Toolkits &amp; Code</h3><ol><li><a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">[FaceBook] fairseq</a></li><li><a href="https://github.com/awslabs/speech-representations" target="_blank" rel="noopener">[Amazon] speech-representations</a></li><li><a href="https://github.com/andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning" target="_blank" rel="noopener">[NTU] MOCKINGJAY</a></li><li><a href="https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark" target="_blank" rel="noopener">[Google] non semantic speech benchmark</a></li></ol><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html" target="_blank" rel="noopener">ankeshanand Contrative SSL blog</a></li><li><a href="https://self-supervised-sp.github.io/Interspeech2020-Special-Session" target="_blank" rel="noopener">Interspeech2020 Special Session: New Trends in self-supervised speech processing</a></li><li><a href="https://icml-sas.gitlab.io/" target="_blank" rel="noopener">ICML2020: Self-supervision in Audio and Speech</a></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>自动驾驶产业的“社会-技术”系统构建与转型</title>
    <link href="/2000/01/01/2020/000_homework/"/>
    <url>/2000/01/01/2020/000_homework/</url>
    
    <content type="html"><![CDATA[<p><strong>摘 要</strong>：    《中国制造2025》报告中明确指出，智能网联汽车是我国将要重点发展的新兴事物之一，它与传统汽车最大的区别在于它运用了AI技术，有着人工智能的加持。随着人工智能的不断发展和低时延高带宽5G网络的普及，自动驾驶在未来必将迎来新的发展。然而整个社会交通的智能化、自动驾驶化整个过程并不是一蹴而就的，需要长时间的不断构建、迭代和转型。因此我结合本门课程所学的“社会—技术”系统分析的相关知识，并结合自己的理解对当前的自动驾驶产业发展进行分析和进一步的深入解读，并提出了自己对这个产业未来发展的看法。</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
